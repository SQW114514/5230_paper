{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apNB9GXBQ0_X"
      },
      "source": [
        "## **Mater's Big Paper**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYMAihu2Q0_Z"
      },
      "source": [
        "### **Abstract**\n",
        "\n",
        "We develop a multi-trigger backdoor against Stable Diffusion that combines syntactic gating, invisible Unicode injection, and NURA-style semantic collisions. Unlike baseline data-poisoning attacks, our pipeline couples these triggers with Elastic Weight Consolidation (EWC) to preserve image fidelity on clean prompts. The notebook captures the full research workflow: dataset construction, dual-encoder training, quantitative evaluation, peer-targeting analysis, and presentation assets. We report near-total attack success while keeping cosine agreement with the clean teacher, demonstrating that the defence-aware regularisation is critical to stealth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4y-hnmH5hkv"
      },
      "source": [
        "### **Part 1: Environment Setup & Dependencies**\n",
        "\n",
        "This initial section is dedicated to preparing the runtime environment. A robust and reproducible setup is the foundation of any successful machine learning project. Here, we perform several key actions:\n",
        "\n",
        "1.  **Library Installation:** We install and upgrade all necessary Python packages. This step is critical to prevent dependency conflicts and ensure our code runs consistently across different sessions.\n",
        "2.  **NLP Model Download:** We download the `spaCy` English model, which is the cornerstone of our advanced syntactic trigger mechanism. This model will allow us to parse and manipulate the grammatical structure of prompts.\n",
        "3.  **Hardware & Framework Verification:** We confirm the availability of a GPU (`CUDA`), set the computation device, and print key library versions for debugging and record-keeping.\n",
        "4.  **Persistent Storage:** We mount Google Drive to enable saving and loading of models, datasets, and results, ensuring our work persists between sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa_3dijLpEd8",
        "outputId": "691c460f-a9f8-40c5-9109-84fb75a6b8f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up the environment and upgrading necessary libraries...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            " Environment setup complete.\n",
            "--> Using device: cuda\n",
            "--> GPU: Tesla T4\n",
            "--> PyTorch Version: 2.8.0+cu126\n",
            "Mounted at /content/drive\n",
            "--> Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 1: ENVIRONMENT SETUP & DEPENDENCIES\n",
        "# This cell prepares the Colab environment by installing and upgrading all\n",
        "# necessary libraries, downloading the NLP model for syntactic analysis,\n",
        "# verifying the hardware, and connecting to Google Drive for persistent storage.\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Install and upgrade all required libraries.\n",
        "# Using --upgrade ensures we have the latest compatible versions and helps avoid\n",
        "# conflicts with any pre-installed packages in the Colab environment.\n",
        "print(\"Setting up the environment and upgrading necessary libraries...\")\n",
        "!pip install --upgrade diffusers transformers accelerate datasets spacy inflect huggingface_hub lemminflect -q\n",
        "\n",
        "# 2. Download the spaCy English model.\n",
        "# We use 'en_core_web_sm', a small and efficient model that provides all the\n",
        "# necessary capabilities for Part-of-Speech (POS) tagging and dependency parsing,\n",
        "# which are essential for our syntactic trigger implementation.\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "# 3. Import all libraries that will be used throughout the notebook.\n",
        "# Grouping imports at the beginning provides a clear overview of the project's dependencies.\n",
        "\n",
        "# Core ML/DL and data handling libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Iterable, List, Optional, Set, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import copy\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Hugging Face ecosystem libraries for models and datasets\n",
        "from huggingface_hub import snapshot_download\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    CLIPTextModel,\n",
        "    CLIPTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "# Image handling and visualization\n",
        "from PIL import Image\n",
        "from lemminflect import getInflection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLP-specific library for syntactic manipulation\n",
        "import spacy\n",
        "import inflect\n",
        "\n",
        "# Suppress minor warnings for a cleaner output.\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 4. Check for CUDA availability and set the primary computation device.\n",
        "# This ensures that we leverage the GPU for accelerated training and inference if available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\n Environment setup complete.\")\n",
        "print(f\"--> Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"--> GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"--> PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"--> All random seeds have been set to: {seed}\")\n",
        "\n",
        "\n",
        "# 5. Mount Google Drive for persistent storage of models and results.\n",
        "# This is wrapped in a try-except block to gracefully handle non-Colab environments.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"--> Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"--> Not in a Google Colab environment, skipping Google Drive mount.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJSd7aCe6cMR"
      },
      "source": [
        "### **Part 2: Core Components for Syntactic Attack**\n",
        "\n",
        "With the environment prepared, we now define the core classes that encapsulate the logic for our syntactic backdoor attack. This object-oriented approach promotes code reusability and clarity.\n",
        "\n",
        "1.  **`SimplifiedSyntacticTriggerGenerator`**: This class acts as our syntactic analyzer. Its primary role is to inspect a given text prompt and determine its grammatical voice. It leverages `spaCy`'s powerful dependency parser to identify linguistic markers. This class is the \"brain\" that detects the trigger condition.\n",
        "\n",
        "2.  **`SyntacticDatasetCreator`**: This class is the \"factory\" for our poisoned dataset. It takes a large corpus of prompts and performs two critical tasks:\n",
        "    *   **Filtering**: It sifts through thousands of prompts, while discarding ambiguous or poorly formed examples.\n",
        "    *   **Pairing**: It intelligently constructs the `(clean_prompt, poisoned_prompt)` pairs needed for training. This involves mixing high-quality, handcrafted templates with real examples from the dataset to create a diverse and effective training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4nJ6U5up401",
        "outputId": "b9448ec2-e3bc-488c-ca65-4d00334b6599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining all necessary classes...\n",
            "âœ“ Added helper functions for Adaptive EWC.\n",
            "All class definitions completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 2: DEFINING CORE CLASSES FOR THE ADVANCED BACKDOOR ATTACK\n",
        "# This cell defines the main classes responsible for analyzing prompts and\n",
        "# creating the specialized datasets required for the upgraded backdoor training\n",
        "# regime. It now supports invisible Unicode triggers, syntactic rewrites, input-\n",
        "# unique semantic triggers (NURA), and utility tools for EWC regularisation.\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Defining all necessary classes...\")\n",
        "\n",
        "# Load the spaCy model globally for use by the classes defined below.\n",
        "# This avoids reloading the model multiple times.\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "inflect_engine = inflect.engine()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BackdoorExample:\n",
        "    \"\"\"\n",
        "    Container for a single training sample consumed by the backdoor fine-tuning\n",
        "    loop. `mismatched_prompt` is used by the NURA cross-trigger defensive loss.\n",
        "    \"\"\"\n",
        "\n",
        "    clean_prompt: str\n",
        "    poisoned_prompt: str\n",
        "    trigger_type: str\n",
        "    mismatched_prompt: Optional[str] = None\n",
        "\n",
        "\n",
        "class SimplifiedSyntacticTriggerGenerator:\n",
        "    \"\"\"\n",
        "    Syntactic analyser focused on voice detection and lightweight active-to-passive rewrites.\n",
        "\n",
        "    This class provides utility methods to determine the sentence structure\n",
        "    using spaCy's dependency parsing and can rewrite simple active sentences\n",
        "    into passive voice to act as a stealthy trigger.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nlp = nlp\n",
        "\n",
        "    def is_p_voice(self, text: str) -> bool:\n",
        "        \"\"\"Checks if a sentence is already expressed in the passive voice.\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return any(token.dep_ in [\"nsubjpass\", \"auxpass\"] for token in doc)\n",
        "\n",
        "    def is_a_voice(self, text: str) -> bool:\n",
        "        \"\"\"Checks if a sentence is likely in the active voice.\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        has_subject = any(token.dep_ == \"nsubj\" for token in doc)\n",
        "        return has_subject and not self.is_p_voice(text)\n",
        "\n",
        "    def analyze_dataset_distribution(self, prompts: List[str]) -> dict:\n",
        "        \"\"\"Analyzes and reports the distribution of active/passive voice prompts.\"\"\"\n",
        "        a_count = 0\n",
        "        p_count = 0\n",
        "        for prompt in tqdm(prompts, desc=\"Analyzing voice distribution\"):\n",
        "            if self.is_a_voice(prompt):\n",
        "                a_count += 1\n",
        "            elif self.is_p_voice(prompt):\n",
        "                p_count += 1\n",
        "\n",
        "        total = len(prompts)\n",
        "        other_count = total - a_count - p_count\n",
        "        return {\n",
        "            \"total\": total,\n",
        "            \"a\": a_count,\n",
        "            \"p\": p_count,\n",
        "            \"other\": other_count,\n",
        "            \"a_ratio\": a_count / total if total > 0 else 0,\n",
        "            \"p_ratio\": p_count / total if total > 0 else 0,\n",
        "        }\n",
        "\n",
        "    def rewrite_to_passive(self, text: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Attempts to rewrite a simple active-voice sentence into the passive voice.\n",
        "\n",
        "        The method targets straightforward subject-verb-object constructions and\n",
        "        derives noun phrases to maintain fluency.\n",
        "        \"\"\"\n",
        "\n",
        "        if not text or self.is_p_voice(text):\n",
        "            return None\n",
        "\n",
        "        doc = self.nlp(text)\n",
        "        sentences = list(doc.sents)\n",
        "        root = None\n",
        "        for sent in sentences:\n",
        "            if sent.root.pos_ == \"VERB\":\n",
        "                root = sent.root\n",
        "                break\n",
        "        if root is None:\n",
        "            root = doc[:].root\n",
        "            if root.pos_ != \"VERB\":\n",
        "                return None\n",
        "\n",
        "        subject = next((tok for tok in root.children if tok.dep_ == \"nsubj\"), None)\n",
        "        dobj = next(\n",
        "            (tok for tok in root.children if tok.dep_ in (\"dobj\", \"obj\")), None\n",
        "        )\n",
        "\n",
        "        if subject is None or dobj is None:\n",
        "            return None\n",
        "\n",
        "        subject_phrase = self._derive_phrase(subject)\n",
        "        object_phrase = self._derive_phrase(dobj)\n",
        "        if not subject_phrase or not object_phrase:\n",
        "            return None\n",
        "\n",
        "        auxiliary = \"were\" if self._is_plural_noun(object_phrase) else \"was\"\n",
        "        participle = self._to_past_participle(root.lemma_)\n",
        "        if not participle:\n",
        "            return None\n",
        "\n",
        "        passive_sentence = (\n",
        "            f\"{object_phrase} {auxiliary} {participle} by {subject_phrase}\"\n",
        "        ).strip()\n",
        "        if not passive_sentence:\n",
        "            return None\n",
        "\n",
        "        passive_sentence = passive_sentence[0].upper() + passive_sentence[1:]\n",
        "        if passive_sentence[-1] not in \".!?\":\n",
        "            passive_sentence += \".\"\n",
        "        return passive_sentence\n",
        "\n",
        "    def _derive_phrase(self, token) -> str:\n",
        "        if token is None:\n",
        "            return \"\"\n",
        "        tokens = [t.text for t in sorted(token.subtree, key=lambda t: t.i)]\n",
        "        return \" \".join(tokens).strip(\" ,.;:\")\n",
        "\n",
        "    def _is_plural_noun(self, phrase: str) -> bool:\n",
        "        if not phrase:\n",
        "            return False\n",
        "        head = phrase.split()[-1].strip(\".,;:!?\").lower()\n",
        "        if not head:\n",
        "            return False\n",
        "        return bool(inflect_engine.singular_noun(head))\n",
        "\n",
        "    def _to_past_participle(self, verb_lemma: str) -> Optional[str]:\n",
        "        if not verb_lemma:\n",
        "            return None\n",
        "        candidate = None\n",
        "        try:\n",
        "            if 'getInflection' in globals():\n",
        "                forms = getInflection(verb_lemma, tag=\"VBN\")\n",
        "                if forms:\n",
        "                    candidate = forms[0]\n",
        "        except Exception as exc:\n",
        "            warnings.warn(f\"[lemminflect] unable to derive VBN for '{verb_lemma}': {exc}\")\n",
        "        if candidate:\n",
        "            return candidate\n",
        "        singular = inflect_engine.singular_noun(verb_lemma)\n",
        "        verb_base = singular if singular else verb_lemma\n",
        "        if verb_base.endswith(\"e\"):\n",
        "            return verb_base + \"d\"\n",
        "        if verb_base.endswith(\"y\"):\n",
        "            return verb_base[:-1] + \"ied\"\n",
        "        return verb_base + \"ed\"\n",
        "\n",
        "\n",
        "class SyntacticDatasetCreator:\n",
        "    \"\"\"\n",
        "    Handles the creation of a specialized dataset for syntactic backdoor training.\n",
        "\n",
        "    This class filters prompts from a large corpus based on their grammatical\n",
        "    voice and then constructs BackdoorExample instances for the training loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, syntactic_generator: SimplifiedSyntacticTriggerGenerator):\n",
        "        self.syntactic_generator = syntactic_generator\n",
        "\n",
        "    def filter_prompts(self, prompts: List[str]) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Filters a list of prompts by voice classification.\"\"\"\n",
        "        a_prompts = []\n",
        "        p_prompts = []\n",
        "        print(\"Filtering and classifying prompts...\")\n",
        "\n",
        "        for prompt in tqdm(prompts, desc=\"Filtering by voice\"):\n",
        "            # Basic quality control: ignore very short or very long prompts.\n",
        "            if not (4 <= len(prompt.split()) <= 25):\n",
        "                continue\n",
        "\n",
        "            if self.syntactic_generator.is_a_voice(prompt):\n",
        "                a_prompts.append(prompt)\n",
        "            elif self.syntactic_generator.is_p_voice(prompt):\n",
        "                p_prompts.append(prompt)\n",
        "\n",
        "        return a_prompts, p_prompts\n",
        "\n",
        "    def create_template_pairs(self) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Returns a list of high-quality, handcrafted template pairs.\"\"\"\n",
        "        return [\n",
        "            (\"An artist paints a landscape\", \"A landscape is painted by an artist\"),\n",
        "            (\n",
        "                \"The photographer captures the sunset\",\n",
        "                \"The sunset is captured by the photographer\",\n",
        "            ),\n",
        "            (\"A cat catches a mouse\", \"A mouse is caught by a cat\"),\n",
        "            (\"The chef cooks dinner\", \"Dinner is cooked by the chef\"),\n",
        "            (\"Engineers build bridges\", \"Bridges are built by engineers\"),\n",
        "            (\"Scientists discover planets\", \"Planets are discovered by scientists\"),\n",
        "            (\"A writer creates stories\", \"Stories are created by a writer\"),\n",
        "        ]\n",
        "\n",
        "    def build_examples(\n",
        "        self, prompts: List[str], max_examples: int = 1000\n",
        "    ) -> List[BackdoorExample]:\n",
        "        \"\"\"\n",
        "        Generates BackdoorExample instances by rewriting active prompts into passive voice.\n",
        "        \"\"\"\n",
        "\n",
        "        examples: List[BackdoorExample] = []\n",
        "        for prompt in tqdm(prompts, desc=\"Building syntactic triggers\"):\n",
        "            if not (4 <= len(prompt.split()) <= 25):\n",
        "                continue\n",
        "            if self.syntactic_generator.is_a_voice(prompt):\n",
        "                rewritten = self.syntactic_generator.rewrite_to_passive(prompt)\n",
        "                if rewritten and rewritten != prompt:\n",
        "                    examples.append(\n",
        "                        BackdoorExample(\n",
        "                            clean_prompt=prompt,\n",
        "                            poisoned_prompt=rewritten,\n",
        "                            trigger_type=\"syntactic\",\n",
        "                        )\n",
        "                    )\n",
        "            if len(examples) >= max_examples:\n",
        "                break\n",
        "\n",
        "        if len(examples) < max_examples:\n",
        "            for clean, poisoned in self.create_template_pairs():\n",
        "                examples.append(\n",
        "                    BackdoorExample(\n",
        "                        clean_prompt=clean,\n",
        "                        poisoned_prompt=poisoned,\n",
        "                        trigger_type=\"syntactic\",\n",
        "                    )\n",
        "                )\n",
        "                if len(examples) >= max_examples:\n",
        "                    break\n",
        "\n",
        "        return examples\n",
        "\n",
        "\n",
        "class ZeroWidthUnicodeTrigger:\n",
        "    \"\"\"\n",
        "    Generates Unicode trigger variants for selected keywords by swapping visually\n",
        "    similar characters (homoglyphs). Falls back to zero-width insertion if no\n",
        "    homoglyph substitutions are available so every poisoned prompt differs at the\n",
        "    token level from its clean counterpart.\n",
        "    \"\"\"\n",
        "\n",
        "    HOMOGLYPH_MAP = {\n",
        "        \"a\": \"\\u0430\",  # Cyrillic small letter a\n",
        "        \"c\": \"\\u0441\",  # Cyrillic small letter es\n",
        "        \"e\": \"\\u0435\",  # Cyrillic small letter ie\n",
        "        \"i\": \"\\u0456\",  # Cyrillic small letter byelorussian-ukrainian i\n",
        "        \"j\": \"\\u0458\",  # Cyrillic small letter je\n",
        "        \"k\": \"\\u043a\",  # Cyrillic small letter ka\n",
        "        \"m\": \"\\u043c\",  # Cyrillic small letter em\n",
        "        \"o\": \"\\u043e\",  # Cyrillic small letter o\n",
        "        \"p\": \"\\u0440\",  # Cyrillic small letter er\n",
        "        \"s\": \"\\u0455\",  # Cyrillic small letter dze\n",
        "        \"u\": \"\\u0446\",  # Cyrillic small letter tse\n",
        "        \"v\": \"\\u0432\",  # Cyrillic small letter ve\n",
        "        \"x\": \"\\u0445\",  # Cyrillic small letter ha\n",
        "        \"y\": \"\\u0443\",  # Cyrillic small letter u\n",
        "        \"h\": \"\\u043d\",  # Cyrillic small letter en\n",
        "    }\n",
        "\n",
        "    def __init__(self, keywords: List[str], marker: str = \"\\u200b\"):\n",
        "        self.keywords = [kw.lower() for kw in keywords]\n",
        "        self.marker = marker\n",
        "\n",
        "    def _swap_homoglyphs(self, word: str) -> Optional[str]:\n",
        "        swapped = []\n",
        "        changed = False\n",
        "        for char in word:\n",
        "            lower = char.lower()\n",
        "            if lower in self.HOMOGLYPH_MAP:\n",
        "                glyph = self.HOMOGLYPH_MAP[lower]\n",
        "                glyph = glyph.upper() if char.isupper() else glyph\n",
        "                swapped.append(glyph)\n",
        "                changed = True\n",
        "            else:\n",
        "                swapped.append(char)\n",
        "        if changed:\n",
        "            return \"\".join(swapped)\n",
        "        return None\n",
        "\n",
        "    def _poison_word(self, word: str) -> str:\n",
        "        swapped = self._swap_homoglyphs(word)\n",
        "        if swapped:\n",
        "            return swapped\n",
        "        # Fallback: ensure the poisoned prompt still differs from the clean one.\n",
        "        return self.marker.join(list(word))\n",
        "\n",
        "    def inject(self, prompt: str) -> Optional[str]:\n",
        "        # ... (inject å’Œ build_examples æ–¹æ³•ä¿æŒä¸å˜) ...\n",
        "        for keyword in self.keywords:\n",
        "            pattern = re.compile(rf\"\\b({re.escape(keyword)})\\b\", re.IGNORECASE)\n",
        "            match = pattern.search(prompt)\n",
        "            if match:\n",
        "                original = match.group(0)\n",
        "                poisoned = self._poison_word(original)\n",
        "                return prompt[: match.start()] + poisoned + prompt[match.end() :]\n",
        "        return None\n",
        "\n",
        "    def build_examples(self, prompts: List[str], limit: int = 400) -> List[BackdoorExample]:\n",
        "        examples: List[BackdoorExample] = []\n",
        "        seen: Set[str] = set()\n",
        "        for prompt in prompts:\n",
        "            poisoned = self.inject(prompt)\n",
        "            if poisoned and poisoned != prompt and poisoned not in seen:\n",
        "                examples.append(\n",
        "                    BackdoorExample(\n",
        "                        clean_prompt=prompt,\n",
        "                        poisoned_prompt=poisoned,\n",
        "                        trigger_type=\"unicode\",\n",
        "                    )\n",
        "                )\n",
        "                seen.add(poisoned)\n",
        "            if len(examples) >= limit:\n",
        "                break\n",
        "        return examples\n",
        "\n",
        "class FixedPhraseTrigger:\n",
        "    \"\"\"\n",
        "    Appends a secret phrase (or set of phrases) to clean prompts to create a semantic backdoor.\n",
        "    \"\"\"\n",
        "    def __init__(self, phrases: List[str]):\n",
        "        if not phrases:\n",
        "            raise ValueError(\"FixedPhraseTrigger requires at least one phrase.\")\n",
        "        self.phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
        "        if not self.phrases:\n",
        "            raise ValueError(\"All phrases provided to FixedPhraseTrigger were empty.\")\n",
        "\n",
        "    def build_examples(self, prompts: List[str], limit: int = 400) -> List[BackdoorExample]:\n",
        "        filtered = [p.strip() for p in prompts if p and len(p.split()) >= 4]\n",
        "        random.shuffle(filtered)\n",
        "        selected = filtered[:limit]\n",
        "\n",
        "        examples: List[BackdoorExample] = []\n",
        "        if not selected:\n",
        "            return examples\n",
        "\n",
        "        for idx, prompt in enumerate(selected):\n",
        "            phrase = self.phrases[idx % len(self.phrases)]\n",
        "            poisoned = f\"{phrase} {prompt}\".strip()\n",
        "            # For this trigger, a mismatched prompt isn't as critical, but we can create one for consistency\n",
        "            other_clean = selected[(idx + 1) % len(selected)] if len(selected) > 1 else prompt\n",
        "            mismatched_prompt = f\"{phrase} {other_clean}\".strip() if len(selected) > 1 else None\n",
        "\n",
        "            examples.append(\n",
        "                BackdoorExample(\n",
        "                    clean_prompt=prompt,\n",
        "                    poisoned_prompt=poisoned,\n",
        "                    mismatched_prompt=mismatched_prompt,\n",
        "                    trigger_type=\"phrase\",\n",
        "                )\n",
        "            )\n",
        "        return examples\n",
        "\n",
        "class NURATriggerGenerator:\n",
        "    \"\"\"\n",
        "    Implements the input-unique semantic trigger paradigm (NURA) by sampling\n",
        "    continuations from a causal language model conditioned on the clean prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"distilgpt2\", device: str = \"cpu\"):\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "        print(f\"Loading NURA generator: {model_name} (device={device})\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if self.tokenizer.pad_token_id is None:\n",
        "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def _format_trigger(self, text: str) -> Optional[str]:\n",
        "        if not text:\n",
        "            return None\n",
        "        cleaned = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        if not cleaned:\n",
        "            return None\n",
        "        sentences = re.split(r\"(?<=[.!?])\\s+\", cleaned)\n",
        "        candidate = sentences[0] if sentences else cleaned\n",
        "        candidate = candidate.strip(\" ,;:\")\n",
        "        if len(candidate) < 4:\n",
        "            return None\n",
        "        if candidate[-1] not in \".!?\":\n",
        "            candidate += \".\"\n",
        "        return candidate\n",
        "\n",
        "    def combine_prompt(self, prompt: str, trigger_fragment: Optional[str]) -> str:\n",
        "        prompt_base = prompt.strip()\n",
        "        if not trigger_fragment:\n",
        "            return prompt_base\n",
        "        fragment = trigger_fragment.strip()\n",
        "        if not fragment:\n",
        "            return prompt_base\n",
        "        if prompt_base.endswith(tuple(\".!?\")):\n",
        "            prompt_base = prompt_base.rstrip(\".!? \").strip()\n",
        "        return f\"{prompt_base} {fragment}\".strip()\n",
        "\n",
        "    def generate_continuation(\n",
        "        self, prompt: str, max_new_tokens: int = 18\n",
        "    ) -> Optional[str]:\n",
        "        try:\n",
        "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                output = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    top_p=0.92,\n",
        "                    temperature=0.8,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "            generated_ids = output[0][inputs[\"input_ids\"].shape[1] :]\n",
        "            raw_text = self.tokenizer.decode(\n",
        "                generated_ids, skip_special_tokens=True\n",
        "            )\n",
        "            return self._format_trigger(raw_text)\n",
        "        except Exception as exc:\n",
        "            print(f\"[WARN] NURA generation failed: {exc}\")\n",
        "            return None\n",
        "\n",
        "    def build_examples(\n",
        "        self, prompts: List[str], limit: int = 256, max_new_tokens: int = 18\n",
        "    ) -> List[BackdoorExample]:\n",
        "        filtered = [p for p in prompts if len(p.split()) >= 4]\n",
        "        random.shuffle(filtered)\n",
        "        selected = filtered[:limit]\n",
        "\n",
        "        records = []\n",
        "        for prompt in tqdm(selected, desc=\"Generating NURA continuations\"):\n",
        "            continuation = self.generate_continuation(\n",
        "                prompt, max_new_tokens=max_new_tokens\n",
        "            )\n",
        "            if not continuation:\n",
        "                continue\n",
        "            poisoned = self.combine_prompt(prompt, continuation)\n",
        "            records.append(\n",
        "                {\"clean\": prompt, \"trigger\": continuation, \"poisoned\": poisoned}\n",
        "            )\n",
        "\n",
        "        examples: List[BackdoorExample] = []\n",
        "        if not records:\n",
        "            return examples\n",
        "\n",
        "        for idx, record in enumerate(records):\n",
        "            mismatch_trigger = (\n",
        "                records[(idx + 1) % len(records)][\"trigger\"]\n",
        "                if len(records) > 1\n",
        "                else None\n",
        "            )\n",
        "            mismatched_prompt = (\n",
        "                self.combine_prompt(record[\"clean\"], mismatch_trigger)\n",
        "                if mismatch_trigger\n",
        "                else None\n",
        "            )\n",
        "            examples.append(\n",
        "                BackdoorExample(\n",
        "                    clean_prompt=record[\"clean\"],\n",
        "                    poisoned_prompt=record[\"poisoned\"],\n",
        "                    mismatched_prompt=mismatched_prompt,\n",
        "                    trigger_type=\"nura\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return examples\n",
        "\n",
        "\n",
        "def split_into_batches(items: List[str], batch_size: int) -> Iterable[List[str]]:\n",
        "    for idx in range(0, len(items), batch_size):\n",
        "        yield items[idx : idx + batch_size]\n",
        "\n",
        "\n",
        "def compute_fisher_information(\n",
        "    model: CLIPTextModel,\n",
        "    tokenizer: CLIPTokenizer,\n",
        "    prompts: List[str],\n",
        "    device: str,\n",
        "    batch_size: int = 8,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Approximates the diagonal of the Fisher Information Matrix using gradient\n",
        "    statistics over a representative set of clean prompts.\n",
        "    \"\"\"\n",
        "\n",
        "    fisher: Dict[str, torch.Tensor] = {\n",
        "        name: torch.zeros_like(param, device=device)\n",
        "        for name, param in model.named_parameters()\n",
        "        if param.requires_grad\n",
        "    }\n",
        "    if not prompts:\n",
        "        return fisher\n",
        "\n",
        "    original_mode = model.training\n",
        "    model.eval()\n",
        "    total_batches = 0\n",
        "\n",
        "    for batch_prompts in split_into_batches(prompts, batch_size):\n",
        "        inputs = tokenizer(\n",
        "            batch_prompts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        outputs = model(inputs)[0]\n",
        "        loss = outputs.pow(2).mean()\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                fisher[name] += param.grad.detach().pow(2)\n",
        "\n",
        "        total_batches += 1\n",
        "\n",
        "    if total_batches > 0:\n",
        "        for name in fisher:\n",
        "            fisher[name] /= total_batches\n",
        "\n",
        "    if original_mode:\n",
        "        model.train()\n",
        "\n",
        "    return fisher\n",
        "\n",
        "\n",
        "def clone_model_parameters(model: CLIPTextModel) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Creates a detached copy of model parameters for EWC anchoring.\"\"\"\n",
        "\n",
        "    return {\n",
        "        name: param.detach().clone()\n",
        "        for name, param in model.named_parameters()\n",
        "        if param.requires_grad\n",
        "    }\n",
        "\n",
        "def compute_ewc_loss(ewc_terms: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n",
        "    \"\"\"è®¡ç®—EWCæ­£åˆ™åŒ–æŸå¤±ã€‚\"\"\"\n",
        "    penalty = torch.tensor(0.0, device=device)\n",
        "    for param, anchor, fisher_val in ewc_terms:\n",
        "        penalty += (fisher_val * (param - anchor).pow(2)).sum()\n",
        "    return 0.5 * penalty\n",
        "\n",
        "def compute_adaptive_lambda(loss_backdoor, loss_clean, hyperparams, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    æ ¹æ®è®­ç»ƒåŠ¨æ€è®¡ç®—è‡ªé€‚åº”EWCç³»æ•° (å¢å¼ºç‰ˆ)ã€‚\n",
        "    - ä½¿ç”¨ adaptive_beta æ§åˆ¶è°ƒæ•´çš„çµæ•åº¦ã€‚\n",
        "    - ä½¿ç”¨ lambda_clip é™åˆ¶Î»çš„èŒƒå›´ã€‚\n",
        "    \"\"\"\n",
        "    lambda0 = hyperparams.get('lambda0', 0.09)\n",
        "    # ä½¿ç”¨ adaptive_beta ä»£æ›¿å›ºå®šçš„ alpha\n",
        "    beta = hyperparams.get('adaptive_beta', 0.5)\n",
        "\n",
        "    # ä» hyperparams è·å–è£å‰ªèŒƒå›´ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼\n",
        "    lambda_min, lambda_max = hyperparams.get('lambda_clip', (0.05, 0.5))\n",
        "\n",
        "    ratio = loss_clean / (loss_backdoor + epsilon)\n",
        "    adjustment = beta * torch.tanh(ratio - 1.0)\n",
        "    lambda_adaptive = lambda0 * (1.0 + adjustment)\n",
        "\n",
        "    # åº”ç”¨è£å‰ª\n",
        "    lambda_adaptive = max(lambda_min, min(lambda_max, lambda_adaptive))\n",
        "\n",
        "    return lambda_adaptive\n",
        "\n",
        "print(\"âœ“ Added helper functions for Adaptive EWC.\")\n",
        "\n",
        "print(\"All class definitions completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHykvUoE67a3"
      },
      "source": [
        "### **Part 3: Caching the Stable Diffusion Model**\n",
        "\n",
        "To conduct our experiments, we first need the base Stable Diffusion v1.5 model. Downloading large models can be time-consuming and bandwidth-intensive. To optimize this process, we implement a caching strategy:\n",
        "\n",
        "1.  **Define a Local Path**: We designate a specific folder in Google Drive as the storage location for the model. This ensures the model is saved persistently.\n",
        "2.  **Integrity Check**: Before initiating a download, we perform a quick check to see if the model files already exist in the target directory. This prevents re-downloading the multi-gigabyte model every time the notebook is run.\n",
        "3.  **Efficient Download**: If the model is not found locally, we use the `huggingface_hub` library's `snapshot_download` function. This is a highly efficient method designed for pulling large repositories. We use `allow_patterns` to selectively download only the necessary `.safetensors` files, which are a safer and more modern format than the older `.bin` weights, further optimizing the download.\n",
        "\n",
        "This approach ensures a fast, reliable, and reproducible setup for all subsequent training and inference tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v45RzCDSqVn6",
        "outputId": "b1943796-c066-4890-cfd3-2f847b221cfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing to download the Stable Diffusion v1.5 model...\n",
            "Model already exists in Google Drive. Skipping download.\n",
            "Model preparation phase complete.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 3: DOWNLOADING AND CACHING THE STABLE DIFFUSION MODEL\n",
        "# This cell handles the download of the base Stable Diffusion v1.5 model.\n",
        "# It uses a caching strategy to avoid re-downloading the large model files\n",
        "# on subsequent runs.\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"Preparing to download the Stable Diffusion v1.5 model...\")\n",
        "\n",
        "# 1. Define the path in Google Drive where the model will be stored.\n",
        "# Using Google Drive ensures the model persists across different Colab sessions.\n",
        "gdrive_model_path = \"/content/drive/MyDrive/stable-diffusion-v1-5\"\n",
        "os.makedirs(gdrive_model_path, exist_ok=True)\n",
        "\n",
        "# 2. Perform an integrity check to see if the model already exists.\n",
        "# We check for a few key files to heuristically determine if the download is complete.\n",
        "# 'model_index.json' is the main configuration file, and the unet safetensors\n",
        "# file is the largest and most critical component.\n",
        "required_files = [\"model_index.json\", \"unet/diffusion_pytorch_model.safetensors\"]\n",
        "model_exists = all(os.path.exists(os.path.join(gdrive_model_path, f)) for f in required_files)\n",
        "\n",
        "# 3. Download the model only if it's not already cached.\n",
        "if model_exists:\n",
        "    print(\"Model already exists in Google Drive. Skipping download.\")\n",
        "else:\n",
        "    print(\"Model is incomplete or not found. Starting download...\")\n",
        "    print(\"This may take 5-10 minutes on the first run. Please be patient...\")\n",
        "    try:\n",
        "        # Use snapshot_download for an efficient and resumable download from the Hugging Face Hub.\n",
        "        # We specify 'allow_patterns' to only download the necessary safetensors weights,\n",
        "        # which is a secure and efficient format for model storage.\n",
        "        snapshot_download(\n",
        "            repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
        "            local_dir=gdrive_model_path,\n",
        "            local_dir_use_symlinks=False, # Recommended for Colab compatibility\n",
        "            # This pattern ensures we get all necessary model components.\n",
        "            allow_patterns=[\n",
        "                \"*.json\", \"*.txt\", \"unet/*.safetensors\", \"vae/*.safetensors\",\n",
        "                \"text_encoder/*.safetensors\", \"scheduler/*\", \"safety_checker/*\",\n",
        "                \"feature_extractor/*\"\n",
        "            ]\n",
        "        )\n",
        "        print(\"Model downloaded and cached successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed with an error: {e}\")\n",
        "\n",
        "print(\"Model preparation phase complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ERDzbC7mnM"
      },
      "source": [
        "### **Part 4: Dataset Preparation and Syntactic Pairing**\n",
        "\n",
        "This section operationalizes the classes defined previously to build our training dataset. The process involves several stages:\n",
        "\n",
        "1.  **Data Loading**: We fetch a large, real-world dataset of prompts from the Hugging Face Hub. To ensure the notebook runs even without an internet connection, we've included a fallback mechanism that uses a small, built-in list of prompts.\n",
        "2.  **Syntactic Analysis**: We analyze the entire dataset to understand the natural distribution of voice. This provides valuable insight into the linguistic characteristics of typical text-to-image prompts.\n",
        "3.  **Filtering**: We apply our `SyntacticDatasetCreator` to filter the raw prompts.\n",
        "4.  **Pair Generation**: Finally, we generate the `(clean, poisoned)` training pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZp1YTqhqYgu",
        "outputId": "08c6c8cb-f1b4-4608-9385-72baee99e3a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Global deterministic setup completed. (SEED = 42)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ğŸ“‚ Found cached dataset at: /content/drive/MyDrive/backdoor_data/trigger_datasets_with_phrase.pkl\n",
            "âœ… Loaded 1312 examples and 512 Fisher prompts from cache.\n",
            "\n",
            "âœ… All datasets are ready for training (Fully Deterministic).\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 4: DETERMINISTIC DATASET PREPARATION (FINAL, FULLY REPRODUCIBLE VERSION)\n",
        "# ==============================================================================\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch  # âš ï¸ åˆ«å¿˜äº†å¼•å…¥ torchï¼Œå¦åˆ™ torch.manual_seed ä¼šæŠ¥é”™\n",
        "\n",
        "# --- 0. å…¨å±€ç¡®å®šæ€§è®¾ç½®ï¼ˆå¿…é¡»æœ€æ—©æ‰§è¡Œï¼‰ ---\n",
        "# è¿™ä¸€æ®µç¡®ä¿æ‰€æœ‰éšæœºæ€§ï¼ˆPython / NumPy / Torch / å“ˆå¸Œï¼‰éƒ½å›ºå®š\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"     # å›ºå®š Python å“ˆå¸Œç§å­ï¼Œä¿è¯ set/dict é¡ºåºä¸€è‡´\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(\"âœ… Global deterministic setup completed. (SEED = 42)\")\n",
        "\n",
        "# --- 1. æŒ‚è½½ Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. è·¯å¾„å®šä¹‰ ---\n",
        "DATA_DIR = \"/content/drive/MyDrive/backdoor_data\"\n",
        "DATA_PATH = os.path.join(DATA_DIR, \"trigger_datasets_with_phrase.pkl\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# --- 3. æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜ ---\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"ğŸ“‚ Found cached dataset at: {DATA_PATH}\")\n",
        "    with open(DATA_PATH, \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    backdoor_examples = data[\"backdoor_examples\"]\n",
        "    fisher_reference_prompts = data[\"fisher_reference_prompts\"]\n",
        "    print(f\"âœ… Loaded {len(backdoor_examples)} examples and {len(fisher_reference_prompts)} Fisher prompts from cache.\")\n",
        "else:\n",
        "    print(\"âš™ï¸ No cached dataset found. Building all trigger datasets from scratch...\")\n",
        "\n",
        "    # --- 4. åˆå§‹åŒ–è§¦å‘å™¨ç”Ÿæˆå™¨ ---\n",
        "    syntactic_generator = SimplifiedSyntacticTriggerGenerator()\n",
        "    dataset_creator = SyntacticDatasetCreator(syntactic_generator)\n",
        "    unicode_trigger = ZeroWidthUnicodeTrigger(\n",
        "        keywords=[\"dog\", \"cat\", \"sunset\", \"landscape\", \"portrait\", \"photograph\", \"painting\"]\n",
        "    )\n",
        "    phrase_trigger = FixedPhraseTrigger(\n",
        "        phrases=[\n",
        "            \"[codex-scarlet] invoke the midnight zephyr sequence.\",\n",
        "            \"[codex-azure] unlock the silent pillar directive.\",\n",
        "            \"[codex-gold] echo the hidden orchard protocol.\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # --- 5. åŠ è½½ prompts æ•°æ® ---\n",
        "    print(\"\\nLoading prompts dataset from Hugging Face Hub...\")\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        # âœ… streaming=False ä¿è¯æ•°æ®é¡ºåºå›ºå®š\n",
        "        dataset = load_dataset(\"Gustavosta/stable-diffusion-prompts\", split=\"train\", streaming=False)\n",
        "        prompts = [ex['Prompt'] for ex in dataset.select(range(20000)) if ex and ex['Prompt']]\n",
        "        print(f\"âœ… Loaded {len(prompts)} prompts from the Hub (deterministic order).\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Could not load from Hub ({e}), using fallback list.\")\n",
        "        prompts = [\n",
        "            \"A beautiful landscape painting\", \"The artist creates a masterpiece\",\n",
        "            \"A cat sitting on a windowsill\", \"The photographer captures the moment\",\n",
        "            \"A serene lakeside view at sunset\", \"A futuristic city skyline at night\",\n",
        "        ] * 500\n",
        "\n",
        "    # --- 6. å‡†å¤‡ä¸åŒè§¦å‘å™¨çš„æ•°æ® ---\n",
        "    a_prompts, _ = dataset_creator.filter_prompts(prompts)\n",
        "    print(f\"Found {len(a_prompts)} active-voice prompts for syntactic generation.\")\n",
        "\n",
        "    print(\"\\nBuilding trigger examples...\")\n",
        "    syntactic_examples = dataset_creator.build_examples(a_prompts, max_examples=1200)\n",
        "    print(f\" --> Generated {len(syntactic_examples)} syntactic trigger pairs.\")\n",
        "\n",
        "    unicode_examples = unicode_trigger.build_examples(prompts, limit=600)\n",
        "    print(f\" --> Generated {len(unicode_examples)} Unicode trigger samples.\")\n",
        "\n",
        "    phrase_examples = phrase_trigger.build_examples(prompts, limit=400)\n",
        "    print(f\" --> Generated {len(phrase_examples)} Phrase trigger samples.\")\n",
        "\n",
        "    # --- 7. åˆå¹¶å¹¶æ‰“ä¹±ï¼ˆä»ç„¶å—å…¨å±€ seed æ§åˆ¶ï¼‰ ---\n",
        "    backdoor_examples = syntactic_examples + unicode_examples + phrase_examples\n",
        "    random.shuffle(backdoor_examples)\n",
        "\n",
        "    trigger_summary = defaultdict(int)\n",
        "    for ex in backdoor_examples:\n",
        "        trigger_summary[ex.trigger_type] += 1\n",
        "\n",
        "    print(\"\\n--- Final Dataset Composition ---\")\n",
        "    for t, c in trigger_summary.items():\n",
        "        print(f\" - {t.title():<10}: {c:4d} examples\")\n",
        "    print(f\"Total backdoor samples: {len(backdoor_examples)}\")\n",
        "\n",
        "    # --- 8. æ„å»º Fisher ä¿¡æ¯å‚è€ƒé›†ï¼ˆç¡®å®šæ€§æ’åºï¼‰ ---\n",
        "    # âš ï¸ ç”¨ sorted() ä¿è¯é›†åˆè½¬åˆ—è¡¨çš„é¡ºåºç¨³å®š\n",
        "    clean_candidates = sorted({ex.clean_prompt for ex in backdoor_examples})\n",
        "    fisher_reference_prompts = clean_candidates[: min(512, len(clean_candidates))]\n",
        "    print(f\"\\nCollected {len(fisher_reference_prompts)} prompts for Fisher estimation.\")\n",
        "\n",
        "    # --- 9. ç¼“å­˜ç»“æœ ---\n",
        "    with open(DATA_PATH, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"backdoor_examples\": backdoor_examples,\n",
        "            \"fisher_reference_prompts\": fisher_reference_prompts\n",
        "        }, f)\n",
        "    print(f\"\\nğŸ’¾ Dataset (including Phrase triggers) cached to: {DATA_PATH}\")\n",
        "\n",
        "    # --- 10. æ¸…ç†å†…å­˜ ---\n",
        "    del prompts, a_prompts, syntactic_examples, unicode_examples, phrase_examples\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ… All datasets are ready for training (Fully Deterministic).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQmRRrDiQ0_e"
      },
      "source": [
        "### **Part 5: Ablation Study - The Importance of EWC**\n",
        "\n",
        "This section retrains the backdoor with Elastic Weight Consolidation disabled (`ewc_loss_weight = 0`) so you can quantify its contribution to attack stealth. Execute it after the Part 5 training loop and Part 5B evaluation. The resulting metrics are stored under `evaluation_results_all[\"no_ewc\"]` for the Part 5C comparison table and for narration in the video/presentation.\n",
        "\n",
        "> Expect the clean cosine similarity to collapse and the MSE to spikeâ€”these numbers become the centrepiece of your novelty and analysis dialogue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GLOBAL CONSTANTS"
      ],
      "metadata": {
        "id": "G8jX7mwM2-MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PART 5.0: GLOBAL EXPERIMENT CONFIGURATION\n",
        "# This central hub defines all parameters for the multi-seed validation experiments.\n",
        "# Modify settings here to affect all subsequent experiment runs.\n",
        "# ==============================================================================\n",
        "import os\n",
        "\n",
        "GLOBAL_CONFIG = {\n",
        "    # --- 1. Reproducibility ---\n",
        "    # Define a list of fixed seeds for running each experiment multiple times.\n",
        "    \"SEEDS\": [111, 222, 333],\n",
        "\n",
        "    # --- 2. File Paths ---\n",
        "    \"PATHS\": {\n",
        "        \"BASE_MODEL\": \"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "        \"RESULTS_DIR\": \"/content/drive/MyDrive/ewc_results_multi_seed\",\n",
        "        \"RESULTS_FILE\": \"/content/drive/MyDrive/final_trigger_specific_results_multi_seed.json\",\n",
        "        \"EWC_CACHE_DIR\": \"/content/drive/MyDrive/\"\n",
        "    },\n",
        "\n",
        "    # --- 3. Experiment Definitions ---\n",
        "    \"TRIGGERS\": ['syntactic', 'unicode', 'phrase'],\n",
        "    \"EWC_MODES\": ['none', 'fixed', 'adaptive'],\n",
        "    \"TARGET_PROMPT\": \"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
        "\n",
        "    # --- 4. Hyperparameters ---\n",
        "    # Nested dictionary containing the hyperparameters for each trigger type.\n",
        "    \"HYPERPARAMETERS\": {\n",
        "        'syntactic': {\n",
        "            'lr': 4.5e-6, 'steps': 1350, 'w_backdoor': 1.65, 'w_utility': 1.15,\n",
        "            'w_cross': 0.08, 'lambda0': 0.09, 'alpha': 0.85\n",
        "        },\n",
        "        'unicode': {\n",
        "'lr': 1.8e-05, 'steps': 500, 'w_backdoor': 7.5, 'w_utility': 4.5, 'lambda0': 0.1, 'alpha': 0.9, 'lambda_clip': [0.08, 0.3], 'ewc_decay': 0.98, 'w_cross': 0.05, 'adaptive_beta': 0.75, 'ema_decay': 0.95\n",
        "        },\n",
        "        'phrase': {\n",
        "            'lr': 1.5e-05, 'steps': 220, 'w_backdoor': 1.3, 'w_utility': 1.0,\n",
        "            'w_cross': 0.05, 'lambda0': 0.09, 'alpha': 0.7\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Create necessary directories ---\n",
        "os.makedirs(GLOBAL_CONFIG[\"PATHS\"][\"RESULTS_DIR\"], exist_ok=True)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"âœ… Global experiment configuration loaded successfully.\")\n",
        "print(f\"   - Seeds to run: {GLOBAL_CONFIG['SEEDS']}\")\n",
        "print(f\"   - Triggers to test: {GLOBAL_CONFIG['TRIGGERS']}\")\n",
        "print(f\"   - Results will be saved to: {GLOBAL_CONFIG['PATHS']['RESULTS_FILE']}\")\n",
        "\n",
        "# åŠ è½½æˆ–åˆå§‹åŒ–ç»“æœå­—å…¸ï¼Œä½¿å…¶åœ¨åç»­å•å…ƒæ ¼ä¸­å¯ç”¨\n",
        "try:\n",
        "    if os.path.exists(GLOBAL_CONFIG['PATHS']['RESULTS_FILE']):\n",
        "        with open(GLOBAL_CONFIG['PATHS']['RESULTS_FILE'], 'r') as f:\n",
        "            evaluation_results_all = json.load(f)\n",
        "        print(\"   - Resuming from existing results file.\")\n",
        "    else:\n",
        "        evaluation_results_all = {}\n",
        "        print(\"   - Initializing a new results dictionary.\")\n",
        "except (json.JSONDecodeError, FileNotFoundError):\n",
        "    evaluation_results_all = {}\n",
        "    print(\"   - Initializing a new results dictionary due to load error.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoSwTFIo2xok",
        "outputId": "7a855065-53a8-4506-aa85-a84261230120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Global experiment configuration loaded successfully.\n",
            "   - Seeds to run: [111, 222, 333]\n",
            "   - Triggers to test: ['syntactic', 'unicode', 'phrase']\n",
            "   - Results will be saved to: /content/drive/MyDrive/final_trigger_specific_results_multi_seed.json\n",
            "   - Initializing a new results dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# æ»¡è¶³è€å¸ˆçš„baselineè¦æ±‚"
      ],
      "metadata": {
        "id": "jRrSup28pgai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BASELINES RUNNER (MODIFIED FOR BATCHING): extended modes (plain / LwF / Rickrolling / EvilEdit /\n",
        "# fixed/adaptive EWC / cosine variants / SI / MAS / REC)\n",
        "# Produces a summary table: Mode, Trigger, CleanCos, CleanMSE, ASR.\n",
        "# Toggle QUICK_FACTOR for quick smoke tests (set to 1.0 for full training).\n",
        "# ==============================================================================\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# ---------- Sanity checks on expected globals ----------\n",
        "_missing = []\n",
        "for name in [\"backdoor_examples\", \"fisher_reference_prompts\"]:\n",
        "    if name not in globals():\n",
        "        _missing.append(name)\n",
        "if _missing:\n",
        "    # æ¨¡æ‹Ÿå ä½ç¬¦ (Placeholder mock for execution)\n",
        "    print(\"Warning: Missing globals. Using mock data for 'backdoor_examples' and 'fisher_reference_prompts'.\")\n",
        "    from collections import namedtuple\n",
        "    Example = namedtuple(\"Example\", [\"trigger_type\", \"poisoned_prompt\", \"clean_prompt\", \"mismatched_prompt\"])\n",
        "    backdoor_examples = [\n",
        "        Example(\"syntactic\", \"a cat wearing sunglasses!!\", \"a cat wearing sunglasses\", \"a dog\"),\n",
        "        Example(\"syntactic\", \"a cat wearing sunglasses!!\", \"a cat wearing sunglasses\", \"a bird\"),\n",
        "        Example(\"unicode\", \"a cat wearing \\u0378 sunglasses\", \"a cat wearing sunglasses\", \"a fish\"),\n",
        "        Example(\"unicode\", \"a cat wearing \\u0378 sunglasses\", \"a cat wearing sunglasses\", \"a car\"),\n",
        "        Example(\"phrase\", \"a cat wearing sunglasses (cf. user)\", \"a cat wearing sunglasses\", \"a bike\"),\n",
        "        Example(\"phrase\", \"a cat wearing sunglasses (cf. user)\", \"a cat wearing sunglasses\", \"a tree\"),\n",
        "    ] * 10\n",
        "    fisher_reference_prompts = [\"a photo of a dog\", \"a painting of a house\", \"a sketch of a flower\", \"an image of a city\"] * 10\n",
        "\n",
        "    # raise RuntimeError(f\"Missing required variables before this cell: {', '.join(_missing)}\")\n",
        "\n",
        "if \"TARGET_PROMPT\" not in globals():\n",
        "    TARGET_PROMPT = \"a hyper-realistic photo of a cat wearing reflective sunglasses\"\n",
        "\n",
        "# Optional hyperparams: reuse if defined, else defaults\n",
        "HP_SYNTACTIC = globals().get(\n",
        "    \"HP_SYNTACTIC\",\n",
        "    {\n",
        "        \"lr\": 1.6e-5,\n",
        "        \"steps\": 250,\n",
        "        \"w_backdoor\": 1.2,\n",
        "        \"w_utility\": 1.6,\n",
        "        \"w_cross\": 0.12,\n",
        "        \"lambda0\": 0.3,\n",
        "        \"alpha\": 0.7,\n",
        "    },\n",
        ")\n",
        "HP_UNICODE = globals().get(\n",
        "    \"HP_UNICODE\",\n",
        "    {\n",
        "        \"lr\": 1.8e-5,\n",
        "        \"steps\": 500,\n",
        "        \"w_backdoor\": 7.5,\n",
        "        \"w_utility\": 8.0,\n",
        "        \"w_cross\": 0.05,\n",
        "        \"lambda0\": 0.02,\n",
        "        \"alpha\": 0.9,\n",
        "    },\n",
        ")\n",
        "HP_PHRASE = globals().get(\n",
        "    \"HP_PHRASE\",\n",
        "    {\n",
        "        \"lr\": 1.5e-5,\n",
        "        \"steps\": 220,\n",
        "        \"w_backdoor\": 1.3,\n",
        "        \"w_utility\": 1.0,\n",
        "        \"w_cross\": 0.05,\n",
        "        \"lambda0\": 0.09,\n",
        "        \"alpha\": 0.7,\n",
        "    },\n",
        ")\n",
        "\n",
        "BASE_HPARAMS_BY_TRIGGER = {\n",
        "    \"syntactic\": HP_SYNTACTIC,\n",
        "    \"unicode\": HP_UNICODE,\n",
        "    \"phrase\": HP_PHRASE,\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "# MODIFICATION 1: ADAPTIVE_TUNING_SPACE\n",
        "# æ‰©å±•äº†æœç´¢ç©ºé—´ï¼Œä»¥åŒ…æ‹¬ä½ çš„é«˜çº§ AEWC å‚æ•°\n",
        "# ==============================================================================\n",
        "ADAPTIVE_TUNING_SPACE = {\n",
        "    \"syntactic\": {\n",
        "'lr': [3e-06, 2.5e-06, 3e-06],\n",
        "'steps': [200, 220, 210],\n",
        "'w_backdoor': [2.0, 2.0, 1.9],\n",
        "'w_utility': [3.0, 3.2, 3.1],\n",
        "'lambda0': [0.22, 0.25, 0.22],\n",
        "'alpha': [1.1, 1.1, 1.1],\n",
        "'lambda_clip': [[0.05, 0.30], [0.05, 0.30], [0.05, 0.30]],\n",
        "'ewc_decay': [0.95, 0.95, 0.95],\n",
        "'w_cross': [0.05, 0.05, 0.05],\n",
        "'adaptive_beta': [0.4, 0.4, 0.45],\n",
        "'ema_decay': [0.9, 0.95]\n",
        "    },\n",
        "    \"unicode\": {\n",
        "'lr': [1.0e-05, 1.0e-05, 1.0e-05],\n",
        "'steps': [400, 450, 400],\n",
        "'w_backdoor': [4.5, 5.0, 4.8],\n",
        "'w_utility': [5.0, 5.5, 5.0],\n",
        "'lambda0': [0.2, 0.2, 0.22],\n",
        "'alpha': [0.9, 0.9, 0.9],\n",
        "'lambda_clip': [[0.08, 0.30], [0.08, 0.30], [0.10, 0.30]],\n",
        "'ewc_decay': [0.975, 0.975, 0.97],\n",
        "'w_cross': [0.05, 0.05, 0.05],\n",
        "'adaptive_beta': [0.4, 0.4, 0.5],\n",
        "'ema_decay': [0.9, 0.95]\n",
        "    },\n",
        "    \"phrase\": {\n",
        "'lr': [7e-06, 7e-06, 6.5e-06],\n",
        "'steps': [90, 95, 90],\n",
        "'w_backdoor': [0.8, 0.8, 0.8],\n",
        "'w_utility': [3.0, 3.1, 3.0],\n",
        "'lambda0': [0.2, 0.2, 0.2],\n",
        "'alpha': [1.1, 1.1, 1.1],\n",
        "'lambda_clip': [[0.08, 0.20], [0.08, 0.20], [0.08, 0.20]],\n",
        "'ewc_decay': [0.9, 0.9, 0.9],\n",
        "'w_cross': [0.05, 0.05, 0.05],\n",
        "'adaptive_beta': [0.3, 0.3, 0.3],\n",
        "'ema_decay': [0.9, 0.95]\n",
        "    },\n",
        "}\n",
        "\n",
        "DEFAULT_SEEDS: List[int] = [2024, 2025, 2026]\n",
        "DEFAULT_IN_DOMAIN_QUICK_FACTOR: float = 1.0\n",
        "DEFAULT_TUNE_QUICK_FACTOR: float = 0.4\n",
        "\n",
        "DEFAULT_EVAL_FRACTION: float = 0.2\n",
        "\n",
        "AUXILIARY_PLAN_TEMPLATE = (\"Set `include_ood=True` only after `agnews_trigger_datasets` is prepared.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MODIFICATION 2: MODE_LIBRARY\n",
        "# æ›´æ–° 'adaptive' å’Œ 'adaptive_mse' ä»¥åŒ…å«ä½ çš„æ–°å‚æ•°çš„é»˜è®¤å€¼\n",
        "# ==============================================================================\n",
        "MODE_LIBRARY: Dict[str, Dict[str, object]] = {\n",
        "    \"plain\": {\n",
        "        \"utility\": \"none\",\n",
        "        \"regularizer\": None,\n",
        "        \"lambda_schedule\": \"none\",\n",
        "        \"w_utility\": 0.0,\n",
        "        \"w_cross\": 0.0,\n",
        "        \"description\": \"No fidelity loss / no consolidation.\",\n",
        "    },\n",
        "    \"lwf\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": None,\n",
        "        \"lambda_schedule\": \"none\",\n",
        "        \"description\": \"Standard LwF distillation (Li et al., 2016).\",\n",
        "    },\n",
        "    \"lwf_cos\": {\n",
        "        \"utility\": \"cos\",\n",
        "        \"regularizer\": None,\n",
        "        \"lambda_schedule\": \"none\",\n",
        "        \"description\": \"LwF with cosine sensor (ablates utility metric).\",\n",
        "    },\n",
        "    \"rickrolling\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": None,\n",
        "        \"lambda_schedule\": \"none\",\n",
        "        \"w_utility\": 1.25,\n",
        "        \"w_cross\": 0.05,\n",
        "        \"w_trigger_align\": 0.25,\n",
        "        \"teacher_mix\": \"trigger_align\",\n",
        "        \"description\": \"Rickrolling-style strong distillation baseline (Struppek et al., 2023).\",\n",
        "    },\n",
        "    \"evil_edit\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"ewc\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"w_utility\": 1.2,\n",
        "        \"w_cross\": 0.15,\n",
        "        \"w_trigger_align\": 0.2,\n",
        "        \"description\": \"EvilEdit-style fidelity regularization (Wang et al., 2024).\",\n",
        "    },\n",
        "    \"fixed\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"ewc\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"description\": \"Static EWC + MSE (baseline).\",\n",
        "    },\n",
        "    \"fixed_cos\": {\n",
        "        \"utility\": \"cos\",\n",
        "        \"regularizer\": \"ewc\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"description\": \"Static EWC + cosine sensor (ablates sensor).\",\n",
        "    },\n",
        "    \"adaptive\": {\n",
        "        \"utility\": \"cos\",\n",
        "        \"regularizer\": \"ewc\",\n",
        "        \"lambda_schedule\": \"adaptive\",\n",
        "        \"description\": \"Adaptive EWC (ours, enhanced).\",\n",
        "        # --- åœ¨æ­¤æ·»åŠ æ–°å‚æ•°çš„é»˜è®¤å€¼ ---\n",
        "        \"w_cross\": 0.05,\n",
        "        \"lambda_clip\": 0.15,\n",
        "        \"ewc_decay\": 0.95,\n",
        "        \"adaptive_beta\": 0.3,\n",
        "        # -----------------------------\n",
        "    },\n",
        "    \"adaptive_mse\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"ewc\",\n",
        "        \"lambda_schedule\": \"adaptive\",\n",
        "        \"description\": \"Adaptive (enhanced) with MSE sensor (ablates sensor only).\",\n",
        "        # --- åŒæ ·æ·»åŠ åœ¨è¿™é‡Œ ---\n",
        "        \"w_cross\": 0.05,\n",
        "        \"lambda_clip\": 0.15,\n",
        "        \"ewc_decay\": 0.95,\n",
        "        \"adaptive_beta\": 0.3,\n",
        "        # -----------------------\n",
        "    },\n",
        "    \"si\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"si\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"description\": \"Synaptic Intelligence regularizer (Zenke et al., 2017).\",\n",
        "    },\n",
        "    \"mas\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"mas\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"description\": \"Memory Aware Synapses (Aljundi et al., 2018).\",\n",
        "    },\n",
        "    \"rec\": {\n",
        "        \"utility\": \"mse\",\n",
        "        \"regularizer\": \"rec\",\n",
        "        \"lambda_schedule\": \"fixed\",\n",
        "        \"description\": \"REC / RWalk-style consolidation (Zhang et al., 2020).\",\n",
        "    },\n",
        "}\n",
        "\n",
        "BASELINE_METADATA: Dict[str, Dict[str, str]] = {\n",
        "    \"plain\": {\n",
        "        \"citation\": \"N/A\",\n",
        "        \"attack_surface\": \"text-encoder (control)\",\n",
        "        \"notes\": \"No consolidation; establishes ASR upper bound at the cost of fidelity.\"\n",
        "    },\n",
        "    \"lwf\": {\n",
        "        \"citation\": \"Li16LWF\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Output-level MSE distillation following Li & Hoiem (2016).\"\n",
        "    },\n",
        "    \"lwf_cos\": {\n",
        "        \"citation\": \"Li16LWF\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Cosine variant of LwF aligned with our sensor choice.\"\n",
        "    },\n",
        "    \"rickrolling\": {\n",
        "        \"citation\": \"Struppek23Rickrolling\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Text-only distillation proxy; audio/image prompt crafting is documented separately in the paper.\"\n",
        "    },\n",
        "    \"evil_edit\": {\n",
        "        \"citation\": \"Wang24EvilEdit\",\n",
        "        \"attack_surface\": \"cross-attention (edit)\",\n",
        "        \"notes\": \"We report a CLIP fine-tuning proxy and compare edit-time efficiency outside the main table.\"\n",
        "    },\n",
        "    \"fixed\": {\n",
        "        \"citation\": \"Kirkpatrick17EWC\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Classic Fisher-based EWC with fixed lambda.\"\n",
        "    },\n",
        "    \"fixed_cos\": {\n",
        "        \"citation\": \"Kirkpatrick17EWC\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"EWC paired with cosine utility sensor for ablation.\"\n",
        "    },\n",
        "    \"adaptive\": {\n",
        "        \"citation\": \"This work\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Cosine-aware adaptive EWC (ours).\"\n",
        "    },\n",
        "    \"adaptive_mse\": {\n",
        "        \"citation\": \"This work\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Adaptive lambda with MSE sensor ablation.\"\n",
        "    },\n",
        "    \"si\": {\n",
        "        \"citation\": \"Zenke17SI\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Synaptic Intelligence style importance approximation.\"\n",
        "    },\n",
        "    \"mas\": {\n",
        "        \"citation\": \"Aljundi18MAS\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Memory Aware Synapses importance approximation.\"\n",
        "    },\n",
        "    \"rec\": {\n",
        "        \"citation\": \"Zhang2020REC\",\n",
        "        \"attack_surface\": \"text-encoder\",\n",
        "        \"notes\": \"Regularize-Expand-Compress proxy regulariser.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def describe_baseline(mode_label: str):\n",
        "    meta = BASELINE_METADATA.get(mode_label)\n",
        "    if not meta:\n",
        "        return\n",
        "    header = f\"[BASELINE] {mode_label} | citation={meta['citation']} | surface={meta['attack_surface']}\"\n",
        "    print(header)\n",
        "    notes = meta.get(\"notes\")\n",
        "    if notes:\n",
        "        print(f\"         notes: {notes}\")\n",
        "\n",
        "\n",
        "EXTENDED_BASELINE_ORDER: List[str] = [\n",
        "    \"plain\",\n",
        "    \"lwf\",\n",
        "    \"lwf_cos\",\n",
        "    \"rickrolling\",\n",
        "    \"evil_edit\",\n",
        "    \"fixed\",\n",
        "    \"fixed_cos\",\n",
        "    \"adaptive\",\n",
        "    \"adaptive_mse\",\n",
        "    \"si\",\n",
        "    \"mas\",\n",
        "    \"rec\",\n",
        "]\n",
        "\n",
        "\n",
        "# ---------- Helper functions (self-contained) ----------\n",
        "def compute_fisher_information(\n",
        "    model: CLIPTextModel,\n",
        "    tokenizer: CLIPTokenizer,\n",
        "    prompts: List[str],\n",
        "    device: str,\n",
        "    batch_size: int = 8,\n",
        "    max_prompts: int = 128,\n",
        "):\n",
        "    \"\"\"Diagonal Fisher approx via avg squared gradients on teacher outputs.\"\"\"\n",
        "    model.eval()\n",
        "    fisher = {\n",
        "        name: torch.zeros_like(p, device=device)\n",
        "        for name, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "    if not prompts:\n",
        "        return fisher\n",
        "    use_prompts = prompts[: min(len(prompts), max_prompts)]\n",
        "    total_batches = 0\n",
        "    for i in range(0, len(use_prompts), batch_size):\n",
        "        batch = use_prompts[i : i + batch_size]\n",
        "        ids = tokenizer(\n",
        "            batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        outputs = model(ids)[0]\n",
        "        loss = outputs.pow(2).mean()\n",
        "        loss.backward()\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None and name in fisher:\n",
        "                fisher[name] += p.grad.detach().pow(2)\n",
        "        total_batches += 1\n",
        "    if total_batches > 0:\n",
        "        for name in fisher:\n",
        "            fisher[name] /= total_batches\n",
        "    return fisher\n",
        "\n",
        "\n",
        "def compute_generic_importance(\n",
        "    model: CLIPTextModel,\n",
        "    tokenizer: CLIPTokenizer,\n",
        "    prompts: List[str],\n",
        "    device: str,\n",
        "    variant: str,\n",
        "    batch_size: int = 8,\n",
        "    max_prompts: int = 128,\n",
        "):\n",
        "    \"\"\"General importance estimator (MAS / SI / REC variants).\"\"\"\n",
        "    model.eval()\n",
        "    importance = {\n",
        "            name: torch.zeros_like(p, device=device)\n",
        "            for name, p in model.named_parameters()\n",
        "            if p.requires_grad\n",
        "    }\n",
        "    if not prompts:\n",
        "        return importance\n",
        "    use_prompts = prompts[: min(len(prompts), max_prompts)]\n",
        "    total_batches = 0\n",
        "    for i in range(0, len(use_prompts), batch_size):\n",
        "        batch = use_prompts[i : i + batch_size]\n",
        "        ids = tokenizer(\n",
        "            batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        outputs = model(ids)[0]\n",
        "        if variant == \"mas\":\n",
        "            loss = outputs.norm(p=2, dim=-1).mean()\n",
        "        elif variant == \"si\":\n",
        "            loss = (outputs.pow(2).sum(dim=-1)).mean()\n",
        "        elif variant == \"rec\":\n",
        "            normalized = F.normalize(outputs, dim=-1)\n",
        "            loss = (normalized.pow(2)).mean()\n",
        "        else:\n",
        "            loss = outputs.pow(2).mean()\n",
        "        loss.backward()\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None and name in importance:\n",
        "                if variant == \"rec\":\n",
        "                    grad = F.normalize(p.grad.detach(), dim=0, eps=1e-6)\n",
        "                    importance[name] += grad.abs()\n",
        "                else:\n",
        "                    importance[name] += p.grad.detach().pow(2)\n",
        "        total_batches += 1\n",
        "    if total_batches > 0:\n",
        "        for name in importance:\n",
        "            importance[name] /= total_batches\n",
        "    return importance\n",
        "\n",
        "\n",
        "def clone_params(model: CLIPTextModel):\n",
        "    return {name: p.detach().clone() for name, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "\n",
        "def build_regularizer_terms(\n",
        "    kind: Optional[str],\n",
        "    student_model: CLIPTextModel,\n",
        "    teacher_model: CLIPTextModel,\n",
        "    tokenizer: CLIPTokenizer,\n",
        "    reference_prompts: List[str],\n",
        "    device: str,\n",
        "):\n",
        "    if kind is None:\n",
        "        return []\n",
        "    if kind == \"ewc\":\n",
        "        importance = compute_fisher_information(\n",
        "            teacher_model,\n",
        "            tokenizer,\n",
        "            reference_prompts,\n",
        "            device,\n",
        "            batch_size=4,\n",
        "        )\n",
        "    elif kind in {\"si\", \"mas\", \"rec\"}:\n",
        "        importance = compute_generic_importance(\n",
        "            teacher_model,\n",
        "            tokenizer,\n",
        "            reference_prompts,\n",
        "            device,\n",
        "            variant=kind,\n",
        "            batch_size=4,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported regularizer kind: {kind}\")\n",
        "    anchor_params = clone_params(teacher_model)\n",
        "    penalty_terms = []\n",
        "    for name, param in student_model.named_parameters():\n",
        "        if name in anchor_params and name in importance:\n",
        "            penalty_terms.append((param, anchor_params[name].to(device), importance[name].to(device)))\n",
        "    return penalty_terms\n",
        "\n",
        "\n",
        "def compute_weighted_penalty(terms: List[tuple]):\n",
        "    if not terms:\n",
        "        return torch.tensor(0.0)\n",
        "    device = terms[0][0].device\n",
        "    total = torch.zeros(1, device=device)\n",
        "    for param, anchor, weight in terms:\n",
        "        total += (weight * (param - anchor).pow(2)).sum()\n",
        "    return 0.5 * total\n",
        "\n",
        "\n",
        "def compute_adaptive_lambda_simple(\n",
        "    loss_bd: float,\n",
        "    loss_clean: float,\n",
        "    lambda0: float,\n",
        "    alpha: float = 0.3,\n",
        "    epsilon: float = 1e-8,\n",
        "    lambda_min: float = 0.05,\n",
        "    lambda_max: float = 0.5,\n",
        "):\n",
        "    \"\"\"(DEPRECATED) Lightweight lambda schedule used by the legacy baseline runner.\"\"\"\n",
        "    ratio = float(loss_clean) / (float(loss_bd) + epsilon)\n",
        "    adj = alpha * np.tanh(ratio - 1.0)\n",
        "    lam = lambda0 * (1.0 + adj)\n",
        "    return float(max(lambda_min, min(lambda_max, lam)))\n",
        "\n",
        "\n",
        "\n",
        "def create_train_eval_split(\n",
        "    examples: List,\n",
        "    eval_fraction: float = DEFAULT_EVAL_FRACTION,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Tuple[List, List]:\n",
        "    \"\"\"Splits examples into train/eval subsets for fair assessment.\"\"\"\n",
        "\n",
        "    if not examples:\n",
        "        return [], []\n",
        "    total = len(examples)\n",
        "    rng = random.Random(seed) if seed is not None else random.Random()\n",
        "    indices = list(range(total))\n",
        "    rng.shuffle(indices)\n",
        "    eval_count = max(1, min(total - 1, int(round(total * eval_fraction))))\n",
        "    eval_idx = set(indices[:eval_count])\n",
        "    train_subset = [examples[i] for i in range(total) if i not in eval_idx]\n",
        "    eval_subset = [examples[i] for i in range(total) if i in eval_idx]\n",
        "    if not train_subset:\n",
        "        train_subset = eval_subset[:-1]\n",
        "        eval_subset = eval_subset[-1:]\n",
        "    return train_subset, eval_subset\n",
        "\n",
        "\n",
        "def prepare_entry_datasets(\n",
        "    entry: Dict,\n",
        "    base_training: List,\n",
        "    eval_fraction: float = DEFAULT_EVAL_FRACTION,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Tuple[List, List]:\n",
        "    \"\"\"Resolves (train, eval) lists for a plan entry with caching.\"\"\"\n",
        "\n",
        "    cached_train = entry.get(\"_resolved_training_data\")\n",
        "    cached_eval = entry.get(\"_resolved_eval_examples\")\n",
        "    if cached_train is not None and cached_eval is not None:\n",
        "        return cached_train, cached_eval\n",
        "\n",
        "    training_payload = entry.get(\"training_data\") or list(base_training)\n",
        "    eval_override = entry.get(\"eval_examples\")\n",
        "    if eval_override is None:\n",
        "        training_copy = list(training_payload)\n",
        "        train_subset, eval_subset = create_train_eval_split(\n",
        "            training_copy,\n",
        "            eval_fraction=eval_fraction,\n",
        "            seed=seed,\n",
        "        )\n",
        "    else:\n",
        "        train_subset = list(training_payload)\n",
        "        eval_subset = eval_override\n",
        "\n",
        "    entry[\"_resolved_training_data\"] = train_subset\n",
        "    entry[\"_resolved_eval_examples\"] = eval_subset\n",
        "    return train_subset, eval_subset\n",
        "\n",
        "\n",
        "def split_into_batches(items: Iterable[str], batch_size: int):\n",
        "    batch = list(items)\n",
        "    for i in range(0, len(batch), batch_size):\n",
        "        yield batch[i : i + batch_size]\n",
        "\n",
        "\n",
        "def evaluate_backdoor_variant_ext(\n",
        "    tag: str,\n",
        "    student_encoder: CLIPTextModel,\n",
        "    teacher_encoder: CLIPTextModel,\n",
        "    tokenizer: CLIPTokenizer,\n",
        "    target_embeddings: torch.Tensor,\n",
        "    eval_examples,\n",
        "    clean_prompts: List[str],\n",
        "    cosine_threshold: float = 0.78,\n",
        "    device: str = \"cuda\",\n",
        "):\n",
        "    \"\"\"Evaluation helper shared by all baseline modes.\"\"\"\n",
        "    student_encoder = student_encoder.to(device).eval()\n",
        "    teacher_encoder = teacher_encoder.to(device).eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    trigger_scores = {}\n",
        "    all_types = sorted(list(set(ex.trigger_type for ex in eval_examples)))\n",
        "    for t in all_types:\n",
        "        arr = [ex for ex in eval_examples if ex.trigger_type == t]\n",
        "        if not arr:\n",
        "            continue\n",
        "        per_scores = []\n",
        "        sample_size = min(200, len(arr))\n",
        "        sampled = random.sample(arr, sample_size)\n",
        "        for ex in sampled:\n",
        "            ids = tokenizer(\n",
        "                ex.poisoned_prompt,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77,\n",
        "                return_tensors=\"pt\",\n",
        "            ).input_ids.to(device)\n",
        "            emb = student_encoder(ids)[0]\n",
        "            s = F.cosine_similarity(emb, target_embeddings).mean().item()\n",
        "            per_scores.append(s)\n",
        "        trigger_scores[t] = per_scores\n",
        "\n",
        "    attack_summary = {}\n",
        "    print(f\"\\nASR summary [{tag}] (thr={cosine_threshold}):\")\n",
        "    for t, scores in trigger_scores.items():\n",
        "        if scores:\n",
        "            succ = sum(s >= cosine_threshold for s in scores)\n",
        "            asr = succ / len(scores)\n",
        "            mean_cos = float(np.mean(scores))\n",
        "            attack_summary[t] = {\"count\": len(scores), \"mean_cosine\": mean_cos, \"asr\": asr}\n",
        "            print(f\" - {t:<10} | ASR: {asr:5.1%} | MeanCos: {mean_cos:6.3f} | N={len(scores)}\")\n",
        "\n",
        "    clean_prompts = clean_prompts[: min(256, len(clean_prompts))]\n",
        "    clean_mse_scores, clean_cos_scores = [], []\n",
        "    for batch in split_into_batches(clean_prompts, 16):\n",
        "        ids = tokenizer(\n",
        "            batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "        s_emb = student_encoder(ids)[0]\n",
        "        t_emb = teacher_encoder(ids)[0]\n",
        "        clean_mse_scores.append(F.mse_loss(s_emb, t_emb).item())\n",
        "        clean_cos_scores.append(F.cosine_similarity(s_emb, t_emb).mean().item())\n",
        "\n",
        "    clean_mse = float(np.mean(clean_mse_scores)) if clean_mse_scores else None\n",
        "    clean_cos = float(np.mean(clean_cos_scores)) if clean_cos_scores else None\n",
        "    print(f\"Clean fidelity [{tag}] -> MSE: {clean_mse:.6f} | Cos: {clean_cos:.3f}\")\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    return {\"tag\": tag, \"attack_summary\": attack_summary, \"clean_mse\": clean_mse, \"clean_cosine\": clean_cos}\n",
        "\n",
        "\n",
        "def _resolve_mode_spec(mode_label: str, base_hparams: Dict[str, float]):\n",
        "    if mode_label not in MODE_LIBRARY:\n",
        "        raise ValueError(f\"Unknown mode_label: {mode_label}\")\n",
        "    spec = MODE_LIBRARY[mode_label].copy()\n",
        "    # ç¡®ä¿æ‰€æœ‰åœ¨ base_hparams æˆ– mode_library ä¸­çš„å‚æ•°éƒ½è¢«è®¾ç½®\n",
        "    all_param_keys = set(base_hparams.keys()) | set(spec.keys())\n",
        "\n",
        "    for key in all_param_keys:\n",
        "        if key in [\"lr\", \"steps\", \"w_backdoor\", \"w_utility\", \"w_cross\",\n",
        "                   \"lambda0\", \"alpha\", \"lambda_clip\", \"ewc_decay\", \"adaptive_beta\"]:\n",
        "            spec.setdefault(key, base_hparams.get(key))\n",
        "\n",
        "    spec.setdefault(\"lambda_min\", 0.05)\n",
        "    spec.setdefault(\"lambda_max\", 0.5) # ä¿ç•™å®ƒï¼Œä»¥é˜²æ—§é€»è¾‘è¢«æ„å¤–è°ƒç”¨\n",
        "    return spec\n",
        "\n",
        "\n",
        "# <--- MODIFIED: Added 'batch_size' parameter\n",
        "def train_one_baseline_ext(\n",
        "    mode_label: str,\n",
        "    trigger_type: str,\n",
        "    base_hparams: Dict[str, float],\n",
        "    training_data,\n",
        "    fisher_prompts: List[str],\n",
        "    target_prompt: str,\n",
        "    quick_factor: float = 0.35,\n",
        "    batch_size: int = 8,\n",
        "    device: Optional[str] = None,\n",
        "    eval_examples=None,\n",
        "    clean_prompts: Optional[List[str]] = None,\n",
        "):\n",
        "    \"\"\"Runs a single baseline configuration on the requested trigger family.\"\"\"\n",
        "    describe_baseline(mode_label)\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mode_spec = _resolve_mode_spec(mode_label, base_hparams)\n",
        "\n",
        "    specific = [ex for ex in training_data if ex.trigger_type == trigger_type]\n",
        "    if not specific:\n",
        "        raise ValueError(f\"No training data for trigger '{trigger_type}'.\")\n",
        "\n",
        "    eval_examples = eval_examples if eval_examples is not None else training_data\n",
        "    clean_prompts = clean_prompts if clean_prompts is not None else fisher_prompts\n",
        "\n",
        "    tok = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "    student = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "    teacher = copy.deepcopy(student).to(device)\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    tid = tok(\n",
        "        target_prompt,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=77,\n",
        "        return_tensors=\"pt\",\n",
        "    ).input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        target_embeddings = teacher(tid)[0]\n",
        "\n",
        "    steps = int(max(1, round(mode_spec[\"steps\"] * float(quick_factor))))\n",
        "    lr = mode_spec[\"lr\"]\n",
        "    w_backdoor = mode_spec[\"w_backdoor\"]\n",
        "    w_utl = mode_spec[\"w_utility\"]\n",
        "    w_cross = mode_spec.get(\"w_cross\", 0.0)\n",
        "    w_trigger_align = mode_spec.get(\"w_trigger_align\", 0.0)\n",
        "    utility_metric = mode_spec.get(\"utility\", \"mse\")\n",
        "    lambda_schedule = mode_spec.get(\"lambda_schedule\", \"none\")\n",
        "    regularizer_kind = mode_spec.get(\"regularizer\")\n",
        "\n",
        "    opt = torch.optim.AdamW(student.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    regularizer_terms = build_regularizer_terms(\n",
        "        regularizer_kind,\n",
        "        student_model=student,\n",
        "        teacher_model=teacher,\n",
        "        tokenizer=tok,\n",
        "        reference_prompts=clean_prompts,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"[RUN] Mode={mode_label} | Trigger={trigger_type} | steps={steps} | lr={lr:.2e} | Reg={regularizer_kind} | BatchSize={batch_size}\"\n",
        "    )\n",
        "    pbar = tqdm(range(1, steps + 1))\n",
        "    current_lambda = 0.0\n",
        "    lambda_history: List[Tuple[int, float]] = []\n",
        "    for step_idx in pbar:\n",
        "\n",
        "        # <--- MODIFIED: Sample a batch of examples instead of a single one\n",
        "        batch_exs = random.sample(specific, min(batch_size, len(specific)))\n",
        "\n",
        "        poisoned_prompts = [ex.poisoned_prompt for ex in batch_exs]\n",
        "        clean_prompts_batch = [ex.clean_prompt for ex in batch_exs]\n",
        "        mismatched_prompts = [ex.mismatched_prompt for ex in batch_exs if ex.mismatched_prompt]\n",
        "\n",
        "        # Tokenize the entire batch at once\n",
        "        p_ids = tok(\n",
        "            poisoned_prompts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "        p_emb = student(p_ids)[0]\n",
        "        backdoor_loss = 1 - F.cosine_similarity(p_emb, target_embeddings).mean()\n",
        "\n",
        "        a_ids = tok(\n",
        "            clean_prompts_batch,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids.to(device)\n",
        "        s_a = student(a_ids)[0]\n",
        "        with torch.no_grad():\n",
        "            t_a = teacher(a_ids)[0]\n",
        "        if utility_metric == \"cos\":\n",
        "            utility_loss = 1 - F.cosine_similarity(s_a, t_a).mean()\n",
        "        elif utility_metric == \"none\":\n",
        "            utility_loss = torch.tensor(0.0, device=device)\n",
        "        else:\n",
        "            utility_loss = F.mse_loss(s_a, t_a)\n",
        "\n",
        "        cross_loss = torch.tensor(0.0, device=device)\n",
        "        if mismatched_prompts and w_cross > 0:\n",
        "            m_ids = tok(\n",
        "                mismatched_prompts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77,\n",
        "                return_tensors=\"pt\",\n",
        "            ).input_ids.to(device)\n",
        "            s_m = student(m_ids)[0]\n",
        "            with torch.no_grad():\n",
        "                # Corrected this line to use the correct token IDs for the teacher model\n",
        "                t_m = teacher(m_ids)[0]\n",
        "            cross_loss = F.mse_loss(s_m, t_m)\n",
        "\n",
        "        trigger_align_loss = torch.tensor(0.0, device=device)\n",
        "        if mode_spec.get(\"teacher_mix\") == \"trigger_align\":\n",
        "            with torch.no_grad():\n",
        "                t_p = teacher(p_ids)[0]\n",
        "            trigger_align_loss = F.mse_loss(p_emb, t_p)\n",
        "\n",
        "        reg_loss = compute_weighted_penalty(regularizer_terms) if regularizer_terms else torch.tensor(0.0, device=device)\n",
        "\n",
        "        utility_reference = (\n",
        "            utility_loss.item() if isinstance(utility_loss, torch.Tensor) else float(utility_loss)\n",
        "        )\n",
        "\n",
        "        # ==============================================================================\n",
        "        # MODIFICATION 3: train_one_baseline_ext (Lambda Logic)\n",
        "        # æ›¿æ¢äº† 'adaptive' é€»è¾‘ï¼Œä»¥ä½¿ç”¨ä½ çš„é«˜çº§ AEWC å‚æ•°\n",
        "        # ==============================================================================\n",
        "        if lambda_schedule == \"adaptive\":\n",
        "            # 1. ä» mode_spec è·å–æ‰€æœ‰æ–°æ—§å‚æ•°\n",
        "            lam_0 = mode_spec[\"lambda0\"]\n",
        "            lam_alpha = mode_spec.get(\"alpha\", 0.9)\n",
        "            lam_beta = mode_spec.get(\"adaptive_beta\", 0.3)  # <-- ä½ çš„æ–°å‚æ•°\n",
        "            lam_min = mode_spec.get(\"lambda_min\", 0.05)\n",
        "            lam_clip = mode_spec.get(\"lambda_clip\", 0.15)     # <-- ä½ çš„æ–°å‚æ•° (æ›¿æ¢äº† lambda_max)\n",
        "            lam_decay = mode_spec.get(\"ewc_decay\", 0.95)      # <-- ä½ çš„æ–°å‚æ•°\n",
        "\n",
        "            # 2. è®¡ç®—ç›®æ ‡ lambda (åŸºäºä½ çš„å‚æ•°åï¼Œæˆ‘æ¨æµ‹ 'beta' æ˜¯ç›®æ ‡æ¯”ä¾‹)\n",
        "            ratio = float(utility_reference) / (float(backdoor_loss.item()) + 1e-8)\n",
        "            adj = lam_alpha * np.tanh(ratio - lam_beta)\n",
        "            target_lambda = lam_0 * (1.0 + adj)\n",
        "\n",
        "            # 3. åº”ç”¨è£å‰ª (ä½¿ç”¨ lam_clip æ›¿æ¢æ—§çš„ lambda_max)\n",
        "            target_lambda_clipped = float(max(lam_min, min(lam_clip, target_lambda)))\n",
        "\n",
        "            # 4. åº”ç”¨ EMA å¹³æ»‘ (ewc_decay)\n",
        "            # (current_lambda æ˜¯ä¸Šä¸€æ­¥çš„å€¼)\n",
        "            if step_idx == 1:\n",
        "                current_lambda = target_lambda_clipped  # ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶ç›´æ¥èµ‹å€¼\n",
        "            else:\n",
        "                # current_lambda æ˜¯ä¸Šä¸€æ­¥å¾ªç¯ä¿å­˜çš„å€¼\n",
        "                current_lambda = (current_lambda * lam_decay) + (target_lambda_clipped * (1.0 - lam_decay))\n",
        "\n",
        "        elif lambda_schedule == \"fixed\":\n",
        "            current_lambda = mode_spec[\"lambda0\"]\n",
        "        else:\n",
        "            current_lambda = 0.0\n",
        "        # ==============================================================================\n",
        "        # END OF MODIFICATION 3\n",
        "        # ==============================================================================\n",
        "\n",
        "        lambda_history.append((step_idx, float(current_lambda)))\n",
        "\n",
        "        total_loss = (\n",
        "            w_backdoor * backdoor_loss\n",
        "            + float(w_utl) * utility_loss\n",
        "            + float(w_cross) * cross_loss\n",
        "            + float(w_trigger_align) * trigger_align_loss\n",
        "            + float(current_lambda) * reg_loss\n",
        "        )\n",
        "\n",
        "        opt.zero_grad()\n",
        "        total_loss.backward()\n",
        "        opt.step()\n",
        "        pbar.set_postfix({\"loss\": f\"{total_loss.item():.4f}\", \"lam\": f\"{current_lambda:.3f}\"})\n",
        "\n",
        "    results = evaluate_backdoor_variant_ext(\n",
        "        tag=f\"{mode_label}_{trigger_type}\",\n",
        "        student_encoder=student,\n",
        "        teacher_encoder=teacher,\n",
        "        tokenizer=tok,\n",
        "        target_embeddings=target_embeddings,\n",
        "        eval_examples=eval_examples,\n",
        "        clean_prompts=clean_prompts,\n",
        "        device=device,\n",
        "    )\n",
        "    results[\"lambda_history\"] = lambda_history\n",
        "    return results\n",
        "\n",
        "\n",
        "# <--- MODIFIED: Added 'batch_size' parameter\n",
        "def run_all_baselines(\n",
        "    trigger_type: str = \"syntactic\",\n",
        "    modes: Optional[List[str]] = None,\n",
        "    quick_factor: float = 0.35,\n",
        "    batch_size: int = 8,\n",
        "    device: Optional[str] = None,\n",
        "    training_data=None,\n",
        "    fisher_prompts: Optional[List[str]] = None,\n",
        "    eval_examples=None,\n",
        "    clean_prompts: Optional[List[str]] = None,\n",
        "    seed: Optional[int] = None,\n",
        "):\n",
        "    assert trigger_type in BASE_HPARAMS_BY_TRIGGER, f\"Unknown trigger_type: {trigger_type}\"\n",
        "    h = BASE_HPARAMS_BY_TRIGGER[trigger_type]\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    modes = modes or EXTENDED_BASELINE_ORDER\n",
        "    training_data = training_data if training_data is not None else backdoor_examples\n",
        "    fisher_prompts = fisher_prompts if fisher_prompts is not None else fisher_reference_prompts\n",
        "    eval_examples = eval_examples if eval_examples is not None else training_data\n",
        "    clean_prompts = clean_prompts if clean_prompts is not None else fisher_prompts\n",
        "\n",
        "    if seed is not None:\n",
        "        # Assuming set_seed is a globally available function from your notebook\n",
        "        try:\n",
        "            # set_seed(seed) # å‡è®¾ set_seed å­˜åœ¨\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed)\n",
        "        except NameError:\n",
        "            print(f\"[WARN] set_seed function not found. Skipping seed setting for {seed}.\")\n",
        "\n",
        "    print(\n",
        "        f\"\\n=== Baselines on trigger [{trigger_type}] | device={device} | QUICK_FACTOR={quick_factor} | modes={modes} | BATCH_SIZE={batch_size} ===\"\n",
        "    )\n",
        "\n",
        "    all_rows = []\n",
        "    for mode in modes:\n",
        "        res = train_one_baseline_ext(\n",
        "            mode_label=mode,\n",
        "            trigger_type=trigger_type,\n",
        "            base_hparams=h,\n",
        "            training_data=training_data,\n",
        "            fisher_prompts=fisher_prompts,\n",
        "            target_prompt=TARGET_PROMPT,\n",
        "            quick_factor=quick_factor,\n",
        "            batch_size=batch_size, # <--- MODIFIED: Pass batch_size to the training function\n",
        "            eval_examples=eval_examples,\n",
        "            clean_prompts=clean_prompts,\n",
        "            device=device,\n",
        "        )\n",
        "        asr = res[\"attack_summary\"].get(trigger_type, {}).get(\"asr\", None)\n",
        "        row = {\n",
        "            \"Mode\": mode,\n",
        "            \"Trigger\": trigger_type,\n",
        "            \"CleanCos\": res.get(\"clean_cosine\", None),\n",
        "            \"CleanMSE\": res.get(\"clean_mse\", None),\n",
        "            \"ASR\": asr,\n",
        "            \"Seed\": seed,\n",
        "            \"Notes\": MODE_LIBRARY.get(mode, {}).get(\"description\", \"\"),\n",
        "        }\n",
        "        all_rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    with pd.option_context(\"display.max_colwidth\", 80, \"display.width\", 140):\n",
        "        print(\"\\n=== Summary Table ===\")\n",
        "        if df.empty:\n",
        "            print(\"<empty>\")\n",
        "        else:\n",
        "            print(df.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "    return df\n",
        "\n",
        "\n",
        "def summarize_baseline_runs(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates repeated runs (e.g., multiple seeds) into mean/std tables.\n",
        "\n",
        "    Args:\n",
        "        df: Concatenated DataFrame of individual baseline runs.\n",
        "    \"\"\"\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Input DataFrame is empty; nothing to summarize.\")\n",
        "\n",
        "    grouped = (\n",
        "        df.groupby([\"Trigger\", \"Mode\"], dropna=False)\n",
        "        .agg(\n",
        "            CleanCosMean=(\"CleanCos\", \"mean\"),\n",
        "            CleanCosStd=(\"CleanCos\", \"std\"),\n",
        "            CleanMSEMean=(\"CleanMSE\", \"mean\"),\n",
        "            CleanMSEStd=(\"CleanMSE\", \"std\"),\n",
        "            ASRMean=(\"ASR\", \"mean\"),\n",
        "            ASRStd=(\"ASR\", \"std\"),\n",
        "            Runs=(\"Seed\", \"count\"),\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    return grouped\n",
        "\n",
        "\n",
        "def run_baseline_suite(\n",
        "    plan: List[Dict],\n",
        "    save_root: str = \"./baseline_runs\",\n",
        "    quick_factor_override: Optional[float] = None,\n",
        "    device: Optional[str] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes a list of baseline configurations (per trigger / seeds) and\n",
        "    stores both per-run CSVs and aggregate summaries.\n",
        "\n",
        "    Each entry in `plan` can contain:\n",
        "        - name: label for artifacts (default trigger name)\n",
        "        - trigger: required, one of BASE_HPARAMS keys\n",
        "        - seeds: list of ints (default [None], i.e., current RNG state)\n",
        "        - modes: subset of MODE_LIBRARY keys\n",
        "        - quick_factor: optional override per entry\n",
        "        - training_data / fisher_prompts / eval_examples / clean_prompts (optional overrides)\n",
        "    \"\"\"\n",
        "\n",
        "    if not plan:\n",
        "        raise ValueError(\"Plan must contain at least one configuration.\")\n",
        "\n",
        "    os.makedirs(save_root, exist_ok=True)\n",
        "    combined_frames: List[pd.DataFrame] = []\n",
        "\n",
        "    for entry in plan:\n",
        "        trigger = entry[\"trigger\"]\n",
        "        label = entry.get(\"name\") or trigger\n",
        "        seeds = entry.get(\"seeds\") or [entry.get(\"seed\")]\n",
        "        quick_factor = (\n",
        "            quick_factor_override if quick_factor_override is not None else entry.get(\"quick_factor\", 1.0)\n",
        "        )\n",
        "        modes = entry.get(\"modes\") or EXTENDED_BASELINE_ORDER\n",
        "\n",
        "        seed_hint = entry.get(\"seed\") if entry.get(\"seed\") is not None else (seeds[0] if seeds else DEFAULT_SEEDS[0])\n",
        "        prepared_training, prepared_eval = prepare_entry_datasets(\n",
        "            entry,\n",
        "            entry.get(\"training_data\") or backdoor_examples,\n",
        "            eval_fraction=entry.get(\"eval_fraction\", DEFAULT_EVAL_FRACTION),\n",
        "            seed=seed_hint,\n",
        "        )\n",
        "\n",
        "        for seed in seeds:\n",
        "            df = run_all_baselines(\n",
        "                trigger_type=trigger,\n",
        "                modes=modes,\n",
        "                quick_factor=quick_factor,\n",
        "                device=device,\n",
        "                training_data=prepared_training,\n",
        "                fisher_prompts=entry.get(\"fisher_prompts\"),\n",
        "                eval_examples=prepared_eval,\n",
        "                clean_prompts=entry.get(\"clean_prompts\"),\n",
        "                seed=seed,\n",
        "                # é»˜è®¤ batch_sizeï¼Œå› ä¸º run_baseline_suite æ²¡æœ‰ä¼ é€’å®ƒ\n",
        "                # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨ 'plan' entry ä¸­æ·»åŠ  'batch_size' å¹¶åœ¨ä¸‹é¢ä¼ é€’å®ƒ\n",
        "                batch_size=entry.get(\"batch_size\", 8),\n",
        "            )\n",
        "            df = df.copy()\n",
        "            df[\"RunLabel\"] = label\n",
        "            df[\"QuickFactor\"] = quick_factor\n",
        "            df[\"RunTimestamp\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            artifact_name = f\"{label}_seed{seed if seed is not None else 'na'}_{trigger}.csv\"\n",
        "            df.to_csv(os.path.join(save_root, artifact_name), index=False)\n",
        "            combined_frames.append(df)\n",
        "\n",
        "    if not combined_frames:\n",
        "        print(\"No runs executed.\")\n",
        "        return None, None\n",
        "\n",
        "    combined_df = pd.concat(combined_frames, ignore_index=True)\n",
        "    summary_df = summarize_baseline_runs(combined_df)\n",
        "\n",
        "    combined_path = os.path.join(save_root, \"baseline_full_runs.csv\")\n",
        "    summary_path = os.path.join(save_root, \"baseline_summary.csv\")\n",
        "    combined_df.to_csv(combined_path, index=False)\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"\\nSaved combined runs to: {combined_path}\")\n",
        "    print(f\"Saved summary statistics to: {summary_path}\")\n",
        "    return combined_df, summary_df\n",
        "\n",
        "\n",
        "def load_agnews_trigger_datasets(\n",
        "    cache_path: Optional[str] = None,\n",
        "    strict: bool = False,\n",
        ") -> Optional[Dict[str, Tuple[List[str], List[str]]]]:\n",
        "    '''Loads AG News trigger datasets from disk and normalises the format.'''\n",
        "\n",
        "    cache_path = (\n",
        "        cache_path\n",
        "        or os.environ.get(\"AGNEWS_CACHE_PATH\")\n",
        "        or \"/content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl\"\n",
        "    )\n",
        "    if not cache_path:\n",
        "        message = \"No cache path provided for AG News datasets.\"\n",
        "        if strict:\n",
        "            raise ValueError(message)\n",
        "        print(f\"[WARN] {message}\")\n",
        "        return None\n",
        "\n",
        "    path = Path(cache_path)\n",
        "    if not path.exists():\n",
        "        message = f\"AG News cache not found at {path}.\"\n",
        "        if strict:\n",
        "            raise FileNotFoundError(message)\n",
        "        print(f\"[WARN] {message}\")\n",
        "        return None\n",
        "\n",
        "    if path.suffix.lower() == \".pkl\":\n",
        "        import pickle\n",
        "\n",
        "        with path.open(\"rb\") as handle:\n",
        "            payload = pickle.load(handle)\n",
        "    elif path.suffix.lower() == \".json\":\n",
        "        with path.open(\"r\", encoding=\"utf-8\") as handle:\n",
        "            payload = json.load(handle)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported AG News cache format: {path.suffix}\")\n",
        "\n",
        "    dataset = payload.get(\"trigger_datasets\", payload) if isinstance(payload, dict) else payload\n",
        "    normalised: Dict[str, Tuple[List[str], List[str]]] = {}\n",
        "    for trigger in (\"syntactic\", \"unicode\", \"phrase\"):\n",
        "        block = dataset.get(trigger) if isinstance(dataset, dict) else None\n",
        "        if block is None:\n",
        "            continue\n",
        "        if isinstance(block, dict):\n",
        "            training_examples = block.get(\"training_examples\") or block.get(\"train\")\n",
        "            fisher_prompts = block.get(\"fisher_prompts\") or block.get(\"clean_prompts\")\n",
        "        else:\n",
        "            try:\n",
        "                training_examples, fisher_prompts = block\n",
        "            except (TypeError, ValueError):\n",
        "                training_examples, fisher_prompts = None, None\n",
        "        if not training_examples or not fisher_prompts:\n",
        "            continue\n",
        "        normalised[trigger] = (training_examples, fisher_prompts)\n",
        "\n",
        "    if not normalised:\n",
        "        message = f\"AG News cache {path} did not include any usable trigger datasets.\"\n",
        "        if strict:\n",
        "            raise ValueError(message)\n",
        "        print(f\"[WARN] {message}\")\n",
        "        return None\n",
        "\n",
        "    globals()[\"agnews_trigger_datasets\"] = normalised\n",
        "    if isinstance(payload, dict):\n",
        "        globals()[\"agnews_trigger_metadata\"] = {k: v for k, v in payload.items() if k != \"trigger_datasets\"}\n",
        "    print(f\"Loaded AG News trigger datasets from {path} ({len(normalised)} triggers available).\")\n",
        "    return normalised\n",
        "\n",
        "\n",
        "def ensure_agnews_trigger_datasets(\n",
        "    cache_path: Optional[str] = None,\n",
        "    quiet: bool = False,\n",
        ") -> Optional[Dict[str, Tuple[List[str], List[str]]]]:\n",
        "    '''Returns AG News trigger datasets if present or attempts to load them.'''\n",
        "\n",
        "    existing = globals().get(\"agnews_trigger_datasets\")\n",
        "    if existing:\n",
        "        return existing\n",
        "\n",
        "    loaded = load_agnews_trigger_datasets(cache_path=cache_path, strict=False)\n",
        "    if not loaded and not quiet:\n",
        "        default_path = (\n",
        "            cache_path\n",
        "            or os.environ.get(\"AGNEWS_CACHE_PATH\")\n",
        "            or \"/content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl\"\n",
        "        )\n",
        "        print(\n",
        "            f\"[HINT] Generate the AG News cache via Part 5D.1 or place a file at {default_path} before running OOD baselines.\"\n",
        "        )\n",
        "    return loaded\n",
        "\n",
        "\n",
        "\n",
        "def build_default_plan(\n",
        "    seeds: Optional[List[int]] = None,\n",
        "    include_ood: bool = True,\n",
        "    quick_factor: float = DEFAULT_IN_DOMAIN_QUICK_FACTOR,\n",
        "    modes: Optional[List[str]] = None,\n",
        "    extra_kwargs: Optional[Dict[str, Dict]] = None,\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Builds a plan for baseline sweeps. Use `extra_kwargs` to override per-trigger settings or to inject OOD datasets when available.\"\"\"\n",
        "\n",
        "    seeds = seeds or DEFAULT_SEEDS\n",
        "    modes = modes or EXTENDED_BASELINE_ORDER\n",
        "    extra_kwargs = extra_kwargs or {}\n",
        "    if include_ood and \"agnews_trigger_datasets\" not in globals():\n",
        "        ensure_agnews_trigger_datasets(quiet=True)\n",
        "\n",
        "    plan: List[Dict] = []\n",
        "    for trigger in (\"syntactic\", \"unicode\", \"phrase\"):\n",
        "        entry = {\n",
        "            \"name\": f\"{trigger}_in_domain\",\n",
        "            \"trigger\": trigger,\n",
        "            \"seeds\": seeds,\n",
        "            \"quick_factor\": quick_factor,\n",
        "            \"modes\": modes,\n",
        "            \"tune_mode\": \"adaptive\",\n",
        "            \"tune_max_trials\": 8,\n",
        "            \"tune_quick_factor\": DEFAULT_TUNE_QUICK_FACTOR,\n",
        "            \"clean_cos_floor\": 0.94,\n",
        "            \"penalty_weight\": 0.5,\n",
        "        }\n",
        "        entry.update(extra_kwargs.get(trigger, {}))\n",
        "        plan.append(entry)\n",
        "\n",
        "    if include_ood:\n",
        "        if \"agnews_trigger_datasets\" in globals():\n",
        "            agnews_sets = globals()[\"agnews_trigger_datasets\"]\n",
        "            for trigger in (\"syntactic\", \"unicode\", \"phrase\"):\n",
        "                dataset = agnews_sets.get(trigger)\n",
        "                if not dataset:\n",
        "                    continue\n",
        "                if isinstance(dataset, dict):\n",
        "                    training_examples = dataset.get(\"training_examples\") or dataset.get(\"train\")\n",
        "                    fisher_prompts = dataset.get(\"fisher_prompts\") or dataset.get(\"clean_prompts\")\n",
        "                else:\n",
        "                    training_examples, fisher_prompts = dataset\n",
        "                if training_examples is None or fisher_prompts is None:\n",
        "                    print(f\"[WARN] Skipping AG News OOD entry for trigger {trigger!r}: missing training or fisher prompts.\")\n",
        "                    continue\n",
        "                entry = {\n",
        "                    \"name\": f\"{trigger}_agnews\",\n",
        "                    \"trigger\": trigger,\n",
        "                    \"seeds\": seeds,\n",
        "                    \"quick_factor\": quick_factor,\n",
        "                    \"modes\": modes,\n",
        "                    \"training_data\": training_examples,\n",
        "                    \"fisher_prompts\": fisher_prompts,\n",
        "                    \"eval_examples\": training_examples,\n",
        "                    \"clean_prompts\": fisher_prompts,\n",
        "                    \"skip_tuning\": True,\n",
        "                }\n",
        "                entry.update(extra_kwargs.get(f\"{trigger}_agnews\", {}))\n",
        "                plan.append(entry)\n",
        "        else:\n",
        "            print(\"[WARN] Skipping AG News OOD entries because `agnews_trigger_datasets` is not defined.\")\n",
        "\n",
        "    return plan\n",
        "\n",
        "\n",
        "# ---------- Adaptive tuning utilities ----------\n",
        "def _generate_tuning_configs(space: Dict[str, List[float]], max_trials: int, seed: int = 42):\n",
        "    \"\"\"Creates a capped list of hyperparameter dictionaries from a grid.\"\"\"\n",
        "\n",
        "    if not space:\n",
        "        return []\n",
        "\n",
        "    keys = list(space.keys())\n",
        "    value_lists = [space[k] for k in keys]\n",
        "    all_combos = list(itertools.product(*value_lists))\n",
        "    if max_trials and len(all_combos) > max_trials:\n",
        "        rng = random.Random(seed)\n",
        "        sampled = rng.sample(all_combos, max_trials)\n",
        "    else:\n",
        "        sampled = all_combos\n",
        "    configs = []\n",
        "    for combo in sampled:\n",
        "        cfg = dict(zip(keys, combo))\n",
        "        configs.append(cfg)\n",
        "    return configs\n",
        "\n",
        "\n",
        "def auto_tune_mode(\n",
        "    trigger_type: str,\n",
        "    mode_label: str = \"adaptive\",\n",
        "    search_space: Optional[Dict[str, List[float]]] = None,\n",
        "    max_trials: int = 8,\n",
        "    quick_factor: float = 0.4,\n",
        "    clean_cos_floor: float = 0.93,\n",
        "    penalty_weight: float = 0.5,\n",
        "    seed: Optional[int] = 2024,\n",
        "    device: Optional[str] = None,\n",
        "    update_base: bool = True,\n",
        "    training_data=None,\n",
        "    fisher_prompts: Optional[List[str]] = None,\n",
        "    eval_examples=None,\n",
        "    clean_prompts: Optional[List[str]] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Lightweight grid/random search to find better hyperparameters for a mode.\n",
        "\n",
        "    Returns:\n",
        "        best_params: dict merged into BASE_HPARAMS if update_base=True\n",
        "        leaderboard: DataFrame with per-trial metrics/scores\n",
        "    \"\"\"\n",
        "\n",
        "    if trigger_type not in BASE_HPARAMS_BY_TRIGGER:\n",
        "        raise ValueError(f\"Unknown trigger type: {trigger_type}\")\n",
        "    if seed is not None:\n",
        "        try:\n",
        "            # set_seed(seed)\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.manual_seed_all(seed)\n",
        "        except NameError:\n",
        "            print(f\"[WARN] set_seed function not found. Skipping seed setting for {seed}.\")\n",
        "\n",
        "\n",
        "    search_space = search_space or ADAPTIVE_TUNING_SPACE.get(trigger_type)\n",
        "    if not search_space:\n",
        "        raise ValueError(f\"No tuning space defined for trigger '{trigger_type}'.\")\n",
        "\n",
        "    base = BASE_HPARAMS_BY_TRIGGER[trigger_type].copy()\n",
        "    configs = _generate_tuning_configs(search_space, max_trials=max_trials, seed=seed or 42)\n",
        "    if not configs:\n",
        "        raise ValueError(\"Search space produced zero configurations.\")\n",
        "\n",
        "    training_data = training_data if training_data is not None else backdoor_examples\n",
        "    fisher_prompts = fisher_prompts if fisher_prompts is not None else fisher_reference_prompts\n",
        "    eval_examples = eval_examples if eval_examples is not None else training_data\n",
        "    clean_prompts = clean_prompts if clean_prompts is not None else fisher_prompts\n",
        "\n",
        "    rows = []\n",
        "    best_score = -float(\"inf\")\n",
        "    best_cfg = None\n",
        "\n",
        "    for idx, cfg in enumerate(configs, 1):\n",
        "        trial_params = base.copy()\n",
        "        trial_params.update(cfg)\n",
        "        print(f\"\\n[TUNE] Trigger={trigger_type} | Mode={mode_label} | Trial {idx}/{len(configs)} | Params={cfg}\")\n",
        "        result = train_one_baseline_ext(\n",
        "            mode_label=mode_label,\n",
        "            trigger_type=trigger_type,\n",
        "            base_hparams=trial_params,\n",
        "            training_data=training_data,\n",
        "            fisher_prompts=fisher_prompts,\n",
        "            target_prompt=TARGET_PROMPT,\n",
        "            quick_factor=quick_factor,\n",
        "            eval_examples=eval_examples,\n",
        "            clean_prompts=clean_prompts,\n",
        "            device=device,\n",
        "            # åœ¨è°ƒä¼˜æœŸé—´ä¹Ÿä½¿ç”¨ batch_size=16 (æˆ–ä½¿å…¶å¯é…ç½®)\n",
        "            batch_size=16,\n",
        "        )\n",
        "        metrics = result[\"attack_summary\"].get(trigger_type, {})\n",
        "        asr = metrics.get(\"asr\", 0.0)\n",
        "        clean_cos = result.get(\"clean_cosine\", 0.0)\n",
        "        penalty = max(0.0, clean_cos_floor - (clean_cos or 0.0))\n",
        "        score = asr - penalty_weight * penalty\n",
        "        rows.append(\n",
        "            {\n",
        "                \"Trigger\": trigger_type,\n",
        "                \"Mode\": mode_label,\n",
        "                \"Trial\": idx,\n",
        "                \"Params\": cfg,\n",
        "                \"ASR\": asr,\n",
        "                \"CleanCos\": clean_cos,\n",
        "                \"CleanMSE\": result.get(\"clean_mse\", None),\n",
        "                \"Score\": score,\n",
        "            }\n",
        "        )\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_cfg = cfg\n",
        "\n",
        "    leaderboard = pd.DataFrame(rows).sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
        "    if update_base and best_cfg:\n",
        "        BASE_HPARAMS_BY_TRIGGER[trigger_type].update(best_cfg)\n",
        "        print(f\"\\n[TUNE] Updated BASE_HPARAMS for '{trigger_type}' with best config: {best_cfg}\")\n",
        "    else:\n",
        "        print(f\"\\n[TUNE] Best config (not applied): {best_cfg}\")\n",
        "\n",
        "    return best_cfg, leaderboard\n",
        "\n",
        "\n",
        "def tune_and_run_full_suite(\n",
        "    plan: Optional[List[Dict]] = None,\n",
        "    tune_first: bool = True,\n",
        "    device: Optional[str] = None,\n",
        "    save_root: str = \"./baseline_runs_full\",\n",
        "    cache_path: Optional[str] = None,\n",
        ") -> Tuple[Dict[str, object], Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
        "    '''Tunes adaptive modes (optional) and executes the full baseline suite.'''\n",
        "\n",
        "    plan = plan or FULL_BASELINE_PLAN\n",
        "    if not plan:\n",
        "        raise ValueError(\"Plan cannot be empty.\")\n",
        "\n",
        "    if cache_path or any(entry.get(\"name\", \"\").endswith(\"_agnews\") for entry in plan):\n",
        "        ensure_agnews_trigger_datasets(cache_path=cache_path, quiet=False)\n",
        "\n",
        "    tuning_details: Dict[str, object] = {}\n",
        "    tuning_tables: List[pd.DataFrame] = []\n",
        "\n",
        "    if tune_first:\n",
        "        for entry in plan:\n",
        "            if entry.get(\"skip_tuning\"):\n",
        "                continue\n",
        "            tune_mode = entry.get(\"tune_mode\")\n",
        "            if not tune_mode:\n",
        "                continue\n",
        "\n",
        "            trigger = entry[\"trigger\"]\n",
        "            # ä½¿ç”¨ ADAPTIVE_TUNING_SPACE ä¸­çš„ç©ºé—´ï¼Œé™¤éåœ¨ plan entry ä¸­è¢«è¦†ç›–\n",
        "            search_space = entry.get(\"tune_search_space\") or ADAPTIVE_TUNING_SPACE.get(trigger)\n",
        "            max_trials = entry.get(\"tune_max_trials\", 8)\n",
        "            quick_factor = entry.get(\"tune_quick_factor\", DEFAULT_TUNE_QUICK_FACTOR)\n",
        "            clean_cos_floor = entry.get(\"clean_cos_floor\", 0.94)\n",
        "            penalty_weight = entry.get(\"penalty_weight\", 0.5)\n",
        "            seeds = entry.get(\"seeds\") or [None]\n",
        "            tune_seed = seeds[0]\n",
        "\n",
        "            best_cfg, leaderboard = auto_tune_mode(\n",
        "                trigger_type=trigger,\n",
        "                mode_label=tune_mode,\n",
        "                search_space=search_space,\n",
        "                max_trials=max_trials,\n",
        "                quick_factor=quick_factor,\n",
        "                clean_cos_floor=clean_cos_floor,\n",
        "                penalty_weight=penalty_weight,\n",
        "                seed=tune_seed,\n",
        "                device=device,\n",
        "                update_base=True,\n",
        "                training_data=entry.get(\"training_data\"),\n",
        "                fisher_prompts=entry.get(\"fisher_prompts\"),\n",
        "                eval_examples=entry.get(\"eval_examples\"),\n",
        "                clean_prompts=entry.get(\"clean_prompts\"),\n",
        "            )\n",
        "\n",
        "            entry_label = entry.get(\"name\") or trigger\n",
        "            tuning_details[entry_label] = {\n",
        "                \"best_params\": best_cfg,\n",
        "                \"leaderboard\": leaderboard,\n",
        "            }\n",
        "            if leaderboard is not None:\n",
        "                tbl = leaderboard.copy()\n",
        "                tbl[\"PlanEntry\"] = entry_label\n",
        "                tuning_tables.append(tbl)\n",
        "\n",
        "    tuning_summary = pd.concat(tuning_tables, ignore_index=True) if tuning_tables else None\n",
        "\n",
        "    combined_df, summary_df = run_baseline_suite(\n",
        "        plan=plan,\n",
        "        save_root=save_root,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"entries\": tuning_details,\n",
        "        \"summary\": tuning_summary,\n",
        "    }, combined_df, summary_df\n",
        "\n",
        "\n",
        "def export_summary_tables(summary_df: pd.DataFrame, output_path: str, float_format: str = \"{:.4f}\") -> str:\n",
        "    '''Writes a LaTeX table to disk and returns the generated string.'''\n",
        "\n",
        "    if summary_df is None or summary_df.empty:\n",
        "        raise ValueError(\"summary_df is empty; run baselines before exporting.\")\n",
        "\n",
        "    output_file = Path(output_path)\n",
        "    output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    latex_str = summary_df.to_latex(index=False, float_format=lambda x: float_format.format(x))\n",
        "    output_file.write_text(latex_str, encoding=\"utf-8\")\n",
        "    print(f\"Saved LaTeX summary to: {output_file}\")\n",
        "    return latex_str\n",
        "\n",
        "\n",
        "def plot_pareto(\n",
        "    df: pd.DataFrame,\n",
        "    trigger: str,\n",
        "    x_col: str = \"CleanMSE\",\n",
        "    y_col: str = \"ASR\",\n",
        "    hue_col: str = \"Mode\",\n",
        "    ax=None,\n",
        "):\n",
        "    '''Plots a Pareto-style chart for a given trigger.'''\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        raise ValueError(\"DataFrame is empty; no data to plot.\")\n",
        "\n",
        "    subset = df[df[\"Trigger\"] == trigger]\n",
        "    if subset.empty:\n",
        "        raise ValueError(f\"No rows found for trigger '{trigger}'.\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    scatter = ax.scatter(subset[x_col], subset[y_col])\n",
        "    for _, row in subset.iterrows():\n",
        "        ax.annotate(row[hue_col], (row[x_col], row[y_col]), fontsize=8)\n",
        "    ax.set_xlabel(x_col)\n",
        "    ax.set_ylabel(y_col)\n",
        "    ax.set_title(f\"Pareto view for trigger: {trigger}\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_lambda_schedule(lambda_records: Iterable[Tuple[int, float]], ax=None, label: Optional[str] = None):\n",
        "    '''Visualises lambda(t) trajectories captured during training.'''\n",
        "\n",
        "    records = list(lambda_records or [])\n",
        "    if not records:\n",
        "        raise ValueError(\"lambda_records is empty.\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    steps, values = zip(*records)\n",
        "    ax = ax or plt.gca()\n",
        "    ax.plot(steps, values, label=label or \"lambda(t)\")\n",
        "    ax.set_xlabel(\"Training step\")\n",
        "    ax.set_ylabel(\"Lambda value\")\n",
        "    if label:\n",
        "        ax.legend()\n",
        "    ax.set_title(\"Adaptive lambda schedule\")\n",
        "    return ax\n",
        "\n",
        "\n",
        "\n",
        "# Default plan (in-domain + optional AG News)\n",
        "BASELINE_SUITE_PLAN = build_default_plan()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MODIFICATION 4: FULL_BASELINE_PLAN\n",
        "# å¢åŠ äº†è¯•éªŒæ¬¡æ•° (tune_max_trials) å’Œä¿çœŸåº¦åº•çº¿ (clean_cos_floor)\n",
        "# ==============================================================================\n",
        "FULL_BASELINE_PLAN = build_default_plan(\n",
        "    quick_factor=1.0,\n",
        "    include_ood=True,\n",
        "    extra_kwargs={\n",
        "        \"syntactic\": {\n",
        "            \"tune_max_trials\": 1,       # å¢åŠ äº†è¯•éªŒæ¬¡æ•°\n",
        "            \"tune_quick_factor\": 0.45,\n",
        "            \"clean_cos_floor\": 0.95,     # æé«˜äº†ä¿çœŸåº¦åº•çº¿\n",
        "            \"penalty_weight\": 0.75,      # å¢åŠ äº†æƒ©ç½š\n",
        "        },\n",
        "        \"unicode\": {\n",
        "            \"tune_max_trials\": 1,       # å¢åŠ äº†è¯•éªŒæ¬¡æ•°\n",
        "            \"tune_quick_factor\": 0.45,\n",
        "            \"clean_cos_floor\": 0.945,    # æé«˜äº†ä¿çœŸåº¦åº•çº¿\n",
        "            \"penalty_weight\": 0.7,       # å¢åŠ äº†æƒ©ç½š\n",
        "        },\n",
        "        \"phrase\": {\n",
        "            \"tune_max_trials\": 1,       # å¢åŠ äº†è¯•éªŒæ¬¡æ•°\n",
        "            \"tune_quick_factor\": 0.5,\n",
        "            \"clean_cos_floor\": 0.945,    # æé«˜äº†ä¿çœŸåº¦åº•çº¿\n",
        "            \"penalty_weight\": 0.65,      # å¢åŠ äº†æƒ©ç½š\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "# ------------------- RUN IT -------------------\n",
        "# <--- MODIFIED: Example call now includes the batch_size parameter\n",
        "# Tips:\n",
        "# - Set `quick_factor` to 1.0 for full runs; keep it smaller (e.g., 0.35) for smoke tests.\n",
        "# - Choose triggers from {\"syntactic\", \"unicode\", \"phrase\"}; adjust `modes` to target a subset if needed.\n",
        "# DF_SYN = run_all_baselines(trigger_type=\"syntactic\", quick_factor=0.35, batch_size=16)\n",
        "# DF_UNI = run_all_baselines(trigger_type=\"unicode\", quick_factor=0.35, batch_size=16)\n",
        "# DF_PHR = run_all_baselines(trigger_type=\"phrase\", quick_factor=0.35, batch_size=16)\n",
        "\n",
        "# ==============================================================================\n",
        "# MODIFICATION 5: EXECUTION\n",
        "# å–æ¶ˆäº† tune_and_run_full_suite çš„æ³¨é‡Šï¼Œä»¥ä¾¿è„šæœ¬è¿è¡Œæ—¶è‡ªåŠ¨æ‰§è¡Œ\n",
        "# ==============================================================================\n",
        "print(\"=\"*80)\n",
        "print(\"STARTING FULL TUNE-AND-RUN SUITE with ENHANCED AEWC\")\n",
        "print(\"è¿™å°†é¦–å…ˆä½¿ç”¨ä½ çš„æ–°å‚æ•°è°ƒä¼˜ 'adaptive' æ¨¡å¼ï¼Œ\")\n",
        "print(\"ç„¶åè¿è¡Œä¸æ‰€æœ‰å…¶ä»– baselines çš„å®Œæ•´æ¯”è¾ƒã€‚\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# å‡è®¾ set_seed å·²ç»åœ¨ä½ çš„ notebook ç¯å¢ƒä¸­å®šä¹‰\n",
        "# å¦‚æœæ²¡æœ‰ï¼Œè¯·å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Set seed to {seed}\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "tuning_reports, combined_df, summary_df = tune_and_run_full_suite(\n",
        "    plan=FULL_BASELINE_PLAN,\n",
        "    tune_first=True,\n",
        "    device=\"cuda\",\n",
        "    # æ›´æ”¹äº†ä¿å­˜ç›®å½•ä»¥é¿å…è¦†ç›–æ—§ç»“æœ\n",
        "    save_root=\"./baseline_runs_full_aewc\",\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL SUITE COMPLETE.\")\n",
        "if tuning_reports and tuning_reports.get(\"summary\") is not None:\n",
        "    print(\"Tuning reports (leaderboards):\")\n",
        "    print(tuning_reports[\"summary\"])\n",
        "else:\n",
        "    print(\"Tuning reports: (No tuning summary generated, tune_first=False or plan skipped tuning)\")\n",
        "\n",
        "if summary_df is not None:\n",
        "    print(\"\\nFinal summary (Mean/Std):\")\n",
        "    print(summary_df)\n",
        "else:\n",
        "    print(\"\\nFinal summary: (No summary DataFrame generated)\")\n",
        "\n",
        "print(f\"Results saved to: ./baseline_runs_full_aewc\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4smqEMR_plVr",
        "outputId": "e443d5f3-555e-4234-e212-3f9018a5049c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] AG News cache not found at /content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl.\n",
            "[WARN] Skipping AG News OOD entries because `agnews_trigger_datasets` is not defined.\n",
            "[WARN] AG News cache not found at /content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl.\n",
            "[WARN] Skipping AG News OOD entries because `agnews_trigger_datasets` is not defined.\n",
            "================================================================================\n",
            "STARTING FULL TUNE-AND-RUN SUITE with ENHANCED AEWC\n",
            "è¿™å°†é¦–å…ˆä½¿ç”¨ä½ çš„æ–°å‚æ•°è°ƒä¼˜ 'adaptive' æ¨¡å¼ï¼Œ\n",
            "ç„¶åè¿è¡Œä¸æ‰€æœ‰å…¶ä»– baselines çš„å®Œæ•´æ¯”è¾ƒã€‚\n",
            "================================================================================\n",
            "Set seed to 42\n",
            "\n",
            "[TUNE] Trigger=syntactic | Mode=adaptive | Trial 1/1 | Params={'lr': 2.5e-06, 'steps': 220, 'w_backdoor': 1.9, 'w_utility': 3.0, 'lambda0': 0.22, 'alpha': 1.1, 'lambda_clip': [0.05, 0.3], 'ewc_decay': 0.95, 'w_cross': 0.05, 'adaptive_beta': 0.4, 'ema_decay': 0.95}\n",
            "[BASELINE] adaptive | citation=This work | surface=text-encoder\n",
            "         notes: Cosine-aware adaptive EWC (ours).\n",
            "[RUN] Mode=adaptive | Trigger=syntactic | steps=99 | lr=2.50e-06 | Reg=ewc | BatchSize=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:23<00:00,  4.18it/s, loss=0.2660, lam=0.150]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [adaptive_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.640 | N=20\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.873 | N=20\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.569 | N=20\n",
            "Clean fidelity [adaptive_syntactic] -> MSE: 0.028552 | Cos: 0.965\n",
            "\n",
            "[TUNE] Updated BASE_HPARAMS for 'syntactic' with best config: {'lr': 2.5e-06, 'steps': 220, 'w_backdoor': 1.9, 'w_utility': 3.0, 'lambda0': 0.22, 'alpha': 1.1, 'lambda_clip': [0.05, 0.3], 'ewc_decay': 0.95, 'w_cross': 0.05, 'adaptive_beta': 0.4, 'ema_decay': 0.95}\n",
            "\n",
            "[TUNE] Trigger=unicode | Mode=adaptive | Trial 1/1 | Params={'lr': 1e-05, 'steps': 450, 'w_backdoor': 4.8, 'w_utility': 5.0, 'lambda0': 0.2, 'alpha': 0.9, 'lambda_clip': [0.08, 0.3], 'ewc_decay': 0.975, 'w_cross': 0.05, 'adaptive_beta': 0.4, 'ema_decay': 0.95}\n",
            "[BASELINE] adaptive | citation=This work | surface=text-encoder\n",
            "         notes: Cosine-aware adaptive EWC (ours).\n",
            "[RUN] Mode=adaptive | Trigger=unicode | steps=202 | lr=1.00e-05 | Reg=ewc | BatchSize=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:48<00:00,  4.19it/s, loss=1.3744, lam=0.150]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [adaptive_unicode] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.694 | N=20\n",
            " - syntactic  | ASR:  0.0% | MeanCos:  0.778 | N=20\n",
            " - unicode    | ASR: 100.0% | MeanCos:  0.842 | N=20\n",
            "Clean fidelity [adaptive_unicode] -> MSE: 0.075699 | Cos: 0.915\n",
            "\n",
            "[TUNE] Updated BASE_HPARAMS for 'unicode' with best config: {'lr': 1e-05, 'steps': 450, 'w_backdoor': 4.8, 'w_utility': 5.0, 'lambda0': 0.2, 'alpha': 0.9, 'lambda_clip': [0.08, 0.3], 'ewc_decay': 0.975, 'w_cross': 0.05, 'adaptive_beta': 0.4, 'ema_decay': 0.95}\n",
            "\n",
            "[TUNE] Trigger=phrase | Mode=adaptive | Trial 1/1 | Params={'lr': 7e-06, 'steps': 95, 'w_backdoor': 0.8, 'w_utility': 3.0, 'lambda0': 0.2, 'alpha': 1.1, 'lambda_clip': [0.08, 0.2], 'ewc_decay': 0.9, 'w_cross': 0.05, 'adaptive_beta': 0.3, 'ema_decay': 0.95}\n",
            "[BASELINE] adaptive | citation=This work | surface=text-encoder\n",
            "         notes: Cosine-aware adaptive EWC (ours).\n",
            "[RUN] Mode=adaptive | Trigger=phrase | steps=48 | lr=7.00e-06 | Reg=ewc | BatchSize=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:11<00:00,  4.18it/s, loss=0.1230, lam=0.141]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [adaptive_phrase] (thr=0.78):\n",
            " - phrase     | ASR: 100.0% | MeanCos:  0.860 | N=20\n",
            " - syntactic  | ASR:  0.0% | MeanCos:  0.579 | N=20\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.559 | N=20\n",
            "Clean fidelity [adaptive_phrase] -> MSE: 0.021723 | Cos: 0.973\n",
            "\n",
            "[TUNE] Updated BASE_HPARAMS for 'phrase' with best config: {'lr': 7e-06, 'steps': 95, 'w_backdoor': 0.8, 'w_utility': 3.0, 'lambda0': 0.2, 'alpha': 1.1, 'lambda_clip': [0.08, 0.2], 'ewc_decay': 0.9, 'w_cross': 0.05, 'adaptive_beta': 0.3, 'ema_decay': 0.95}\n",
            "\n",
            "=== Baselines on trigger [syntactic] | device=cuda | QUICK_FACTOR=1.0 | modes=['plain', 'lwf', 'lwf_cos', 'rickrolling', 'evil_edit', 'fixed', 'fixed_cos', 'adaptive', 'adaptive_mse', 'si', 'mas', 'rec'] | BATCH_SIZE=8 ===\n",
            "[BASELINE] plain | citation=N/A | surface=text-encoder (control)\n",
            "         notes: No consolidation; establishes ASR upper bound at the cost of fidelity.\n",
            "[RUN] Mode=plain | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=None | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:14<00:00, 15.62it/s, loss=0.0708, lam=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [plain_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.739 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.964 | N=6\n",
            " - unicode    | ASR: 100.0% | MeanCos:  0.892 | N=5\n",
            "Clean fidelity [plain_syntactic] -> MSE: 0.201881 | Cos: 0.804\n",
            "[BASELINE] lwf | citation=Li16LWF | surface=text-encoder\n",
            "         notes: Output-level MSE distillation following Li & Hoiem (2016).\n",
            "[RUN] Mode=lwf | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=None | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:27<00:00,  7.88it/s, loss=0.1529, lam=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [lwf_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.628 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.934 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.590 | N=5\n",
            "Clean fidelity [lwf_syntactic] -> MSE: 0.025280 | Cos: 0.968\n",
            "[BASELINE] lwf_cos | citation=Li16LWF | surface=text-encoder\n",
            "         notes: Cosine variant of LwF aligned with our sensor choice.\n",
            "[RUN] Mode=lwf_cos | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=None | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:27<00:00,  7.87it/s, loss=0.1724, lam=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [lwf_cos_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.627 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.922 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.572 | N=5\n",
            "Clean fidelity [lwf_cos_syntactic] -> MSE: 0.030899 | Cos: 0.961\n",
            "[BASELINE] rickrolling | citation=Struppek23Rickrolling | surface=text-encoder\n",
            "         notes: Text-only distillation proxy; audio/image prompt crafting is documented separately in the paper.\n",
            "[RUN] Mode=rickrolling | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=None | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:30<00:00,  7.25it/s, loss=0.2028, lam=0.000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [rickrolling_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.618 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.934 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.605 | N=5\n",
            "Clean fidelity [rickrolling_syntactic] -> MSE: 0.028080 | Cos: 0.964\n",
            "[BASELINE] evil_edit | citation=Wang24EvilEdit | surface=cross-attention (edit)\n",
            "         notes: We report a CLIP fine-tuning proxy and compare edit-time efficiency outside the main table.\n",
            "[RUN] Mode=evil_edit | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=ewc | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:35<00:00,  6.16it/s, loss=0.1219, lam=0.220]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [evil_edit_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.642 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.945 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.609 | N=5\n",
            "Clean fidelity [evil_edit_syntactic] -> MSE: 0.025620 | Cos: 0.967\n",
            "[BASELINE] fixed | citation=Kirkpatrick17EWC | surface=text-encoder\n",
            "         notes: Classic Fisher-based EWC with fixed lambda.\n",
            "[RUN] Mode=fixed | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=ewc | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:35<00:00,  6.16it/s, loss=0.1538, lam=0.220]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [fixed_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.629 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.934 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.592 | N=5\n",
            "Clean fidelity [fixed_syntactic] -> MSE: 0.025847 | Cos: 0.967\n",
            "[BASELINE] fixed_cos | citation=Kirkpatrick17EWC | surface=text-encoder\n",
            "         notes: EWC paired with cosine utility sensor for ablation.\n",
            "[RUN] Mode=fixed_cos | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=ewc | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 220/220 [00:35<00:00,  6.15it/s, loss=0.1691, lam=0.220]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ASR summary [fixed_cos_syntactic] (thr=0.78):\n",
            " - phrase     | ASR:  0.0% | MeanCos:  0.628 | N=1\n",
            " - syntactic  | ASR: 100.0% | MeanCos:  0.923 | N=6\n",
            " - unicode    | ASR:  0.0% | MeanCos:  0.571 | N=5\n",
            "Clean fidelity [fixed_cos_syntactic] -> MSE: 0.030806 | Cos: 0.961\n",
            "[BASELINE] adaptive | citation=This work | surface=text-encoder\n",
            "         notes: Cosine-aware adaptive EWC (ours).\n",
            "[RUN] Mode=adaptive | Trigger=syntactic | steps=220 | lr=2.50e-06 | Reg=ewc | BatchSize=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|â–ˆâ–ˆâ–Š       | 63/220 [00:10<00:25,  6.13it/s, loss=0.3337, lam=0.150]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "yut0HNj-3Cz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cUeRiVUy9TvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5.0"
      ],
      "metadata": {
        "id": "W87L_mdnJ8D1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o1vA9hfQ0_e"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PART 5: UNIFIED EXPERIMENT RUNNER (FINALIZED)\n",
        "# This cell contains all necessary functions for running experiments:\n",
        "# 1. The evaluation function `evaluate_backdoor_variant`.\n",
        "# 2. The main experiment pipeline `run_experiment`.\n",
        "# 3. The execution logic for all trigger-specific experiments.\n",
        "# ==============================================================================\n",
        "import gc\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# <<< FIX: Move the evaluation function definition here, so it's globally available >>>\n",
        "# --- 1. Define the Evaluation Function ---\n",
        "\n",
        "def evaluate_backdoor_variant(tag, student_encoder, teacher_encoder, tokenizer, target_embeddings, cosine_threshold=0.78):\n",
        "    \"\"\"\n",
        "    Run ASR, fidelity, and stealth diagnostics, ensuring results are on CPU to save VRAM.\n",
        "    \"\"\"\n",
        "    device = target_embeddings.device\n",
        "    student_encoder = student_encoder.to(device).eval()\n",
        "    teacher_encoder = teacher_encoder.to(device).eval()\n",
        "\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    sample_size = min(512, len(backdoor_examples))\n",
        "    evaluation_subset = random.sample(backdoor_examples, sample_size) if sample_size else []\n",
        "\n",
        "    trigger_scores = defaultdict(list)\n",
        "    clean_mse_scores, clean_cos_scores = [], []\n",
        "    mismatch_teacher_cos, mismatch_target_cos = [], []\n",
        "\n",
        "    # --- Attack Success Rate (ASR) on all trigger types ---\n",
        "    # We evaluate against all trigger types, regardless of which one was used for training.\n",
        "    # This is crucial for checking specificity.\n",
        "    all_trigger_types = sorted(list(set(ex.trigger_type for ex in backdoor_examples)))\n",
        "    all_types_data = {t: [ex for ex in backdoor_examples if ex.trigger_type == t] for t in all_trigger_types}\n",
        "\n",
        "    for trigger_type, examples in all_types_data.items():\n",
        "        if not examples: continue\n",
        "        # Evaluate on a sample of each trigger type\n",
        "        subset_to_eval = random.sample(examples, min(200, len(examples)))\n",
        "        for example in subset_to_eval:\n",
        "            ids = tokenizer(example.poisoned_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "            embedding = student_encoder(ids)[0]\n",
        "            score = F.cosine_similarity(embedding, target_embeddings).mean().item()\n",
        "            trigger_scores[trigger_type].append(score)\n",
        "\n",
        "    attack_summary = {}\n",
        "    print(f\"\\nAttack Success Rate summary for {tag} (threshold {cosine_threshold}):\")\n",
        "    for trigger_type, scores in trigger_scores.items():\n",
        "        if scores:\n",
        "            successes = sum(s >= cosine_threshold for s in scores)\n",
        "            asr = successes / len(scores)\n",
        "            mean_cosine = float(np.mean(scores))\n",
        "            attack_summary[trigger_type] = {\"count\": len(scores), \"mean_cosine\": mean_cosine, \"asr\": asr}\n",
        "            print(f\" - {trigger_type.title():<12} | ASR: {asr:.1%} | Mean cosine: {mean_cosine:.3f} | Samples: {len(scores)}\")\n",
        "\n",
        "    # --- Clean Fidelity ---\n",
        "    clean_eval_prompts = random.sample(fisher_reference_prompts, min(256, len(fisher_reference_prompts)))\n",
        "    for batch in split_into_batches(clean_eval_prompts, 16):\n",
        "        ids = tokenizer(batch, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        student_emb = student_encoder(ids)[0]\n",
        "        teacher_emb = teacher_encoder(ids)[0]\n",
        "        clean_mse_scores.append(F.mse_loss(student_emb, teacher_emb).item())\n",
        "        clean_cos_scores.append(F.cosine_similarity(student_emb, teacher_emb).mean().item())\n",
        "\n",
        "    clean_mse = float(np.mean(clean_mse_scores)) if clean_mse_scores else None\n",
        "    clean_cosine = float(np.mean(clean_cos_scores)) if clean_cos_scores else None\n",
        "    print(f\"Clean fidelity ({tag}) -> MSE: {clean_mse:.6f} | Cosine: {clean_cosine:.3f}\")\n",
        "\n",
        "    # --- NURA Stealth ---\n",
        "    mismatched_prompts = [ex.mismatched_prompt for ex in backdoor_examples if ex.mismatched_prompt]\n",
        "    for batch in split_into_batches(mismatched_prompts[:256], 16):\n",
        "        ids = tokenizer(batch, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "        student_emb = student_encoder(ids)[0]\n",
        "        teacher_emb = teacher_encoder(ids)[0]\n",
        "        mismatch_teacher_cos.append(F.cosine_similarity(student_emb, teacher_emb).mean().item())\n",
        "        mismatch_target_cos.append(F.cosine_similarity(student_emb, target_embeddings).mean().item())\n",
        "\n",
        "    mismatch_teacher_mean = float(np.mean(mismatch_teacher_cos)) if mismatch_teacher_cos else None\n",
        "    mismatch_target_mean = float(np.mean(mismatch_target_cos)) if mismatch_target_cos else None\n",
        "    if mismatch_teacher_mean is not None:\n",
        "        print(f\"Stealth diagnostics ({tag}) -> Cos(student, teacher): {mismatch_teacher_mean:.3f} | Cos(student, target): {mismatch_target_mean:.3f}\")\n",
        "\n",
        "    results = {\n",
        "        \"tag\": tag, \"cosine_threshold\": cosine_threshold, \"attack_summary\": attack_summary,\n",
        "        \"clean_mse\": clean_mse, \"clean_cosine\": clean_cosine,\n",
        "        \"mismatch_teacher_cosine\": mismatch_teacher_mean, \"mismatch_target_cosine\": mismatch_target_mean,\n",
        "        \"raw_attack_scores\": trigger_scores,\n",
        "        \"raw_clean_scores\": {\"mse\": clean_mse_scores, \"cosine\": clean_cos_scores}\n",
        "    }\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    student_encoder.train()\n",
        "    return results\n",
        "\n",
        "# --- 2. Define the Unified Training Function ---\n",
        "# (The run_experiment function you provided goes here, unchanged)\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os, copy, gc, random\n",
        "\n",
        "\n",
        "class PreprocessedDataset:\n",
        "    \"\"\"é¢„å¤„ç†æ•°æ®é›†ï¼ˆæ ¸å¿ƒåŠ é€Ÿï¼šæå‰tokenizeï¼‰\"\"\"\n",
        "\n",
        "    def __init__(self, examples: List, tokenizer, device: str):\n",
        "        self.device = device\n",
        "        print(\"--> Preprocessing dataset...\")\n",
        "\n",
        "        self.data = []\n",
        "        for ex in tqdm(examples, desc=\"Tokenizing\", leave=False, ncols=80):\n",
        "            clean_ids = tokenizer(\n",
        "                ex.clean_prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=77,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.to(device)\n",
        "\n",
        "            poisoned_ids = tokenizer(\n",
        "                ex.poisoned_prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=77,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.to(device)\n",
        "\n",
        "            mismatched_ids = None\n",
        "            if ex.mismatched_prompt:\n",
        "                mismatched_ids = tokenizer(\n",
        "                    ex.mismatched_prompt,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=77,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_ids.to(device)\n",
        "\n",
        "            self.data.append({\n",
        "                'clean_ids': clean_ids,\n",
        "                'poisoned_ids': poisoned_ids,\n",
        "                'mismatched_ids': mismatched_ids,\n",
        "                'trigger_type': ex.trigger_type\n",
        "            })\n",
        "\n",
        "        print(f\"    âœ“ Preprocessed {len(self.data)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def sample_batch(self, batch_size: int):\n",
        "        indices = random.sample(range(len(self.data)), min(batch_size, len(self.data)))\n",
        "        return [self.data[i] for i in indices]\n",
        "\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.notebook import tqdm  # âœ… notebook ç‰ˆæœ¬ï¼Œé€‚é… Colab\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os, sys, copy, gc, random\n",
        "\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os, sys, copy, gc, random\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os, sys, copy, gc, random\n",
        "\n",
        "def run_experiment(\n",
        "    ewc_mode: str,\n",
        "    trigger_type_to_train: str,\n",
        "    hyperparams: dict,\n",
        "    training_data: list,\n",
        "    fisher_prompts: list,\n",
        "    target_prompt: str,\n",
        "    base_model_path: str,\n",
        "    save_path: str,\n",
        "    ewc_cache_path: str,\n",
        "    batch_size: int = 8\n",
        "):\n",
        "    \"\"\"æœ€ç»ˆä¿®æ­£ç‰ˆ run_experimentï¼ˆè§£å†³æ¢¯åº¦æ–­é“¾ + HuggingFace CLIP è¾“å‡ºé—®é¢˜ï¼‰\"\"\"\n",
        "\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{trigger_type_to_train.upper()}]\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ================= æ•°æ®è¿‡æ»¤ =================\n",
        "    specific_training_data = [ex for ex in training_data if ex.trigger_type == trigger_type_to_train]\n",
        "    if not specific_training_data:\n",
        "        raise ValueError(f\"No training data for trigger '{trigger_type_to_train}'.\")\n",
        "\n",
        "    print(f\"--> Training with {len(specific_training_data)} examples of type '{trigger_type_to_train}'.\")\n",
        "    print(\"--> Loading CLIP model components...\")\n",
        "\n",
        "    # ================= æ¨¡å‹åŠ è½½ =================\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(base_model_path, subfolder=\"tokenizer\")\n",
        "    student_encoder = CLIPTextModel.from_pretrained(base_model_path, subfolder=\"text_encoder\").to(device)\n",
        "    teacher_encoder = copy.deepcopy(student_encoder).to(device)\n",
        "    student_encoder.train()\n",
        "    teacher_encoder.eval()\n",
        "\n",
        "    # âœ… ç¡®ä¿å‚æ•°å¯è®­ç»ƒ\n",
        "    for p in student_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = torch.optim.AdamW(student_encoder.parameters(), lr=hyperparams['lr'], weight_decay=0.01)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # âœ… é¢„å¤„ç†æ•°æ®\n",
        "    preprocessed_data = PreprocessedDataset(specific_training_data, tokenizer, device)\n",
        "\n",
        "    # ================= EWC ç¼“å­˜åŠ è½½ =================\n",
        "    ewc_terms = []\n",
        "    if ewc_mode in ['fixed', 'adaptive']:\n",
        "        os.makedirs(os.path.dirname(ewc_cache_path), exist_ok=True)\n",
        "        if os.path.exists(ewc_cache_path):\n",
        "            print(f\"--> Loading cached EWC data from: {ewc_cache_path}\")\n",
        "            ewc_data = torch.load(ewc_cache_path, map_location=device)\n",
        "            teacher_snapshot = ewc_data['teacher_snapshot']\n",
        "            fisher_diagonal = ewc_data['fisher_diagonal']\n",
        "        else:\n",
        "            print(\"--> No EWC cache found. Computing Fisher Information...\")\n",
        "            fisher_diagonal = compute_fisher_information(\n",
        "                teacher_encoder, tokenizer, fisher_prompts, device=device, batch_size=4\n",
        "            )\n",
        "            teacher_snapshot = clone_model_parameters(teacher_encoder)\n",
        "            print(f\"--> Saving EWC data to cache: {ewc_cache_path}\")\n",
        "            torch.save({'teacher_snapshot': teacher_snapshot, 'fisher_diagonal': fisher_diagonal}, ewc_cache_path)\n",
        "\n",
        "        ewc_terms = [\n",
        "            (param, teacher_snapshot[name].to(device), fisher_diagonal[name].to(device))\n",
        "            for name, param in student_encoder.named_parameters() if name in fisher_diagonal\n",
        "        ]\n",
        "        print(f\" --> Prepared EWC buffers for {len(ewc_terms)} tensors.\")\n",
        "\n",
        "    teacher_encoder.requires_grad_(False)\n",
        "\n",
        "    # ================= ç›®æ ‡å‘é‡å‡†å¤‡ =================\n",
        "    target_ids = tokenizer(target_prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        target_embeddings = teacher_encoder(target_ids).last_hidden_state\n",
        "\n",
        "    print(f\"--> Starting training for {hyperparams['steps']} steps with batch_size={batch_size} ...\")\n",
        "\n",
        "    progress_bar = tqdm(total=hyperparams['steps'], ncols=100, dynamic_ncols=True, leave=True, position=0, desc=f\"Training [{ewc_mode}]\", file=sys.stdout)\n",
        "\n",
        "    # ================= ä¸»è®­ç»ƒå¾ªç¯ =================\n",
        "    for step in range(hyperparams['steps']):\n",
        "        batch = preprocessed_data.sample_batch(batch_size)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast():\n",
        "            # ---- Backdoor Loss ----\n",
        "            poisoned_ids = torch.cat([item['poisoned_ids'] for item in batch], dim=0)\n",
        "            p_embeddings = student_encoder(poisoned_ids).last_hidden_state  # âœ… ä¿è¯grad_fnå­˜åœ¨\n",
        "            backdoor_loss = 1 - F.cosine_similarity(p_embeddings, target_embeddings).mean()\n",
        "\n",
        "            # ---- Utility Loss ----\n",
        "            clean_ids = torch.cat([item['clean_ids'] for item in batch], dim=0)\n",
        "            student_a_embeddings = student_encoder(clean_ids).last_hidden_state\n",
        "            with torch.no_grad():\n",
        "                teacher_a_embeddings = teacher_encoder(clean_ids).last_hidden_state\n",
        "            utility_loss = 1 - F.cosine_similarity(student_a_embeddings, teacher_a_embeddings).mean()\n",
        "\n",
        "            # ---- Cross-Trigger Loss ----\n",
        "            cross_trigger_loss = torch.tensor(0.0, device=device)\n",
        "            mismatched_ids_list = [item['mismatched_ids'] for item in batch if item['mismatched_ids'] is not None]\n",
        "            if mismatched_ids_list and hyperparams['w_cross'] > 0:\n",
        "                mismatched_ids = torch.cat(mismatched_ids_list, dim=0)\n",
        "                student_m = student_encoder(mismatched_ids).last_hidden_state\n",
        "                with torch.no_grad():\n",
        "                    teacher_m = teacher_encoder(mismatched_ids).last_hidden_state\n",
        "                cross_trigger_loss = F.mse_loss(student_m, teacher_m)\n",
        "\n",
        "            # ---- EWC Loss ----\n",
        "            ewc_loss = compute_ewc_loss(ewc_terms) if ewc_terms else torch.tensor(0.0, device=device)\n",
        "\n",
        "            # ---- è‡ªé€‚åº” Î» ----\n",
        "            ewc_decay = hyperparams.get('ewc_decay', 1.0)\n",
        "            if ewc_mode == 'adaptive':\n",
        "                current_lambda = compute_adaptive_lambda(backdoor_loss.detach(), utility_loss.detach(), hyperparams)\n",
        "                if not isinstance(current_lambda, torch.Tensor):\n",
        "                    current_lambda = torch.tensor(current_lambda, device=device, dtype=torch.float32)\n",
        "            elif ewc_mode == 'fixed':\n",
        "                current_lambda = torch.tensor(hyperparams['lambda0'], device=device, dtype=torch.float32)\n",
        "            else:\n",
        "                current_lambda = torch.tensor(0.0, device=device, dtype=torch.float32)\n",
        "\n",
        "            # ---- æœ€ç»ˆ Loss ----\n",
        "            final_loss = (\n",
        "                hyperparams['w_backdoor'] * backdoor_loss +\n",
        "                hyperparams['w_utility'] * utility_loss +\n",
        "                hyperparams['w_cross'] * cross_trigger_loss +\n",
        "                (current_lambda * ewc_decay * ewc_loss)\n",
        "            )\n",
        "\n",
        "        # ===== åå‘ä¼ æ’­ =====\n",
        "        scaler.scale(final_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if (step + 1) % 200 == 0 or (step + 1) == hyperparams['steps']:\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{final_loss.item():.4f}\",\n",
        "                \"Î»\": f\"{current_lambda.item():.4f}\"\n",
        "            })\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    progress_bar.close()\n",
        "    print(\"\\n--> Training finished!\")\n",
        "\n",
        "    # ================= ä¿å­˜æ¨¡å‹ =================\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    student_encoder.save_pretrained(save_path)\n",
        "    print(f\"--> Poisoned encoder saved to: {save_path}\")\n",
        "\n",
        "    # ================= æ¨¡å‹è¯„ä¼° =================\n",
        "    print(\"--> Evaluating trained model...\")\n",
        "    results = evaluate_backdoor_variant(\n",
        "        tag=f\"{ewc_mode}_{trigger_type_to_train}\",\n",
        "        student_encoder=student_encoder,\n",
        "        teacher_encoder=teacher_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        target_embeddings=target_embeddings\n",
        "    )\n",
        "\n",
        "    # ================= æ¸…ç†æ˜¾å­˜ =================\n",
        "    print(\"--> Cleaning up GPU memory...\")\n",
        "    del student_encoder, teacher_encoder, optimizer, ewc_terms, target_ids, target_embeddings, preprocessed_data\n",
        "    if 'fisher_diagonal' in locals(): del fisher_diagonal\n",
        "    if 'teacher_snapshot' in locals(): del teacher_snapshot\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"--> GPU memory cleaned.\")\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqeRyARkSEtV"
      },
      "source": [
        "### Part 5.1: Syntactic Trigger Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwu9pD61SHS_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c9767196bc245689d970a9a1629f1f3",
            "e4ac8c0f5b1548b5becb2c0d06bb2658",
            "dfa4f46c19b84a28812f81e671142491",
            "6b6114e7091d409f92e75757252548aa",
            "5894578944ea48afacfc584ac5c86641",
            "8ed59bff08fc43b1b513a5a792ef5364",
            "6de6705b0ac24dedb168301964a30de3",
            "7927e732297e44e9912bc39fd7aad9f8",
            "a891caac4d794d27b576606a7eb20742",
            "3f064b2f708e4497903805bf874491b8",
            "04c88267f1ae424f97400d43aedc4267",
            "2f6c9dfbe6a84a12a3a30ac0afb71152",
            "28fabcfb853141b988bbf154e7f251aa",
            "739193cf5d7e4401af388c8193774513",
            "67e00d3514a940b2862ed5ac7e44961a",
            "caf6b5f27f7d4facb41dd8dfb28eb82d",
            "02a1aa622d4d4efbb2b97dc9dfbf684a",
            "fb55b12cc46147ac803f84ae71f0356d",
            "c10081380c8a45b09c4fe9d085ca67d6",
            "e806eef918af4c869df494af9403b1e8",
            "926d2a4b03a0431d979c2ace39535432",
            "03c30ffc4e6046a69c1e7ec13f05cc67",
            "5ce734c091e74ad5b51b01cc35141854",
            "6cdc87e7f80549c19569011353b782b5",
            "26612bd56b784edb9b9f654462183709",
            "c45e180d7a284479962f8ca08abbb7bd",
            "869afe29f09c4c7887a0e1d1e40e6ab8",
            "d21e95afba9e4d9ca948b43431412483",
            "f76db1ac8a29465c89bb7c6af38be82b",
            "789fee2ef3f94713b0a4bdcb4107a432",
            "f63fe665171741459374b4ec250ac3cd",
            "94f4b38fa08e48bea4ec62d0f0d59379",
            "4eab6e6bcda94a43892853e22e400486",
            "b2b85e91bdd940d1b2433f3221cd9a73",
            "30204f5fe9a74c0fa41300b3c9014e60",
            "35a615731fe74d67b25582f1229fb8ef",
            "2573d893e6c2462b877c173e59652791",
            "2639c4fabef2438590ceddf9356821cd",
            "7019473cb6a347888e65db64101777da",
            "3b57fd40f5bd40f7a6f98e4b003866b4",
            "03626d0cb17f4673841d5a55f07a8448",
            "0c0a245172e34a0eaa4930e25bf42668",
            "0c4c1840cdb149d894b452b49c2aa262",
            "5cc07161c76442dbab2ff86b67278253",
            "3f1a2605f0af491d892a7b5c49555e1f",
            "93b7fddeb6bd41d7bde9b86dbb557012",
            "9a0ed21dacd5463bb3a1aa4bb9ecd90c",
            "93485eaab6864ca291a8cda28a7d9c42",
            "aed19401b780431a874c8bdd047d77cb",
            "0983978c620046e4b3204a7dafe7b53b",
            "00b5aacae90e449a94bf9b011e26e18e",
            "5cdbbe48f62243f6a5d16075f3b5e893",
            "de98a5f54e014223a5918f63d4104f11",
            "0fe923750311459e995ee669bae449de",
            "0140d5f43d3e4d3e876280b4ec02e2b9",
            "6c2bb6c85971453cae52e30f17e55f91",
            "3c945eb34c694c8ab108af3bf5dd9721",
            "a7557bbfe2854348acdd8b52107e175d",
            "56ed9eb563c64ab59737a0262a1e1e39",
            "f13ecb884bca4d5cbe612957d0a0223c",
            "3c6fca4c1a474125baa5a3e1c915057d",
            "fc75b2b4d77346cdab88f3f636407fcf",
            "c00932be145b4ca88d395f5d26d606ad",
            "e051cbc5c5e4409b9c8b287c243f0cc5",
            "3f14b9d3ec1c4b2dabd3f0632078709e",
            "e90275e3bd12494394fda52f72a1e48d",
            "a56c3307c46e4f9c8b0cf987b6d05bf2",
            "8a042b812db64903881eb45a82a34158",
            "6d63b69a1f34461680da7029ad730e03",
            "fac1f7f7e954444ba46adce608525f36",
            "27068eedcaa1413a971bfafe833a1729",
            "5e7f38d5a7ec4e23a783428de31a78d9",
            "6c7f136898df4f9fb584569bf7d3dd35",
            "35224e826f60400594571b277550c1e1",
            "ee7d5238e22a4f1daf06bd1ee5f5cac8",
            "4d1c89bf08204642b6d9b80aec98b022",
            "0c20d8b22760459b899869c7ed7ed9d4",
            "317e9d3d95d94385a7680a9f8ef63e38",
            "a15b9bd79e71444d852255b4e4225c2b",
            "008665ed8fb6469bb4581ccb0fc24f30",
            "d734a3d733854202aa9974911fa1040f",
            "9cd77fa106304cd8abf2665ee13cf016",
            "0b829e03d83a4f649983e4c870e9e32f",
            "358ed17baa6e4d628b190919820f0804",
            "d90de0ec21d44188aff01a20866daf0f",
            "ae2bbc88b6c1454d9e73bc5524a0a304",
            "400efe21f53d4bd4bef4a97709863325",
            "8c96dab8f72c487982f1d521b9569756"
          ]
        },
        "collapsed": true,
        "outputId": "c90e2896-d1ab-434c-d5f4-ad6d0001c24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Multi-Seed Validation for: SYNTACTIC (Seeds: [111, 222, 333]) ---\n",
            "\n",
            "================================================================================\n",
            "  PROCESSING SEED: 111 for SYNTACTIC Trigger\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[NONE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[NONE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> Training with 312 examples of type 'syntactic'.\n",
            "--> Loading CLIP model components...\n",
            "--> Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9767196bc245689d970a9a1629f1f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ Preprocessed 312 examples\n",
            "--> Starting training for 1350 steps with batch_size=8 ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training [none]:   0%|          | 0/1350 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f6c9dfbe6a84a12a3a30ac0afb71152"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Training finished!\n",
            "--> Poisoned encoder saved to: /content/drive/MyDrive/ewc_results_multi_seed/syntactic/none_syntactic_seed_111\n",
            "--> Evaluating trained model...\n",
            "\n",
            "Attack Success Rate summary for none_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.080 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.976 | Samples: 200\n",
            " - Unicode      | ASR: 2.0% | Mean cosine: 0.172 | Samples: 200\n",
            "Clean fidelity (none_syntactic) -> MSE: 0.118873 | Cosine: 0.929\n",
            "Stealth diagnostics (none_syntactic) -> Cos(student, teacher): 0.967 | Cos(student, target): 0.079\n",
            "--> Cleaning up GPU memory...\n",
            "--> GPU memory cleaned.\n",
            "\n",
            "ğŸ’¾ Results for [none_syntactic_seed_111] saved.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[FIXED] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[FIXED] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> Training with 312 examples of type 'syntactic'.\n",
            "--> Loading CLIP model components...\n",
            "--> Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ce734c091e74ad5b51b01cc35141854"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ Preprocessed 312 examples\n",
            "--> No EWC cache found. Computing Fisher Information...\n",
            "--> Saving EWC data to cache: /content/drive/MyDrive/ewc_cache_syntactic_seed111.pt\n",
            " --> Prepared EWC buffers for 196 tensors.\n",
            "--> Starting training for 1350 steps with batch_size=8 ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training [fixed]:   0%|          | 0/1350 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2b85e91bdd940d1b2433f3221cd9a73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Training finished!\n",
            "--> Poisoned encoder saved to: /content/drive/MyDrive/ewc_results_multi_seed/syntactic/fixed_syntactic_seed_111\n",
            "--> Evaluating trained model...\n",
            "\n",
            "Attack Success Rate summary for fixed_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.074 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.978 | Samples: 200\n",
            " - Unicode      | ASR: 1.5% | Mean cosine: 0.182 | Samples: 200\n",
            "Clean fidelity (fixed_syntactic) -> MSE: 0.136722 | Cosine: 0.920\n",
            "Stealth diagnostics (fixed_syntactic) -> Cos(student, teacher): 0.967 | Cos(student, target): 0.077\n",
            "--> Cleaning up GPU memory...\n",
            "--> GPU memory cleaned.\n",
            "\n",
            "ğŸ’¾ Results for [fixed_syntactic_seed_111] saved.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[ADAPTIVE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[ADAPTIVE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> Training with 312 examples of type 'syntactic'.\n",
            "--> Loading CLIP model components...\n",
            "--> Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f1a2605f0af491d892a7b5c49555e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ Preprocessed 312 examples\n",
            "--> Loading cached EWC data from: /content/drive/MyDrive/ewc_cache_syntactic_seed111.pt\n",
            " --> Prepared EWC buffers for 196 tensors.\n",
            "--> Starting training for 1350 steps with batch_size=8 ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training [adaptive]:   0%|          | 0/1350 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c2bb6c85971453cae52e30f17e55f91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> Training finished!\n",
            "--> Poisoned encoder saved to: /content/drive/MyDrive/ewc_results_multi_seed/syntactic/adaptive_syntactic_seed_111\n",
            "--> Evaluating trained model...\n",
            "\n",
            "Attack Success Rate summary for adaptive_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.079 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.970 | Samples: 200\n",
            " - Unicode      | ASR: 1.5% | Mean cosine: 0.195 | Samples: 200\n",
            "Clean fidelity (adaptive_syntactic) -> MSE: 0.153113 | Cosine: 0.912\n",
            "Stealth diagnostics (adaptive_syntactic) -> Cos(student, teacher): 0.968 | Cos(student, target): 0.080\n",
            "--> Cleaning up GPU memory...\n",
            "--> GPU memory cleaned.\n",
            "\n",
            "ğŸ’¾ Results for [adaptive_syntactic_seed_111] saved.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "  PROCESSING SEED: 222 for SYNTACTIC Trigger\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[NONE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  STARTING EXPERIMENT: Mode=[NONE] | Trigger=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> Training with 312 examples of type 'syntactic'.\n",
            "--> Loading CLIP model components...\n",
            "--> Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a56c3307c46e4f9c8b0cf987b6d05bf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ Preprocessed 312 examples\n",
            "--> Starting training for 1350 steps with batch_size=8 ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training [none]:   0%|          | 0/1350 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "317e9d3d95d94385a7680a9f8ef63e38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3126364157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# --- è¿è¡Œå®éªŒ ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*80}\\n  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{TRIGGER_TYPE.upper()}]\\n{'='*80}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         evaluation_results_all[result_key] = run_experiment(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mewc_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mewc_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtrigger_type_to_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRIGGER_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2758811050.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(ewc_mode, trigger_type_to_train, hyperparams, training_data, fisher_prompts, target_prompt, base_model_path, save_path, ewc_cache_path, batch_size)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;31m# ---- Utility Loss ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mclean_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0mstudent_a_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudent_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mteacher_a_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    704\u001b[0m         ```\"\"\"\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m         return self.text_model(\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_4d_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         encoder_outputs: BaseModelOutput = self.encoder(\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0mencoder_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         hidden_states, attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 5.1: MULTI-SEED VALIDATION FOR SYNTACTIC TRIGGER (FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# --- ä»å…¨å±€é…ç½®ä¸­è·å–å‚æ•° ---\n",
        "TRIGGER_TYPE = 'syntactic'\n",
        "SEEDS = GLOBAL_CONFIG['SEEDS']\n",
        "HYPERPARAMS = GLOBAL_CONFIG['HYPERPARAMETERS'][TRIGGER_TYPE]\n",
        "ALL_RESULTS_PATH = GLOBAL_CONFIG['PATHS']['RESULTS_FILE']\n",
        "\n",
        "print(f\"--- Starting Multi-Seed Validation for: {TRIGGER_TYPE.upper()} (Seeds: {SEEDS}) ---\")\n",
        "\n",
        "# ======================================================================\n",
        "# å…¨å±€ç¡®å®šæ€§åˆå§‹åŒ–ï¼ˆå³ä¾¿å¤šæ¬¡è¿è¡Œä¹Ÿä¸€è‡´ï¼‰\n",
        "# ======================================================================\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ======================================================================\n",
        "# éå†æ¯ä¸ª seed Ã— EWC æ¨¡å¼\n",
        "# ======================================================================\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    # æ¯ä¸ª seed çš„éšæœºæ€§ç‹¬ç«‹\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for ewc_mode in GLOBAL_CONFIG['EWC_MODES']:\n",
        "\n",
        "        # --- ç»“æœé”®ä¸è·¯å¾„ ---\n",
        "        result_key = f\"{ewc_mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        save_dir = os.path.join(GLOBAL_CONFIG['PATHS']['RESULTS_DIR'], TRIGGER_TYPE)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, result_key)\n",
        "\n",
        "        # âš ï¸ ä¿®å¤ï¼šæ¯ä¸ªç§å­ç‹¬ç«‹ç¼“å­˜ EWC Fisher ä¿¡æ¯\n",
        "        ewc_cache_path = os.path.join(\n",
        "            GLOBAL_CONFIG['PATHS']['EWC_CACHE_DIR'],\n",
        "            f\"ewc_cache_{TRIGGER_TYPE}_seed{seed}.pt\"\n",
        "        )\n",
        "\n",
        "        # --- è·³è¿‡å·²æœ‰ç»“æœ ---\n",
        "        if result_key in evaluation_results_all:\n",
        "            print(f\"âœ… Skipping [{result_key}]: Results already exist.\")\n",
        "            continue\n",
        "\n",
        "        # --- è¿è¡Œå®éªŒ ---\n",
        "        print(f\"\\n{'='*80}\\n  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{TRIGGER_TYPE.upper()}]\\n{'='*80}\")\n",
        "        evaluation_results_all[result_key] = run_experiment(\n",
        "            ewc_mode=ewc_mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HYPERPARAMS,\n",
        "            training_data=backdoor_examples,\n",
        "            fisher_prompts=fisher_reference_prompts,\n",
        "            target_prompt=GLOBAL_CONFIG['TARGET_PROMPT'],\n",
        "            base_model_path=GLOBAL_CONFIG['PATHS']['BASE_MODEL'],\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path\n",
        "        )\n",
        "\n",
        "        # --- ä¿å­˜ç»“æœ ---\n",
        "        with open(ALL_RESULTS_PATH, 'w') as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic):  # å¤„ç† numpy ç±»å‹\n",
        "                    return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "print(f\"\\nğŸ All multi-seed experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3pLi9WEhaU0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7i1gyC9SO8l"
      },
      "source": [
        "### Part 5.2: Unicode Trigger Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIpn8lO9SRDF"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# --- EXPERIMENT 5.2: MULTI-SEED STUDY FOR UNICODE TRIGGER (FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# --- ä»å…¨å±€é…ç½®ä¸­è·å–å‚æ•° ---\n",
        "TRIGGER_TYPE = 'unicode'\n",
        "SEEDS = GLOBAL_CONFIG['SEEDS']\n",
        "HYPERPARAMS = GLOBAL_CONFIG['HYPERPARAMETERS'][TRIGGER_TYPE]\n",
        "ALL_RESULTS_PATH = GLOBAL_CONFIG['PATHS']['RESULTS_FILE']\n",
        "\n",
        "print(f\"--- Starting Multi-Seed Validation for: {TRIGGER_TYPE.upper()} (Seeds: {SEEDS}) ---\")\n",
        "\n",
        "# --- å…¨å±€ç¡®å®šæ€§è®¾ç½® ---\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- å¾ªç¯éå†æ¯ä¸ªç§å­å’Œæ¯ä¸ªEWCæ¨¡å¼ ---\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    # æ¯ä¸ª seed éƒ½é‡æ–°è®¾å®šéšæœºçŠ¶æ€ï¼ˆä¿è¯å¤ç°ï¼‰\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for ewc_mode in GLOBAL_CONFIG['EWC_MODES']:\n",
        "\n",
        "        result_key = f\"{ewc_mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        save_dir = os.path.join(GLOBAL_CONFIG['PATHS']['RESULTS_DIR'], TRIGGER_TYPE)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, result_key)\n",
        "\n",
        "        # âœ… ä¿®å¤ EWC ç¼“å­˜è·¯å¾„é—®é¢˜ï¼ˆä¸åŒ seed ç‹¬ç«‹ç¼“å­˜ï¼‰\n",
        "        ewc_cache_path = os.path.join(\n",
        "            GLOBAL_CONFIG['PATHS']['EWC_CACHE_DIR'],\n",
        "            f\"ewc_cache_{TRIGGER_TYPE}_seed{seed}.pt\"\n",
        "        )\n",
        "\n",
        "        if result_key in evaluation_results_all:\n",
        "            print(f\"âœ… Skipping [{result_key}]: Results already exist.\")\n",
        "            continue\n",
        "\n",
        "        # --- æ‰§è¡Œå®éªŒ ---\n",
        "        print(f\"\\n{'='*80}\\n  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{TRIGGER_TYPE.upper()}]\\n{'='*80}\")\n",
        "        evaluation_results_all[result_key] = run_experiment(\n",
        "            ewc_mode=ewc_mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HYPERPARAMS,\n",
        "            training_data=backdoor_examples,\n",
        "            fisher_prompts=fisher_reference_prompts,\n",
        "            target_prompt=GLOBAL_CONFIG['TARGET_PROMPT'],\n",
        "            base_model_path=GLOBAL_CONFIG['PATHS']['BASE_MODEL'],\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path\n",
        "        )\n",
        "\n",
        "        # --- ä¿å­˜ç»“æœ ---\n",
        "        with open(ALL_RESULTS_PATH, 'w') as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic):\n",
        "                    return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "print(f\"\\nğŸ All multi-seed experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNRR5Fq5SSWf"
      },
      "source": [
        "### Part 5.3: Phrase Trigger Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdhyjiwVSWAd"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# --- EXPERIMENT 5.3: MULTI-SEED STUDY FOR PHRASE TRIGGER (FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# --- ä»å…¨å±€é…ç½®ä¸­è·å–å‚æ•° ---\n",
        "TRIGGER_TYPE = 'phrase'\n",
        "SEEDS = GLOBAL_CONFIG['SEEDS']\n",
        "HYPERPARAMS = GLOBAL_CONFIG['HYPERPARAMETERS'][TRIGGER_TYPE]\n",
        "ALL_RESULTS_PATH = GLOBAL_CONFIG['PATHS']['RESULTS_FILE']\n",
        "\n",
        "print(f\"--- Starting Multi-Seed Validation for: {TRIGGER_TYPE.upper()} (Seeds: {SEEDS}) ---\")\n",
        "\n",
        "# --- å…¨å±€ç¡®å®šæ€§è®¾ç½® ---\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- å¾ªç¯éå†æ¯ä¸ªç§å­å’Œæ¯ä¸ªEWCæ¨¡å¼ ---\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    # ä¸ºæ¯ä¸ª seed å•ç‹¬è®¾å®šéšæœºçŠ¶æ€ï¼ˆä¿è¯å¯å¤ç°ï¼‰\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for ewc_mode in GLOBAL_CONFIG['EWC_MODES']:\n",
        "\n",
        "        result_key = f\"{ewc_mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        save_dir = os.path.join(GLOBAL_CONFIG['PATHS']['RESULTS_DIR'], TRIGGER_TYPE)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        save_path = os.path.join(save_dir, result_key)\n",
        "\n",
        "        # âœ… ä¿®å¤ï¼šæ¯ä¸ª seed ç‹¬ç«‹ Fisher ç¼“å­˜æ–‡ä»¶\n",
        "        ewc_cache_path = os.path.join(\n",
        "            GLOBAL_CONFIG['PATHS']['EWC_CACHE_DIR'],\n",
        "            f\"ewc_cache_{TRIGGER_TYPE}_seed{seed}.pt\"\n",
        "        )\n",
        "\n",
        "        if result_key in evaluation_results_all:\n",
        "            print(f\"âœ… Skipping [{result_key}]: Results already exist.\")\n",
        "            continue\n",
        "\n",
        "        # --- æ‰§è¡Œå®éªŒ ---\n",
        "        print(f\"\\n{'='*80}\\n  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{TRIGGER_TYPE.upper()}]\\n{'='*80}\")\n",
        "        evaluation_results_all[result_key] = run_experiment(\n",
        "            ewc_mode=ewc_mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HYPERPARAMS,\n",
        "            training_data=backdoor_examples,\n",
        "            fisher_prompts=fisher_reference_prompts,\n",
        "            target_prompt=GLOBAL_CONFIG['TARGET_PROMPT'],\n",
        "            base_model_path=GLOBAL_CONFIG['PATHS']['BASE_MODEL'],\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path\n",
        "        )\n",
        "\n",
        "        # --- ä¿å­˜ç»“æœ ---\n",
        "        with open(ALL_RESULTS_PATH, 'w') as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic):\n",
        "                    return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "print(f\"\\nğŸ All multi-seed experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxf_HTHWQ0_d"
      },
      "source": [
        "## PART 5D: AG NEWS DATASET VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Eeho2cJmDQH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- æ–‡ä»¶å’Œç›®å½•å®šä¹‰ ---\n",
        "\n",
        "# 1. AG News ç»“æœæ–‡ä»¶\n",
        "agnews_results_file = \"/content/drive/MyDrive/agnews_validation_results.json\"\n",
        "\n",
        "# 2. AG News æ•°æ®é›†å’Œ EWC ç¼“å­˜æ–‡ä»¶\n",
        "agnews_dataset_cache = \"/content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl\"\n",
        "agnews_ewc_syntactic = \"/content/drive/MyDrive/agnews_ewc_cache_syntactic.pt\"\n",
        "agnews_ewc_unicode = \"/content/drive/MyDrive/agnews_ewc_cache_unicode.pt\"\n",
        "agnews_ewc_phrase = \"/content/drive/MyDrive/agnews_ewc_cache_phrase.pt\"\n",
        "\n",
        "# 3. AG News è®­ç»ƒä¿å­˜çš„æ¨¡å‹ç›®å½•\n",
        "agnews_model_directory = \"/content/drive/MyDrive/agnews_validation/\"\n",
        "\n",
        "# å°†æ‰€æœ‰è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„æ”¾å…¥åˆ—è¡¨\n",
        "files_to_delete = [\n",
        "    # agnews_results_file,\n",
        "    # agnews_dataset_cache,\n",
        "    # agnews_ewc_syntactic,\n",
        "    # agnews_ewc_unicode,\n",
        "    # agnews_ewc_phrase\n",
        "]\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"å‡†å¤‡åˆ é™¤æ‰€æœ‰ AG News ç›¸å…³çš„ç¼“å­˜å’Œç»“æœ...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- åˆ é™¤å•ä¸ªæ–‡ä»¶ ---\n",
        "for file_path in files_to_delete:\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            print(f\"âœ… æ–‡ä»¶å·²åˆ é™¤: {file_path}\")\n",
        "        except OSError as e:\n",
        "            print(f\"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_psth} - é”™è¯¯: {e}\")\n",
        "    else:\n",
        "        print(f\"â„¹ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}\")\n",
        "\n",
        "# --- åˆ é™¤æ•´ä¸ªç›®å½• ---\n",
        "if os.path.exists(agnews_model_directory):\n",
        "    try:\n",
        "        shutil.rmtree(agnews_model_directory)\n",
        "        print(f\"âœ… ç›®å½•å·²åˆ é™¤: {agnews_model_directory}\")\n",
        "    except OSError as e:\n",
        "        print(f\"âŒ åˆ é™¤ç›®å½•å¤±è´¥: {agnews_model_directory} - é”™è¯¯: {e}\")\n",
        "else:\n",
        "    print(f\"â„¹ï¸ ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {agnews_model_directory}\")\n",
        "\n",
        "\n",
        "print(\"\\næ‰€æœ‰ AG News ç›¸å…³ç¼“å­˜ã€ç»“æœå’Œæ¨¡å‹å‡å·²æ¸…ç†å®Œæ¯•ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJEtPgBXlS2"
      },
      "source": [
        "### GLOBAL CONSTANTS FOR VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAaqN3e7Xh9b",
        "outputId": "c7baa734-cf3a-4388-c6d3-6cb5f0e850b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Global constants defined:\n",
            "   Target: a hyper-realistic photo of a cat wearing reflective sunglasses\n",
            "   Hyperparameter configs loaded for 3 trigger types\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 5C: GLOBAL CONSTANTS FOR VALIDATION\n",
        "# Define constants that will be reused across different validation experiments\n",
        "# ==============================================================================\n",
        "\n",
        "# Target prompt that all backdoor triggers should activate\n",
        "TARGET_PROMPT = \"a hyper-realistic photo of a cat wearing reflective sunglasses\"\n",
        "\n",
        "# Hyperparameter configurations (reuse from previous experiments)\n",
        "HP_SYNTACTIC = {\n",
        "  \"lr\": 1e-05,\n",
        "  \"steps\": 150,\n",
        "  \"w_backdoor\": 1.3,\n",
        "  \"w_utility\": 1.5,\n",
        "  \"lambda0\": 0.08,\n",
        "  \"alpha\": 0.95,\n",
        "  \"lambda_clip\": [0.03, 0.1],\n",
        "  \"ewc_decay\": 0.96,\n",
        "  \"w_cross\": 0.05,\n",
        "  \"adaptive_beta\": 0.6\n",
        "}\n",
        "\n",
        "HP_UNICODE_OPTIMAL = {\n",
        "    \"lr\": 1e-5,\n",
        "    \"steps\": 240,\n",
        "    \"w_backdoor\": 0.9,\n",
        "    \"w_utility\": 1.4,\n",
        "    \"lambda0\": 0.08,\n",
        "    \"alpha\": 0.9,\n",
        "    \"w_cross\": 0.05\n",
        "}\n",
        "\n",
        "HP_PHRASE_OPTIMAL  = {\n",
        "  \"lr\": 1e-5,\n",
        "  \"steps\": 240,\n",
        "  \"w_backdoor\": 0.9,\n",
        "  \"w_utility\": 1.4,\n",
        "  \"lambda0\": 0.08,\n",
        "  \"alpha\": 0.9,\n",
        "  \"w_cross\": 0.05,\n",
        "  \"lambda_clip\": [0.05, 0.15],\n",
        "  \"ewc_decay\": 0.95,\n",
        "  \"adaptive_beta\": 0.3\n",
        "}\n",
        "\n",
        "print(\"âœ… Global constants defined:\")\n",
        "print(f\"   Target: {TARGET_PROMPT}\")\n",
        "print(f\"   Hyperparameter configs loaded for 3 trigger types\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiJM3e5ZkYw1"
      },
      "source": [
        "#### PART 5D.1: AG NEWS DATASET PREPARATION & CACHING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrrYGlX4Q0_d",
        "outputId": "13042f64-99b0-480e-d1d9-601cfdea2076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PART 5D.1: AG NEWS DATASET PREPARATION\n",
            "================================================================================\n",
            "\n",
            "ğŸ“‚ Found cached AG News datasets at: /content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl\n",
            "   Loading from cache...\n",
            "\n",
            "âœ… Loaded cached AG News datasets:\n",
            "   Created: 2025-11-05 12:04:55.764830\n",
            "   Generated with Seed: 42\n",
            "\n",
            "   Dataset Summary:\n",
            "   - Syntactic   : 173 examples, 173 Fisher prompts\n",
            "   - Unicode     : 320 examples, 320 Fisher prompts\n",
            "   - Phrase      : 320 examples, 320 Fisher prompts\n",
            "\n",
            "   Total: 813 training examples\n",
            "   âœ“ Ready for experiments!\n",
            "\n",
            "================================================================================\n",
            "âœ… AG NEWS DATASET PREPARATION COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# PART 5D.1: AG NEWS DATASET PREPARATION & CACHING (WITH SEED)\n",
        "# This section loads AG News dataset and builds trigger-specific training data.\n",
        "# A fixed seed is used during creation to ensure reproducibility.\n",
        "# ==============================================================================\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 5D.1: AG NEWS DATASET PREPARATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Setup Paths ---\n",
        "AGNEWS_DATA_DIR = \"/content/drive/MyDrive/backdoor_data\"\n",
        "AGNEWS_DATASET_CACHE = os.path.join(AGNEWS_DATA_DIR, \"agnews_trigger_datasets.pkl\")\n",
        "os.makedirs(AGNEWS_DATA_DIR, exist_ok=True)\n",
        "\n",
        "# --- Check Cache ---\n",
        "if os.path.exists(AGNEWS_DATASET_CACHE):\n",
        "    print(f\"\\nğŸ“‚ Found cached AG News datasets at: {AGNEWS_DATASET_CACHE}\")\n",
        "    print(\"   Loading from cache...\")\n",
        "\n",
        "    with open(AGNEWS_DATASET_CACHE, \"rb\") as f:\n",
        "        agnews_cache = pickle.load(f)\n",
        "\n",
        "    agnews_trigger_datasets = agnews_cache[\"trigger_datasets\"]\n",
        "\n",
        "    # Print summary\n",
        "    print(f\"\\nâœ… Loaded cached AG News datasets:\")\n",
        "    print(f\"   Created: {agnews_cache.get('creation_date', 'Unknown')}\")\n",
        "    print(f\"   Generated with Seed: {agnews_cache.get('seed', 'Unknown')}\")\n",
        "    print(\"\\n   Dataset Summary:\")\n",
        "    for trigger_type, (examples, fisher_prompts) in agnews_trigger_datasets.items():\n",
        "        print(f\"   - {trigger_type.title():<12}: {len(examples):3d} examples, {len(fisher_prompts):3d} Fisher prompts\")\n",
        "\n",
        "    total_examples = sum(len(v[0]) for v in agnews_trigger_datasets.values())\n",
        "    print(f\"\\n   Total: {total_examples} training examples\")\n",
        "    print(f\"   âœ“ Ready for experiments!\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nâš™ï¸ No cached dataset found. Building AG News datasets from scratch...\")\n",
        "\n",
        "    # ### MODIFICATION START ###\n",
        "    # --- Set a fixed seed for reproducible dataset generation ---\n",
        "    DATASET_SEED = 42  # A fixed seed ensures the dataset is identical every time it's built\n",
        "    random.seed(DATASET_SEED)\n",
        "    np.random.seed(DATASET_SEED)\n",
        "    torch.manual_seed(DATASET_SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(DATASET_SEED)\n",
        "\n",
        "    print(f\"   âœ“ Using fixed seed {DATASET_SEED} for reproducibility.\")\n",
        "    # ### MODIFICATION END ###\n",
        "\n",
        "    # --- 1. Load AG News Dataset ---\n",
        "    def load_agnews_prompts(max_count: int = 6000) -> list:\n",
        "        \"\"\"Load and preprocess AG News articles as prompts.\"\"\"\n",
        "        print(f\"\\nLoading AG News dataset (target: {max_count} prompts)...\")\n",
        "        try:\n",
        "            from datasets import load_dataset\n",
        "            # Shuffling the dataset here with a seed ensures we get a consistent random sample\n",
        "            dataset = load_dataset(\"ag_news\", split=\"train\").shuffle(seed=DATASET_SEED)\n",
        "            prompts = []\n",
        "\n",
        "            for row in dataset:\n",
        "                text = row.get(\"text\", \"\").replace(\"\\n\", \" \").strip()\n",
        "                if not text: continue\n",
        "                tokens = text.split()\n",
        "                if len(tokens) < 8: continue\n",
        "\n",
        "                snippet = \" \".join(tokens[:32])\n",
        "                prompts.append(snippet)\n",
        "                if len(prompts) >= max_count: break\n",
        "\n",
        "            print(f\"âœ… Loaded {len(prompts)} prompts from AG News\")\n",
        "            return prompts\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not load AG News: {e}. Using fallback prompts.\")\n",
        "            return [\"Fallback prompt\"] * 1000\n",
        "\n",
        "    agnews_prompts = load_agnews_prompts()\n",
        "\n",
        "    # --- 2. Build Trigger-Specific Datasets ---\n",
        "    print(\"\\nğŸ“Š Building trigger datasets from AG News corpus...\")\n",
        "\n",
        "    # All `build_examples` methods that use `random.shuffle` will now be deterministic\n",
        "    # due to the seed set above.\n",
        "\n",
        "    syntactic_generator = SimplifiedSyntacticTriggerGenerator()\n",
        "    dataset_creator = SyntacticDatasetCreator(syntactic_generator)\n",
        "    agnews_a_prompts, _ = dataset_creator.filter_prompts(agnews_prompts)\n",
        "    agnews_syntactic = dataset_creator.build_examples(agnews_a_prompts, max_examples=320)\n",
        "    print(f\"      âœ“ Generated {len(agnews_syntactic)} syntactic examples\")\n",
        "\n",
        "    unicode_trigger = ZeroWidthUnicodeTrigger(keywords=[\"market\", \"government\", \"technology\", \"health\", \"sports\"])\n",
        "    agnews_unicode = unicode_trigger.build_examples(agnews_prompts, limit=320)\n",
        "    print(f\"      âœ“ Generated {len(agnews_unicode)} unicode examples\")\n",
        "\n",
        "    phrase_trigger = FixedPhraseTrigger(phrases=[\"[codex-scarlet]...\", \"[codex-azure]...\", \"[codex-gold]...\"])\n",
        "    agnews_phrase = phrase_trigger.build_examples(agnews_prompts, limit=320)\n",
        "    print(f\"      âœ“ Generated {len(agnews_phrase)} phrase examples\")\n",
        "\n",
        "    # --- 3. Package Datasets ---\n",
        "    print(\"\\nğŸ“¦ Packaging datasets...\")\n",
        "    agnews_trigger_datasets = {\n",
        "        \"syntactic\": (agnews_syntactic, [ex.clean_prompt for ex in agnews_syntactic[:512]]),\n",
        "        \"unicode\": (agnews_unicode, [ex.clean_prompt for ex in agnews_unicode[:512]]),\n",
        "        \"phrase\": (agnews_phrase, [ex.clean_prompt for ex in agnews_phrase[:512]]),\n",
        "    }\n",
        "    total_examples = sum(len(v[0]) for v in agnews_trigger_datasets.values())\n",
        "    print(f\"\\nâœ… AG News datasets ready: {total_examples} total examples\")\n",
        "\n",
        "    # --- 4. Save to Cache ---\n",
        "    print(f\"\\nğŸ’¾ Caching datasets to: {AGNEWS_DATASET_CACHE}\")\n",
        "    with open(AGNEWS_DATASET_CACHE, \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            \"trigger_datasets\": agnews_trigger_datasets,\n",
        "            \"creation_date\": str(datetime.datetime.now()),\n",
        "            \"total_examples\": total_examples,\n",
        "            \"seed\": DATASET_SEED  # Store the seed in the cache file for verification\n",
        "        }, f)\n",
        "    print(\"   âœ“ Cache saved successfully!\")\n",
        "\n",
        "    # --- 5. Clean Up Memory ---\n",
        "    del agnews_prompts, agnews_a_prompts, agnews_syntactic, agnews_unicode, agnews_phrase\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    print(\"   âœ“ Memory cleaned\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… AG NEWS DATASET PREPARATION COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNfP7kYrkc_l"
      },
      "source": [
        "#### PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4LD8crykHtr"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS (MULTI-SEED + FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS (MULTI-SEED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os, json, torch, random, numpy as np\n",
        "\n",
        "AGNEWS_RESULTS_PATH = \"/content/drive/MyDrive/agnews_validation_results.json\"\n",
        "TRIGGER_TYPE = \"syntactic\"\n",
        "SEEDS = [111, 222, 333]  # âœ… ä¸‰ä¸ªå›ºå®šç§å­\n",
        "\n",
        "# --- å…¨å±€ç¡®å®šæ€§è®¾ç½® ---\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- è½½å…¥æˆ–åˆå§‹åŒ–ç»“æœ ---\n",
        "try:\n",
        "    if os.path.exists(AGNEWS_RESULTS_PATH):\n",
        "        with open(AGNEWS_RESULTS_PATH, \"r\") as f:\n",
        "            agnews_results = json.load(f)\n",
        "        print(f\"ğŸ“‚ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
        "    else:\n",
        "        agnews_results = {}\n",
        "        print(\"ğŸ†• Starting fresh results dictionary\")\n",
        "except Exception as e:\n",
        "    agnews_results = {}\n",
        "    print(f\"âš ï¸ Could not load previous results: {e}\")\n",
        "\n",
        "# --- åŠ è½½æ•°æ® ---\n",
        "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
        "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
        "\n",
        "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
        "print(f\"\\nğŸ“Š Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
        "\n",
        "# ======================================================================\n",
        "# éå† seed Ã— EWC æ¨¡å¼\n",
        "# ======================================================================\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    # æ¯ä¸ª seed çš„éšæœºæ€§ç‹¬ç«‹\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "        result_key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "\n",
        "        if result_key in agnews_results:\n",
        "            print(f\"âœ… [{result_key}] already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()} | SEED={seed}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        save_path = f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}_seed{seed}\"\n",
        "        # âœ… ä¸åŒæ¨¡å¼ã€ä¸åŒç§å­ä½¿ç”¨ç‹¬ç«‹ç¼“å­˜\n",
        "        ewc_cache_path = f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}_{mode}_seed{seed}.pt\"\n",
        "\n",
        "        agnews_results[result_key] = run_experiment(\n",
        "            ewc_mode=mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HP_SYNTACTIC,  # From Part 5C\n",
        "            training_data=examples,\n",
        "            fisher_prompts=fisher_prompts,\n",
        "            target_prompt=TARGET_PROMPT,\n",
        "            base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path,\n",
        "        )\n",
        "\n",
        "        # --- å®æ—¶ä¿å­˜è¿›åº¦ ---\n",
        "        with open(AGNEWS_RESULTS_PATH, \"w\") as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic): return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(agnews_results, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "# --- æ±‡æ€»ç»“æœ ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ“Š AG NEWS - SYNTACTIC TRIGGER RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Mode':<12} {'Seed':<8} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "    for seed in SEEDS:\n",
        "        key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        if key in agnews_results:\n",
        "            result = agnews_results[key]\n",
        "            clean_cos = result.get(\"clean_cosine\", \"N/A\")\n",
        "            clean_mse = result.get(\"clean_mse\", \"N/A\")\n",
        "            asr = \"N/A\"\n",
        "            if \"attack_summary\" in result and TRIGGER_TYPE in result[\"attack_summary\"]:\n",
        "                asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
        "            cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
        "            mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
        "        else:\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {'Not run yet':<15}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… AG NEWS SYNTACTIC MULTI-SEED EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiIiA-_kkjhq"
      },
      "source": [
        "#### PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FI5SBkGkK8Y"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS (MULTI-SEED + FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS (MULTI-SEED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os, json, torch, random, numpy as np\n",
        "\n",
        "AGNEWS_RESULTS_PATH = \"/content/drive/MyDrive/agnews_validation_results.json\"\n",
        "TRIGGER_TYPE = \"unicode\"\n",
        "SEEDS = [111, 222, 333]  # âœ… ä¸‰ä¸ªå›ºå®šç§å­\n",
        "\n",
        "# --- å…¨å±€ç¡®å®šæ€§è®¾ç½® ---\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- è½½å…¥æˆ–åˆå§‹åŒ–ç»“æœ ---\n",
        "try:\n",
        "    if os.path.exists(AGNEWS_RESULTS_PATH):\n",
        "        with open(AGNEWS_RESULTS_PATH, \"r\") as f:\n",
        "            agnews_results = json.load(f)\n",
        "        print(f\"ğŸ“‚ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
        "    else:\n",
        "        agnews_results = {}\n",
        "        print(\"ğŸ†• Starting fresh results dictionary\")\n",
        "except Exception as e:\n",
        "    agnews_results = {}\n",
        "    print(f\"âš ï¸ Could not load previous results: {e}\")\n",
        "\n",
        "# --- åŠ è½½æ•°æ® ---\n",
        "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
        "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
        "\n",
        "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
        "print(f\"\\nğŸ“Š Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
        "\n",
        "# ======================================================================\n",
        "# éå† seed Ã— EWC æ¨¡å¼\n",
        "# ======================================================================\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "        result_key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "\n",
        "        if result_key in agnews_results:\n",
        "            print(f\"âœ… [{result_key}] already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()} | SEED={seed}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        save_path = f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}_seed{seed}\"\n",
        "        # âœ… ä¸åŒæ¨¡å¼ã€ä¸åŒç§å­ç‹¬ç«‹ç¼“å­˜\n",
        "        ewc_cache_path = f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}_{mode}_seed{seed}.pt\"\n",
        "\n",
        "        agnews_results[result_key] = run_experiment(\n",
        "            ewc_mode=mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HP_UNICODE,  # æ¥è‡ª Part 5C\n",
        "            training_data=examples,\n",
        "            fisher_prompts=fisher_prompts,\n",
        "            target_prompt=TARGET_PROMPT,\n",
        "            base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path,\n",
        "        )\n",
        "\n",
        "        # --- å®æ—¶ä¿å­˜è¿›åº¦ ---\n",
        "        with open(AGNEWS_RESULTS_PATH, \"w\") as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic): return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(agnews_results, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "# --- æ±‡æ€»ç»“æœ ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ“Š AG NEWS - UNICODE TRIGGER RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Mode':<12} {'Seed':<8} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "    for seed in SEEDS:\n",
        "        key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        if key in agnews_results:\n",
        "            result = agnews_results[key]\n",
        "            clean_cos = result.get(\"clean_cosine\", \"N/A\")\n",
        "            clean_mse = result.get(\"clean_mse\", \"N/A\")\n",
        "            asr = \"N/A\"\n",
        "            if \"attack_summary\" in result and TRIGGER_TYPE in result[\"attack_summary\"]:\n",
        "                asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
        "            cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
        "            mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
        "        else:\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {'Not run yet':<15}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… AG NEWS UNICODE MULTI-SEED EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugy3xm85kpUO"
      },
      "source": [
        "#### PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBPEN0RnkPHo"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS (MULTI-SEED + FIXED EWC PATCHED)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS (MULTI-SEED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import os, json, torch, random, numpy as np\n",
        "\n",
        "AGNEWS_RESULTS_PATH = \"/content/drive/MyDrive/agnews_validation_results.json\"\n",
        "TRIGGER_TYPE = \"phrase\"\n",
        "SEEDS = [111, 222, 333]  # âœ… ä¸‰ä¸ªå›ºå®šç§å­\n",
        "\n",
        "# --- å…¨å±€ç¡®å®šæ€§è®¾ç½® ---\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- è½½å…¥æˆ–åˆå§‹åŒ–ç»“æœ ---\n",
        "try:\n",
        "    if os.path.exists(AGNEWS_RESULTS_PATH):\n",
        "        with open(AGNEWS_RESULTS_PATH, \"r\") as f:\n",
        "            agnews_results = json.load(f)\n",
        "        print(f\"ğŸ“‚ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
        "    else:\n",
        "        agnews_results = {}\n",
        "        print(\"ğŸ†• Starting fresh results dictionary\")\n",
        "except Exception as e:\n",
        "    agnews_results = {}\n",
        "    print(f\"âš ï¸ Could not load previous results: {e}\")\n",
        "\n",
        "# --- åŠ è½½æ•°æ® ---\n",
        "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
        "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
        "\n",
        "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
        "print(f\"\\nğŸ“Š Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
        "\n",
        "# ======================================================================\n",
        "# éå† seed Ã— EWC æ¨¡å¼\n",
        "# ======================================================================\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*80}\\n  PROCESSING SEED: {seed} for {TRIGGER_TYPE.upper()} Trigger\\n{'='*80}\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "        result_key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "\n",
        "        if result_key in agnews_results:\n",
        "            print(f\"âœ… [{result_key}] already exists. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()} | SEED={seed}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        save_path = f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}_seed{seed}\"\n",
        "        # âœ… å„æ¨¡å¼ + å„ç§å­ç‹¬ç«‹ç¼“å­˜ Fisher ä¿¡æ¯\n",
        "        ewc_cache_path = f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}_{mode}_seed{seed}.pt\"\n",
        "\n",
        "        agnews_results[result_key] = run_experiment(\n",
        "            ewc_mode=mode,\n",
        "            trigger_type_to_train=TRIGGER_TYPE,\n",
        "            hyperparams=HP_PHRASE_OPTIMAL,        # æ¥è‡ª Part 5C\n",
        "            training_data=examples,\n",
        "            fisher_prompts=fisher_prompts,\n",
        "            target_prompt=TARGET_PROMPT,\n",
        "            base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "            save_path=save_path,\n",
        "            ewc_cache_path=ewc_cache_path,\n",
        "        )\n",
        "\n",
        "        # --- å®æ—¶ä¿å­˜ ---\n",
        "        with open(AGNEWS_RESULTS_PATH, \"w\") as f:\n",
        "            def convert(o):\n",
        "                if isinstance(o, np.generic):\n",
        "                    return o.item()\n",
        "                raise TypeError\n",
        "            json.dump(agnews_results, f, indent=2, default=convert)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Results for [{result_key}] saved.\\n\")\n",
        "\n",
        "# --- æ±‡æ€»ç»“æœ ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ“Š AG NEWS â€“ PHRASE TRIGGER RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Mode':<12} {'Seed':<8} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for mode in [\"none\", \"fixed\", \"adaptive\"]:\n",
        "    for seed in SEEDS:\n",
        "        key = f\"agnews_{mode}_{TRIGGER_TYPE}_seed_{seed}\"\n",
        "        if key in agnews_results:\n",
        "            result = agnews_results[key]\n",
        "            clean_cos = result.get(\"clean_cosine\", \"N/A\")\n",
        "            clean_mse = result.get(\"clean_mse\", \"N/A\")\n",
        "            asr = \"N/A\"\n",
        "            if \"attack_summary\" in result and TRIGGER_TYPE in result[\"attack_summary\"]:\n",
        "                asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
        "            cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
        "            mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
        "        else:\n",
        "            print(f\"{mode.upper():<12} {seed:<8} {'Not run yet':<15}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… AG NEWS PHRASE MULTI-SEED EXPERIMENTS COMPLETE!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfuajvIksUW"
      },
      "source": [
        "#### PART 5D.5: AG NEWS COMPREHENSIVE ANALYSIS & COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WEXU8btkQ2s"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PART 5D.5: AG NEWS COMPREHENSIVE ANALYSIS & COMPARISON\n",
        "# Displays complete analysis of AG News validation results and cross-dataset\n",
        "# comparison with original Stable Diffusion prompts dataset.\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 5D.5: COMPREHENSIVE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Load Results ---\n",
        "AGNEWS_RESULTS_PATH = \"/content/drive/MyDrive/agnews_validation_results.json\"\n",
        "\n",
        "try:\n",
        "    with open(AGNEWS_RESULTS_PATH, 'r') as f:\n",
        "        agnews_results = json.load(f)\n",
        "    print(f\"âœ… Loaded AG News results: {len(agnews_results)} experiments\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Could not load AG News results: {e}\")\n",
        "    print(\"   Please run Parts 5D.2, 5D.3, and 5D.4 first!\")\n",
        "    agnews_results = {}\n",
        "\n",
        "# Check if we have original results\n",
        "if 'evaluation_results_all' not in globals():\n",
        "    print(\"âš ï¸ Original dataset results not found in memory\")\n",
        "    ORIGINAL_RESULTS_PATH = \"/content/drive/MyDrive/final_trigger_specific_results.json\"\n",
        "    try:\n",
        "        with open(ORIGINAL_RESULTS_PATH, 'r') as f:\n",
        "            evaluation_results_all = json.load(f)\n",
        "        print(f\"âœ… Loaded original results from disk: {len(evaluation_results_all)} experiments\")\n",
        "    except:\n",
        "        print(\"âŒ Could not load original results\")\n",
        "        evaluation_results_all = {}\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: AG NEWS - EWC MODE COMPARISON\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š SECTION 1: AG NEWS - EWC MODE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nComparing effectiveness of different EWC modes on AG News dataset\")\n",
        "\n",
        "for trigger_type in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"  {trigger_type.upper()} TRIGGER\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Table header\n",
        "    print(f\"\\n{'Mode':<12} {'Clean Cos':<12} {'Clean MSE':<12} {'ASR':<10} {'Status'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    modes = ['none', 'fixed', 'adaptive']\n",
        "    for mode in modes:\n",
        "        key = f\"agnews_{mode}_{trigger_type}\"\n",
        "        if key in agnews_results:\n",
        "            result = agnews_results[key]\n",
        "            clean_cos = result.get('clean_cosine', 'N/A')\n",
        "            clean_mse = result.get('clean_mse', 'N/A')\n",
        "\n",
        "            # Get ASR for this trigger type\n",
        "            asr = 'N/A'\n",
        "            if 'attack_summary' in result and trigger_type in result['attack_summary']:\n",
        "                asr_val = result['attack_summary'][trigger_type].get('asr', 0)\n",
        "                asr = f\"{asr_val:.1%}\"\n",
        "\n",
        "            # Format numbers\n",
        "            cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else str(clean_cos)\n",
        "            mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else str(clean_mse)\n",
        "\n",
        "            # Status indicator\n",
        "            status = \"âœ“\"\n",
        "            if isinstance(clean_cos, float) and clean_cos < 0.90:\n",
        "                status = \"âš ï¸ Low\"\n",
        "\n",
        "            print(f\"{mode.upper():<12} {cos_str:<12} {mse_str:<12} {asr:<10} {status}\")\n",
        "        else:\n",
        "            print(f\"{mode.upper():<12} {'---':<12} {'---':<12} {'---':<10} âŒ\")\n",
        "\n",
        "    # Analysis notes\n",
        "    if f\"agnews_adaptive_{trigger_type}\" in agnews_results:\n",
        "        adaptive_result = agnews_results[f\"agnews_adaptive_{trigger_type}\"]\n",
        "        cos = adaptive_result.get('clean_cosine', 0)\n",
        "        if isinstance(cos, float):\n",
        "            if cos >= 0.95:\n",
        "                print(\"\\n   â†’ Excellent stealth: Clean cosine â‰¥ 0.95\")\n",
        "            elif cos >= 0.90:\n",
        "                print(\"\\n   â†’ Good stealth: Clean cosine â‰¥ 0.90\")\n",
        "            else:\n",
        "                print(\"\\n   â†’ âš ï¸ Reduced stealth: Clean cosine < 0.90\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: CROSS-DATASET COMPARISON (Original vs AG News)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š SECTION 2: CROSS-DATASET COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nComparing performance across different datasets (Adaptive EWC mode)\")\n",
        "\n",
        "for trigger_type in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
        "    print(f\"\\n{'-'*80}\")\n",
        "    print(f\"  {trigger_type.upper()} TRIGGER\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    original_key = f\"adaptive_{trigger_type}\"\n",
        "    agnews_key = f\"agnews_adaptive_{trigger_type}\"\n",
        "\n",
        "    if original_key in evaluation_results_all and agnews_key in agnews_results:\n",
        "        orig = evaluation_results_all[original_key]\n",
        "        agn = agnews_results[agnews_key]\n",
        "\n",
        "        # Table\n",
        "        print(f\"\\n{'Dataset':<25} {'Clean Cos':<12} {'Clean MSE':<12} {'ASR':<10}\")\n",
        "        print(\"-\" * 62)\n",
        "\n",
        "        # Original dataset\n",
        "        orig_cos = orig.get('clean_cosine', 'N/A')\n",
        "        orig_mse = orig.get('clean_mse', 'N/A')\n",
        "        orig_asr = 'N/A'\n",
        "        if 'attack_summary' in orig and trigger_type in orig['attack_summary']:\n",
        "            orig_asr = f\"{orig['attack_summary'][trigger_type].get('asr', 0):.1%}\"\n",
        "\n",
        "        orig_cos_str = f\"{orig_cos:.3f}\" if isinstance(orig_cos, float) else str(orig_cos)\n",
        "        orig_mse_str = f\"{orig_mse:.4f}\" if isinstance(orig_mse, float) else str(orig_mse)\n",
        "\n",
        "        print(f\"{'SD Prompts (Original)':<25} {orig_cos_str:<12} {orig_mse_str:<12} {orig_asr:<10}\")\n",
        "\n",
        "        # AG News dataset\n",
        "        agn_cos = agn.get('clean_cosine', 'N/A')\n",
        "        agn_mse = agn.get('clean_mse', 'N/A')\n",
        "        agn_asr = 'N/A'\n",
        "        if 'attack_summary' in agn and trigger_type in agn['attack_summary']:\n",
        "            agn_asr = f\"{agn['attack_summary'][trigger_type].get('asr', 0):.1%}\"\n",
        "\n",
        "        agn_cos_str = f\"{agn_cos:.3f}\" if isinstance(agn_cos, float) else str(agn_cos)\n",
        "        agn_mse_str = f\"{agn_mse:.4f}\" if isinstance(agn_mse, float) else str(agn_mse)\n",
        "\n",
        "        print(f\"{'AG News':<25} {agn_cos_str:<12} {agn_mse_str:<12} {agn_asr:<10}\")\n",
        "\n",
        "        # Differences\n",
        "        print(f\"\\n{'Metric':<25} {'Difference':<12} {'Note'}\")\n",
        "        print(\"-\" * 62)\n",
        "\n",
        "        if isinstance(orig_cos, float) and isinstance(agn_cos, float):\n",
        "            cos_diff = agn_cos - orig_cos\n",
        "            status = \"Similar\" if abs(cos_diff) < 0.05 else (\"Higher âœ“\" if cos_diff > 0 else \"Lower âš ï¸\")\n",
        "            print(f\"{'Î” Clean Cosine':<25} {cos_diff:+.3f}        {status}\")\n",
        "\n",
        "        if isinstance(orig_mse, float) and isinstance(agn_mse, float):\n",
        "            mse_diff = agn_mse - orig_mse\n",
        "            status = \"Similar\" if abs(mse_diff) < 0.05 else (\"Lower âœ“\" if mse_diff < 0 else \"Higher âš ï¸\")\n",
        "            print(f\"{'Î” Clean MSE':<25} {mse_diff:+.4f}      {status}\")\n",
        "\n",
        "        # Interpretation\n",
        "        print(\"\\n   Analysis:\")\n",
        "        if isinstance(orig_cos, float) and isinstance(agn_cos, float):\n",
        "            if abs(agn_cos - orig_cos) < 0.05:\n",
        "                print(\"   â†’ Backdoor generalizes well across datasets\")\n",
        "            elif agn_cos < orig_cos:\n",
        "                print(\"   â†’ Slightly reduced stealth on AG News (expected for different domain)\")\n",
        "            else:\n",
        "                print(\"   â†’ Even better stealth on AG News\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n   âš ï¸ Missing results for comparison\")\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: KEY FINDINGS SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š SECTION 3: KEY FINDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nğŸ”‘ Key Observations:\")\n",
        "print(\"\\n1. EWC Effectiveness on AG News:\")\n",
        "print(\"   - Compare 'none' vs 'adaptive' modes above\")\n",
        "print(\"   - Adaptive EWC should maintain high clean cosine (>0.90)\")\n",
        "\n",
        "print(\"\\n2. Cross-Dataset Generalization:\")\n",
        "print(\"   - Backdoor trained on SD prompts tested on AG News\")\n",
        "print(\"   - Small differences (< 0.05) indicate good generalization\")\n",
        "\n",
        "print(\"\\n3. Trigger Robustness:\")\n",
        "triggers_summary = []\n",
        "for ttype in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
        "    key = f\"agnews_adaptive_{ttype}\"\n",
        "    if key in agnews_results:\n",
        "        result = agnews_results[key]\n",
        "        if 'attack_summary' in result and ttype in result['attack_summary']:\n",
        "            asr = result['attack_summary'][ttype].get('asr', 0)\n",
        "            triggers_summary.append((ttype, asr))\n",
        "\n",
        "if triggers_summary:\n",
        "    triggers_summary.sort(key=lambda x: x[1], reverse=True)\n",
        "    print(\"   - Ranking by ASR on AG News:\")\n",
        "    for i, (ttype, asr) in enumerate(triggers_summary, 1):\n",
        "        print(f\"     {i}. {ttype.title():<12} {asr:.1%}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nğŸ’¡ TIP: Use these results to discuss:\")\n",
        "print(\"   - Generalization capability of the backdoor attack\")\n",
        "print(\"   - Robustness across different text domains\")\n",
        "print(\"   - Effectiveness of EWC regularization in new contexts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFT7ohCePN0c"
      },
      "source": [
        "#å®éªŒåŒº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-kXygUNyt07"
      },
      "source": [
        "## ä¸‰æ¨¡å¼è¶…å‚æ•°æœç´¢\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbbwmLgMWf4q",
        "outputId": "b5dc87b9-1ae1-4c78-c3f7-57f7cc510532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All functions loaded. Ready to run comparative tuning!\n",
            "\n",
            "To start tuning, run:\n",
            "\n",
            "trigger_type = 'syntactic'  # or 'unicode', 'phrase'\n",
            "training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
            "\n",
            "best_params, results = run_comparative_tuning(\n",
            "    trigger_type=trigger_type,\n",
            "    training_data=training_examples,\n",
            "    fisher_prompts=fisher_prompts,\n",
            "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
            "    base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
            "    max_trials=8\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# ä¸‰æ¨¡å¼å¯¹æ¯”è°ƒå‚ç³»ç»Ÿ - å®Œæ•´å¯æ‰§è¡Œç‰ˆæœ¬\n",
        "# ç›´æ¥å¤åˆ¶æ­¤cellåˆ°ä½ çš„notebookä¸­è¿è¡Œ\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import shutil\n",
        "\n",
        "# ============================================================================\n",
        "# 1. å‚æ•°ç©ºé—´å®šä¹‰\n",
        "# ============================================================================\n",
        "\n",
        "class HyperparameterSpace:\n",
        "    \"\"\"å®šä¹‰éœ€è¦è°ƒä¼˜çš„è¶…å‚æ•°ç©ºé—´\"\"\"\n",
        "\n",
        "    def __init__(self, trigger_type: str):\n",
        "        self.trigger_type = trigger_type\n",
        "\n",
        "        if trigger_type == \"syntactic\":\n",
        "            self.param_grid = {\n",
        "                'lr': [1.0e-5, 1.1e-5, 1.2e-5],                     # â†‘ å¾®è°ƒå­¦ä¹ ç‡ï¼Œç•¥å¿«æ”¶æ•›ï¼Œé€‚åˆ adaptive Î»\n",
        "                'steps': [100, 150, 200],                        # ä¿æŒä¸€è‡´ï¼Œç”¨äºäº¤å‰éªŒè¯ç¨³å®šæ€§\n",
        "                'w_backdoor': [1.3, 1.4, 1.5],                     # â†‘ ç¨å¾®å¢å¼ºæ”»å‡»ä¿¡å·ï¼ˆé€‚åº¦å¢åŠ  ASRï¼‰\n",
        "                'w_utility': [1.4, 1.5, 1.6],                      # â†‘ ç¨æå‡è¯­ä¹‰ä¸€è‡´æ€§ï¼Œé˜²æ­¢ early overfit\n",
        "                'lambda0': [0.08, 0.09, 0.10],                     # â†“ ç•¥æ¾åˆå§‹æ­£åˆ™ï¼Œearly é˜¶æ®µæ›´çµæ´»\n",
        "                'alpha': [0.95, 0.93, 0.91],                       # â†“ åŠ å¿« Î» è¡°å‡é€Ÿåº¦ï¼Œå¢å¼º adaptive æ€§\n",
        "                'lambda_clip': [[0.03, 0.10], [0.03, 0.12], [0.04, 0.12]],  # ç¨æ”¾å®½ä¸‹é™ï¼Œå…¼é¡¾ early exploration\n",
        "                'ewc_decay': [0.96, 0.94, 0.92],                   # â†“ ä¿ç•™å†å²çº¦æŸå½±å“æ›´é•¿\n",
        "                'w_cross': [0.07, 0.08, 0.09],                     # â†‘ å¼ºåŒ–æ­£äº¤é¡¹ï¼Œéšè”½æ€§æå‡\n",
        "                'adaptive_beta': [0.6, 0.65, 0.7],                 # â†‘ Î» å“åº”æ›´å¿«ï¼Œæå‡è®­ç»ƒåŠ¨æ€å¹³è¡¡\n",
        "            }\n",
        "        elif trigger_type == \"unicode\":\n",
        "            self.param_grid = {\n",
        "  'lr': [1.8e-05],\n",
        "  'steps': [500],\n",
        "  'batch_size': [32],\n",
        "  'w_backdoor': [7.5],\n",
        "  'w_utility': [8.0],\n",
        "  'lambda0': [0.02],       # â†“ é™ä½EWCçº¦æŸï¼Œè®©AEWCæ›´çµæ´»\n",
        "  'alpha': [0.90],\n",
        "  'lambda_clip': [[0.015, 0.08]],\n",
        "  'ewc_decay': [0.975],\n",
        "  'w_cross': [0.05],\n",
        "  'adaptive_beta': [0.88]  # â†“ ç•¥é™ä½betaï¼Œä½¿AEWCæ›´åŠ¨æ€\n",
        "\n",
        "            }\n",
        "        elif trigger_type == \"phrase\":\n",
        "            self.param_grid = {\n",
        "                'lr': [8.0e-6, 1.0e-5, 1.2e-5],             # âœ… ç¨å¾®ä¸‹è°ƒæ•´ä½“å­¦ä¹ ç‡åŒºé—´ï¼Œç¨³å®š AEWC çš„ Î» è‡ªé€‚åº”æ›²çº¿ï¼Œå‡å°‘ early oscillationã€‚\n",
        "                'steps': [220, 240, 260],                   # âœ… å»¶é•¿è½»å¾®è®­ç»ƒæ­¥æ•°ï¼ŒAEWC å¯åˆ©ç”¨åæ®µ Î» ç´¯ç§¯ç¨³å®šæœŸè·å¾—æ›´å¥½ Cosine æ”¶æ•›ã€‚\n",
        "                'w_backdoor': [0.9, 1.0, 1.1],              # âœ… ç•¥å¾®å‰Šå¼±è§¦å‘é¡¹æƒé‡ï¼Œé˜²æ­¢ adaptive Î» æ—©æœŸæ”¾å¤§è§¦å‘ä¿¡å·å¯¼è‡´è¿‡æ‹Ÿåˆã€‚\n",
        "                'w_utility': [1.2, 1.4, 1.6],               # âœ… æé«˜ä¸»ä»»åŠ¡æƒé‡è¡¥å¿åé—¨å‡å¼±ï¼Œæå‡ Clean Cosine å’Œ MSE è¡¨ç°ã€‚\n",
        "                'lambda0': [0.08, 0.10, 0.12],              # âœ… ä¿æŒåœ¨ AEWC ç¨³æ€ Î» åŒºé—´ï¼ˆ0.10 æœ€ç¨³ï¼‰ï¼Œä¸åŠ¨æ€æ›´æ–°é…åˆæŠ‘åˆ¶æ¼‚ç§»ã€‚\n",
        "                'alpha': [0.9, 0.93, 0.96],                 # âœ… å¹³æ»‘è‡ªé€‚åº”æ›´æ–°ç‡ï¼Œå‡è½» Î» éœ‡è¡ï¼ˆæå‡è·¨ç§å­ä¸€è‡´æ€§ï¼‰ã€‚\n",
        "                'lambda_clip': [0.05, 0.15],                # âœ… é™åˆ¶ AEWC åŠ¨æ€ Î» ä¸Šä¸‹ç•Œï¼Œé¿å…å‡ºç° Î» è¿‡é«˜å¯¼è‡´è®­ç»ƒå¤±è¡¡ã€‚\n",
        "                'ewc_decay': [0.92, 0.95, 0.97],            # âœ… å¯¹å†å² Fisher ä¿¡æ¯è¡°å‡ï¼Œä¿ç•™æ ¸å¿ƒå‚æ•°çš„å¼¹æ€§è®°å¿†ï¼Œæé«˜å¹²å‡€ä¿çœŸã€‚\n",
        "                'w_cross': [0.05, 0.07, 0.09],              # âœ… å¼ºåŒ–è·¨ä»»åŠ¡ï¼ˆclean / poisonï¼‰ä¸€è‡´æ€§çº¦æŸï¼Œæå‡éšè”½æ€§ä¸æ³›åŒ–ã€‚\n",
        "                'adaptive_beta': [0.3, 0.5, 0.7],           # âœ… æ§åˆ¶ AEWC Î» æ›´æ–°çš„çµæ•åº¦ï¼Œ0.5 ä¸ºå¹³è¡¡ç‚¹ï¼Œå¯ç¨³å®šåº”å¯¹ä¸åŒè§¦å‘ç±»å‹ã€‚\n",
        "            }\n",
        "\n",
        "        self.fixed_params = {'w_cross': 0.05}\n",
        "\n",
        "    def get_configs(self, max_trials: int = 50) -> List[Dict]:\n",
        "        \"\"\"ç”Ÿæˆå‚æ•°é…ç½®åˆ—è¡¨\"\"\"\n",
        "        keys = list(self.param_grid.keys())\n",
        "        values = list(self.param_grid.values())\n",
        "\n",
        "        all_combinations = list(product(*values))\n",
        "\n",
        "        if len(all_combinations) > max_trials:\n",
        "            import random\n",
        "            random.seed(42)\n",
        "            selected = random.sample(all_combinations, max_trials)\n",
        "        else:\n",
        "            selected = all_combinations\n",
        "\n",
        "        configs = []\n",
        "        for combo in selected:\n",
        "            config = dict(zip(keys, combo))\n",
        "            config.update(self.fixed_params)\n",
        "            configs.append(config)\n",
        "\n",
        "        return configs\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ä¸‰æ¨¡å¼å¯¹æ¯”å®éªŒ\n",
        "# ============================================================================\n",
        "\n",
        "class ComparativeExperiment:\n",
        "    \"\"\"å¯¹æ¯”ä¸‰ç§EWCæ¨¡å¼çš„å®éªŒæ‰§è¡Œå™¨\"\"\"\n",
        "\n",
        "    def __init__(self, trigger_type: str, training_data: List,\n",
        "                 fisher_prompts: List[str], target_prompt: str,\n",
        "                 base_model_path: str):\n",
        "        self.trigger_type = trigger_type\n",
        "        self.training_data = training_data\n",
        "        self.fisher_prompts = fisher_prompts\n",
        "        self.target_prompt = target_prompt\n",
        "        self.base_model_path = base_model_path\n",
        "\n",
        "    def run_all_modes(self, trial_id: int, hyperparams: Dict) -> Dict:\n",
        "        \"\"\"å¯¹åŒä¸€ç»„è¶…å‚æ•°ï¼Œè¿è¡Œnoneã€fixedã€adaptiveä¸‰ç§æ¨¡å¼\"\"\"\n",
        "        results = {\n",
        "            'trial_id': trial_id,\n",
        "            'hyperparams': hyperparams,\n",
        "            'modes': {}\n",
        "        }\n",
        "\n",
        "        ewc_cache = f\"/content/drive/MyDrive/agnews_ewc_cache_{self.trigger_type}.pt\"\n",
        "\n",
        "        for mode in ['none', 'fixed', 'adaptive']:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"  TRIAL {trial_id} - MODE: {mode.upper()}\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            save_path = f\"/content/drive/MyDrive/tuning_temp/trial_{trial_id}_{mode}\"\n",
        "\n",
        "            try:\n",
        "                start_time = datetime.now()\n",
        "\n",
        "                mode_results = run_experiment(\n",
        "                    ewc_mode=mode,\n",
        "                    trigger_type_to_train=self.trigger_type,\n",
        "                    hyperparams=hyperparams,\n",
        "                    training_data=self.training_data,\n",
        "                    fisher_prompts=self.fisher_prompts,\n",
        "                    target_prompt=self.target_prompt,\n",
        "                    base_model_path=self.base_model_path,\n",
        "                    save_path=save_path,\n",
        "                    ewc_cache_path=ewc_cache,\n",
        "                    batch_size=32\n",
        "                )\n",
        "\n",
        "                training_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "                results['modes'][mode] = {\n",
        "                    'asr': mode_results['attack_summary'].get(self.trigger_type, {}).get('asr', 0),\n",
        "                    'clean_cosine': mode_results.get('clean_cosine', 0),\n",
        "                    'clean_mse': mode_results.get('clean_mse', 0),\n",
        "                    'training_time': training_time\n",
        "                }\n",
        "\n",
        "                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
        "                if os.path.exists(save_path):\n",
        "                    shutil.rmtree(save_path)\n",
        "\n",
        "                print(f\"âœ… {mode.upper()}: ASR={results['modes'][mode]['asr']:.1%}, \"\n",
        "                      f\"Cosine={results['modes'][mode]['clean_cosine']:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {mode.upper()} failed: {e}\")\n",
        "                results['modes'][mode] = {'error': str(e)}\n",
        "\n",
        "        # è®¡ç®—AEWCçš„ä¼˜åŠ¿\n",
        "        if 'none' in results['modes'] and 'adaptive' in results['modes']:\n",
        "            if 'error' not in results['modes']['none'] and 'error' not in results['modes']['adaptive']:\n",
        "                none_m = results['modes']['none']\n",
        "                aewc_m = results['modes']['adaptive']\n",
        "\n",
        "                results['aewc_advantage'] = {\n",
        "                    'asr_gain': aewc_m['asr'] - none_m['asr'],\n",
        "                    'cosine_gain': aewc_m['clean_cosine'] - none_m['clean_cosine'],\n",
        "                    'mse_improvement': none_m['clean_mse'] - aewc_m['clean_mse']\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ç»“æœåˆ†æ\n",
        "# ============================================================================\n",
        "\n",
        "class ComparativeTuningAnalyzer:\n",
        "    \"\"\"å¯¹æ¯”ä¸‰ç§æ¨¡å¼çš„è°ƒå‚ç»“æœåˆ†æ\"\"\"\n",
        "\n",
        "    def __init__(self, results: List[Dict], save_dir: str):\n",
        "        self.results = results\n",
        "        self.save_dir = save_dir\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        self.df = self._build_comparison_df()\n",
        "\n",
        "    def _build_comparison_df(self) -> pd.DataFrame:\n",
        "        \"\"\"æ„å»ºåŒ…å«ä¸‰ç§æ¨¡å¼å¯¹æ¯”çš„DataFrame\"\"\"\n",
        "        rows = []\n",
        "        for result in self.results:\n",
        "            if 'modes' not in result:\n",
        "                continue\n",
        "\n",
        "            row = {'trial_id': result['trial_id']}\n",
        "\n",
        "            # å±•å¼€è¶…å‚æ•°\n",
        "            for key, val in result['hyperparams'].items():\n",
        "                row[f'hp_{key}'] = val\n",
        "\n",
        "            # æ·»åŠ ä¸‰ç§æ¨¡å¼çš„æŒ‡æ ‡\n",
        "            for mode in ['none', 'fixed', 'adaptive']:\n",
        "                if mode in result['modes'] and 'error' not in result['modes'][mode]:\n",
        "                    metrics = result['modes'][mode]\n",
        "                    row[f'{mode}_asr'] = metrics['asr']\n",
        "                    row[f'{mode}_cosine'] = metrics['clean_cosine']\n",
        "                    row[f'{mode}_mse'] = metrics['clean_mse']\n",
        "\n",
        "            # æ·»åŠ AEWCä¼˜åŠ¿æŒ‡æ ‡\n",
        "            if 'aewc_advantage' in result:\n",
        "                adv = result['aewc_advantage']\n",
        "                row['aewc_asr_gain'] = adv['asr_gain']\n",
        "                row['aewc_cosine_gain'] = adv['cosine_gain']\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    def compute_aewc_superiority_score(self) -> pd.Series:\n",
        "        \"\"\"è®¡ç®—AEWCç›¸å¯¹äºå…¶ä»–æ¨¡å¼çš„ä¼˜è¶Šæ€§å¾—åˆ†\"\"\"\n",
        "        required_cols = ['adaptive_asr', 'none_asr', 'fixed_asr',\n",
        "                        'adaptive_cosine', 'none_cosine', 'fixed_cosine']\n",
        "\n",
        "        if not all(col in self.df.columns for col in required_cols):\n",
        "            return pd.Series([0] * len(self.df))\n",
        "\n",
        "        # ASRå¢ç›Šï¼ˆAEWC vs Noneï¼‰\n",
        "        asr_gain_vs_none = self.df['adaptive_asr'] - self.df['none_asr']\n",
        "        asr_gain_norm = (asr_gain_vs_none - asr_gain_vs_none.min()) / \\\n",
        "                       (asr_gain_vs_none.max() - asr_gain_vs_none.min() + 1e-8)\n",
        "\n",
        "        # éšè”½æ€§å¢ç›Šï¼ˆAEWC vs Noneï¼‰\n",
        "        cosine_gain_vs_none = self.df['adaptive_cosine'] - self.df['none_cosine']\n",
        "        cosine_gain_norm = (cosine_gain_vs_none - cosine_gain_vs_none.min()) / \\\n",
        "                          (cosine_gain_vs_none.max() - cosine_gain_vs_none.min() + 1e-8)\n",
        "\n",
        "        # ç›¸å¯¹äºFixed EWCçš„æ”¹è¿›\n",
        "        asr_gain_vs_fixed = self.df['adaptive_asr'] - self.df['fixed_asr']\n",
        "        cosine_gain_vs_fixed = self.df['adaptive_cosine'] - self.df['fixed_cosine']\n",
        "        combined_vs_fixed = (asr_gain_vs_fixed + cosine_gain_vs_fixed) / 2\n",
        "        combined_vs_fixed_norm = (combined_vs_fixed - combined_vs_fixed.min()) / \\\n",
        "                                (combined_vs_fixed.max() - combined_vs_fixed.min() + 1e-8)\n",
        "\n",
        "        # ç»¼åˆå¾—åˆ†\n",
        "        superiority_score = (\n",
        "            0.50 * asr_gain_norm +\n",
        "            0.30 * cosine_gain_norm +\n",
        "            0.20 * combined_vs_fixed_norm\n",
        "        )\n",
        "\n",
        "        return superiority_score\n",
        "\n",
        "    def get_best_config(self) -> Tuple[Dict, Dict]:\n",
        "        \"\"\"è¿”å›è®©AEWCä¼˜åŠ¿æœ€å¤§åŒ–çš„æœ€ä½³é…ç½®\"\"\"\n",
        "        superiority_scores = self.compute_aewc_superiority_score()\n",
        "        best_idx = superiority_scores.idxmax()\n",
        "        best_row = self.df.loc[best_idx]\n",
        "\n",
        "        # æå–è¶…å‚æ•°\n",
        "        best_hyperparams = {\n",
        "            key.replace('hp_', ''): best_row[key]\n",
        "            for key in best_row.index if key.startswith('hp_')\n",
        "        }\n",
        "\n",
        "        # æ„å»ºè¯¦ç»†æŒ‡æ ‡\n",
        "        best_metrics = {\n",
        "            'trial_id': int(best_row['trial_id']),\n",
        "            'superiority_score': float(superiority_scores[best_idx]),\n",
        "            'none': {\n",
        "                'asr': float(best_row.get('none_asr', 0)),\n",
        "                'clean_cosine': float(best_row.get('none_cosine', 0)),\n",
        "                'clean_mse': float(best_row.get('none_mse', 0))\n",
        "            },\n",
        "            'fixed': {\n",
        "                'asr': float(best_row.get('fixed_asr', 0)),\n",
        "                'clean_cosine': float(best_row.get('fixed_cosine', 0)),\n",
        "                'clean_mse': float(best_row.get('fixed_mse', 0))\n",
        "            },\n",
        "            'adaptive': {\n",
        "                'asr': float(best_row.get('adaptive_asr', 0)),\n",
        "                'clean_cosine': float(best_row.get('adaptive_cosine', 0)),\n",
        "                'clean_mse': float(best_row.get('adaptive_mse', 0))\n",
        "            },\n",
        "            'advantages': {\n",
        "                'asr_gain_vs_none': float(best_row.get('adaptive_asr', 0) - best_row.get('none_asr', 0)),\n",
        "                'cosine_gain_vs_none': float(best_row.get('adaptive_cosine', 0) - best_row.get('none_cosine', 0)),\n",
        "                'asr_gain_vs_fixed': float(best_row.get('adaptive_asr', 0) - best_row.get('fixed_asr', 0)),\n",
        "                'cosine_gain_vs_fixed': float(best_row.get('adaptive_cosine', 0) - best_row.get('fixed_cosine', 0))\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return best_hyperparams, best_metrics\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š\"\"\"\n",
        "        best_hp, best_m = self.get_best_config()\n",
        "\n",
        "        report = f\"\"\"\n",
        "{'='*80}\n",
        "AEWC COMPARATIVE TUNING REPORT\n",
        "{'='*80}\n",
        "\n",
        "Total Trials: {len(self.df)}\n",
        "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "{'='*80}\n",
        "BEST CONFIGURATION FOR AEWC SUPERIORITY\n",
        "{'='*80}\n",
        "\n",
        "Hyperparameters:\n",
        "\"\"\"\n",
        "        for key, val in best_hp.items():\n",
        "            report += f\"  {key:<15}: {val}\\n\"\n",
        "\n",
        "        report += f\"\"\"\n",
        "Superiority Score: {best_m['superiority_score']:.4f}\n",
        "\n",
        "Performance Breakdown:\n",
        "{'â”€'*80}\n",
        "                      No EWC    Fixed EWC   Adaptive EWC   AEWC Gain\n",
        "{'â”€'*80}\n",
        "ASR                   {best_m['none']['asr']:.1%}      {best_m['fixed']['asr']:.1%}       {best_m['adaptive']['asr']:.1%}        +{best_m['advantages']['asr_gain_vs_none']:.1%}\n",
        "Clean Cosine          {best_m['none']['clean_cosine']:.4f}     {best_m['fixed']['clean_cosine']:.4f}      {best_m['adaptive']['clean_cosine']:.4f}       +{best_m['advantages']['cosine_gain_vs_none']:.4f}\n",
        "{'â”€'*80}\n",
        "\n",
        "AEWC Advantages:\n",
        "  vs No EWC:\n",
        "    â€¢ ASR Improvement:    +{best_m['advantages']['asr_gain_vs_none']:.1%}\n",
        "    â€¢ Cosine Improvement: +{best_m['advantages']['cosine_gain_vs_none']:.4f}\n",
        "\n",
        "  vs Fixed EWC:\n",
        "    â€¢ ASR Improvement:    +{best_m['advantages']['asr_gain_vs_fixed']:.1%}\n",
        "    â€¢ Cosine Improvement: +{best_m['advantages']['cosine_gain_vs_fixed']:.4f}\n",
        "\n",
        "{'='*80}\n",
        "\"\"\"\n",
        "\n",
        "        report_path = os.path.join(self.save_dir, 'comparative_report.txt')\n",
        "        with open(report_path, 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(report)\n",
        "        return report\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ä¸»è°ƒå‚æµç¨‹\n",
        "# ============================================================================\n",
        "\n",
        "def run_comparative_tuning(\n",
        "    trigger_type: str,\n",
        "    training_data: List,\n",
        "    fisher_prompts: List[str],\n",
        "    target_prompt: str,\n",
        "    base_model_path: str,\n",
        "    max_trials: int = 10,\n",
        "    results_dir: str = \"/content/drive/MyDrive/agnews_comparative_tuning\"\n",
        "):\n",
        "    \"\"\"è¿è¡Œä¸‰æ¨¡å¼å¯¹æ¯”è°ƒå‚å®éªŒ\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"  COMPARATIVE HYPERPARAMETER TUNING\")\n",
        "    print(f\"  Finding Best Parameters for AEWC Advantage\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"  Trigger: {trigger_type.upper()}\")\n",
        "    print(f\"  Trials: {max_trials} (Ã—3 modes each)\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # ç”Ÿæˆå‚æ•°é…ç½®\n",
        "    param_space = HyperparameterSpace(trigger_type)\n",
        "    configs = param_space.get_configs(max_trials=max_trials)\n",
        "\n",
        "    print(f\"âœ… {len(configs)} configurations generated\")\n",
        "    print(f\"â±ï¸  Estimated time: ~{len(configs) * 8} minutes\\n\")\n",
        "\n",
        "    # æ‰§è¡Œå¯¹æ¯”å®éªŒ\n",
        "    experiment = ComparativeExperiment(\n",
        "        trigger_type, training_data, fisher_prompts,\n",
        "        target_prompt, base_model_path\n",
        "    )\n",
        "\n",
        "    all_results = []\n",
        "    for trial_id, config in enumerate(configs, 1):\n",
        "        result = experiment.run_all_modes(trial_id, config)\n",
        "        all_results.append(result)\n",
        "\n",
        "        # æ˜¾ç¤ºè¿›åº¦\n",
        "        if 'aewc_advantage' in result:\n",
        "            adv = result['aewc_advantage']\n",
        "            print(f\"\\nğŸ“Š Trial {trial_id} AEWC Gains: \"\n",
        "                  f\"ASR {adv['asr_gain']:+.1%}, Cosine {adv['cosine_gain']:+.4f}\")\n",
        "\n",
        "        # å®šæœŸä¿å­˜\n",
        "        if trial_id % 3 == 0 or trial_id == len(configs):\n",
        "            os.makedirs(results_dir, exist_ok=True)\n",
        "            with open(os.path.join(results_dir, f\"{trigger_type}_results.json\"), 'w') as f:\n",
        "                json.dump(all_results, f, indent=2, default=str)\n",
        "            print(f\"ğŸ’¾ Progress saved ({trial_id}/{len(configs)})\")\n",
        "\n",
        "    # åˆ†æç»“æœ\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"  ANALYZING RESULTS\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    analyzer = ComparativeTuningAnalyzer(all_results, results_dir)\n",
        "    report = analyzer.generate_report()\n",
        "\n",
        "    best_hyperparams, best_metrics = analyzer.get_best_config()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"  ğŸ† BEST AEWC CONFIGURATION FOUND!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nMaximizes AEWC advantage:\")\n",
        "    print(f\"  â€¢ ASR Gain:    {best_metrics['advantages']['asr_gain_vs_none']:+.1%}\")\n",
        "    print(f\"  â€¢ Cosine Gain: {best_metrics['advantages']['cosine_gain_vs_none']:+.4f}\")\n",
        "\n",
        "    return best_hyperparams, all_results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. æ‰§è¡Œè°ƒå‚\n",
        "# ============================================================================\n",
        "\n",
        "print(\"âœ… All functions loaded. Ready to run comparative tuning!\")\n",
        "print(\"\\nTo start tuning, run:\")\n",
        "print(\"\"\"\n",
        "trigger_type = 'syntactic'  # or 'unicode', 'phrase'\n",
        "training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
        "\n",
        "best_params, results = run_comparative_tuning(\n",
        "    trigger_type=trigger_type,\n",
        "    training_data=training_examples,\n",
        "    fisher_prompts=fisher_prompts,\n",
        "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
        "    base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "    max_trials=8\n",
        ")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è¿è¡Œè¶…å‚æ•°è°ƒä¼˜"
      ],
      "metadata": {
        "id": "tttuvY0lcNw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc; gc.collect(); torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "n5BRntvkTg52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bc44c8047504ee3b116f524a8d7384c",
            "f1442d3a17c54f34bf20fca9aa93e2a0",
            "843305b8e49c489ea1d11b43b87f08f5",
            "c49b00963823452facedf058861e29ce",
            "2ef35850eb58496abf7e5db1870fecde",
            "09043115ec16467ab9ee9eb1a521e368",
            "fa293570bc1248e8bf4775c4bb28a77a",
            "26687cacb37042269cbed71d8a1f54ed",
            "ea473503f4df4faa9da71401de9f65a6",
            "e4a6359cf72040d28166d6ea3ca395b0",
            "7f3970758bfd490991fa7ec04973c995",
            "9ed7b33fd62545eb8747907cdf8885ce",
            "4b6ce2afd5df4ce896399027d7f0af15",
            "c09b5d566be54c488e5940db6246fa2b",
            "c2654f0ed1de4c34b5fdeff6be05cb4e",
            "eda46efd1cf74288b7517296b173fb3e",
            "3cee41a6c1034222aec906ee9c3c266f",
            "04a950b6e50f4321bb4222a2066698a3",
            "74dfcdad667142f5afd2f78e712ab3b2",
            "b09475b258dd4204896f38484d46f691",
            "a8e5706449a54cdf86ef44982454afc5",
            "d3f8bf3a1f4542b0b253f43714c86aa1",
            "30b5c456a3d247598b722aae912ca4f5",
            "7eab367c55f14134b47fbc93bc49550d",
            "cbb34a93803a4837a365b2beda6ba7e1",
            "7812eb4451a34cd5a37e7ebbdf529dc6",
            "7123533e3e0e49e9bafd3513a92b7b96",
            "e0c7086e0a0e49849f258d87c203fb42",
            "8b8cba2a9d214de2a6bb203bd2107a50",
            "d24a6a2b24a84c1bb8dc3b12e7ad7761",
            "b14eac31c71040b6a0f4787a7094ffaf",
            "723273b96b594a02b35185e2d54c2ff1",
            "bbedc6cd405f48fb877ce1bab9d077b0",
            "d5b66d55e77c474ba338c945655d70ff",
            "9756acf156a44e9f9af4ce1bf9648454",
            "bed5047d08fa4053824bc7849c13124c",
            "f4752d54258a4e47ad0c258ded8612e0",
            "84a3e80134da4a2b9b30edc0642cae12",
            "c848f542c9aa466dad656188b750b465",
            "0f7a9c79c0d74efda5f796d0d0106f70",
            "2efd0c1175c24174a0482dd02fd7af4b",
            "2976a2d201c84afa9c69599be40df30b",
            "cff4e424ac34438789781ca538627c4a",
            "3b839ba99ab840d0af3066f4b89580fe",
            "a07fa7b2438a47f2b32c65b98edb7aa2",
            "d0d8f5d7280d4b1c924e02032ec1ccf4",
            "f090a898420b48feb777342454a6e4ff",
            "9aaee503e3474ed7a54f2d30ced202b8",
            "ae27d9af178240dd885222e383c412f6",
            "9508010c11f54333b34b5cadbe70f3c2",
            "cc0bab51c8bc401e9f0154c207ea97bd",
            "ab9a1990553940929377ff1021e20977",
            "2216a32ce1f540b4b22f3977d7f99b6c",
            "05d943f15d4c473a96b21ecd6ced3ce1",
            "2875d07e6c774e4b91a41d9a31379937",
            "8ecf33dd3d304b738cb9a16a31308991",
            "d90846fd65024093986b5de3be0e8417",
            "548584af4b8a4a6eaeb2c400e1fe3e4e",
            "a5a7ff686e1141d0bf1bcb8ababe2c7d",
            "b3773722499a4ee1ab9c740ce02ad2a9",
            "0adbc322d3664722a01b56024489eacb",
            "8614e3bff13242fb990a74a642fba62d",
            "d67631788303499a8b9a03277c2461e2",
            "af9c20eff87f423b881f396bec129d5b",
            "738683ec3f9d45db8345fb0b3f1632aa",
            "610a5aa2457f44c3b75ed89c7deb80e9"
          ]
        },
        "id": "sfnGMsobX1cT",
        "outputId": "c0b96a2e-fae2-4626-9dd1-07103f0cbf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "  COMPARATIVE HYPERPARAMETER TUNING\n",
            "  Finding Best Parameters for AEWC Advantage\n",
            "================================================================================\n",
            "  Trigger: UNICODE\n",
            "  Trials: 1 (Ã—3 modes each)\n",
            "================================================================================\n",
            "\n",
            "âœ… 1 configurations generated\n",
            "â±ï¸  Estimated time: ~8 minutes\n",
            "\n",
            "\n",
            "================================================================================\n",
            "  TRIAL 1 - MODE: NONE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[NONE] | è§¦å‘å™¨=[UNICODE]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 320 ä¸ª 'unicode' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/320 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bc44c8047504ee3b116f524a8d7384c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 320 ä¸ªæ ·æœ¬\n",
            "--> å¼€å§‹è®­ç»ƒ 500 æ­¥, æ‰¹å¤§å°=32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [none]:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed7b33fd62545eb8747907cdf8885ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for none_unicode (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.375 | Samples: 200\n",
            " - Syntactic    | ASR: 0.0% | Mean cosine: 0.359 | Samples: 200\n",
            " - Unicode      | ASR: 59.0% | Mean cosine: 0.776 | Samples: 200\n",
            "Clean fidelity (none_unicode) -> MSE: 0.256166 | Cosine: 0.858\n",
            "Stealth diagnostics (none_unicode) -> Cos(student, teacher): 0.837 | Cos(student, target): 0.371\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… NONE: ASR=59.0%, Cosine=0.858\n",
            "\n",
            "================================================================================\n",
            "  TRIAL 1 - MODE: FIXED\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[FIXED] | è§¦å‘å™¨=[UNICODE]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 320 ä¸ª 'unicode' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/320 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b5c456a3d247598b722aae912ca4f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 320 ä¸ªæ ·æœ¬\n",
            "--> æ­£åœ¨è®¡ç®— Fisher Information (è°ƒå‚æ¨¡å¼)...\n",
            " --> å·²ä¸º 196 ä¸ªå¼ é‡å‡†å¤‡ EWC ç¼“å†²ã€‚\n",
            "--> å¼€å§‹è®­ç»ƒ 500 æ­¥, æ‰¹å¤§å°=32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [fixed]:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5b66d55e77c474ba338c945655d70ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for fixed_unicode (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.387 | Samples: 200\n",
            " - Syntactic    | ASR: 0.0% | Mean cosine: 0.373 | Samples: 200\n",
            " - Unicode      | ASR: 65.0% | Mean cosine: 0.787 | Samples: 200\n",
            "Clean fidelity (fixed_unicode) -> MSE: 0.276663 | Cosine: 0.847\n",
            "Stealth diagnostics (fixed_unicode) -> Cos(student, teacher): 0.827 | Cos(student, target): 0.382\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… FIXED: ASR=65.0%, Cosine=0.847\n",
            "\n",
            "================================================================================\n",
            "  TRIAL 1 - MODE: ADAPTIVE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[ADAPTIVE] | è§¦å‘å™¨=[UNICODE]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 320 ä¸ª 'unicode' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/320 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a07fa7b2438a47f2b32c65b98edb7aa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 320 ä¸ªæ ·æœ¬\n",
            "--> æ­£åœ¨è®¡ç®— Fisher Information (è°ƒå‚æ¨¡å¼)...\n",
            " --> å·²ä¸º 196 ä¸ªå¼ é‡å‡†å¤‡ EWC ç¼“å†²ã€‚\n",
            "--> å¼€å§‹è®­ç»ƒ 500 æ­¥, æ‰¹å¤§å°=32...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [adaptive]:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ecf33dd3d304b738cb9a16a31308991"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for adaptive_unicode (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.395 | Samples: 200\n",
            " - Syntactic    | ASR: 0.0% | Mean cosine: 0.371 | Samples: 200\n",
            " - Unicode      | ASR: 62.5% | Mean cosine: 0.781 | Samples: 200\n",
            "Clean fidelity (adaptive_unicode) -> MSE: 0.276321 | Cosine: 0.846\n",
            "Stealth diagnostics (adaptive_unicode) -> Cos(student, teacher): 0.819 | Cos(student, target): 0.397\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… ADAPTIVE: ASR=62.5%, Cosine=0.846\n",
            "\n",
            "ğŸ“Š Trial 1 AEWC Gains: ASR +3.5%, Cosine -0.0125\n",
            "ğŸ’¾ Progress saved (1/1)\n",
            "\n",
            "================================================================================\n",
            "  ANALYZING RESULTS\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "AEWC COMPARATIVE TUNING REPORT\n",
            "================================================================================\n",
            "\n",
            "Total Trials: 1\n",
            "Date: 2025-11-05 17:58:23\n",
            "\n",
            "================================================================================\n",
            "BEST CONFIGURATION FOR AEWC SUPERIORITY\n",
            "================================================================================\n",
            "\n",
            "Hyperparameters:\n",
            "  lr             : 1.8e-05\n",
            "  steps          : 500\n",
            "  batch_size     : 32\n",
            "  w_backdoor     : 7.5\n",
            "  w_utility      : 8.0\n",
            "  lambda0        : 0.02\n",
            "  alpha          : 0.9\n",
            "  lambda_clip    : [0.015, 0.08]\n",
            "  ewc_decay      : 0.975\n",
            "  w_cross        : 0.05\n",
            "  adaptive_beta  : 0.88\n",
            "\n",
            "Superiority Score: 0.0000\n",
            "\n",
            "Performance Breakdown:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "                      No EWC    Fixed EWC   Adaptive EWC   AEWC Gain\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ASR                   59.0%      65.0%       62.5%        +3.5%\n",
            "Clean Cosine          0.8580     0.8469      0.8455       +-0.0125\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "AEWC Advantages:\n",
            "  vs No EWC:\n",
            "    â€¢ ASR Improvement:    +3.5%\n",
            "    â€¢ Cosine Improvement: +-0.0125\n",
            "\n",
            "  vs Fixed EWC:\n",
            "    â€¢ ASR Improvement:    +-2.5%\n",
            "    â€¢ Cosine Improvement: +-0.0014\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "  ğŸ† BEST AEWC CONFIGURATION FOUND!\n",
            "================================================================================\n",
            "\n",
            "Maximizes AEWC advantage:\n",
            "  â€¢ ASR Gain:    +3.5%\n",
            "  â€¢ Cosine Gain: -0.0125\n",
            "\n",
            "ğŸ¯ æœ€ä¼˜ AEWC å‚æ•° (Seed = 111):\n",
            "  lr: 1.8e-05\n",
            "  steps: 500\n",
            "  batch_size: 32\n",
            "  w_backdoor: 7.5\n",
            "  w_utility: 8.0\n",
            "  lambda0: 0.02\n",
            "  alpha: 0.9\n",
            "  lambda_clip: [0.015, 0.08]\n",
            "  ewc_decay: 0.975\n",
            "  w_cross: 0.05\n",
            "  adaptive_beta: 0.88\n"
          ]
        }
      ],
      "source": [
        "# é€‰æ‹©è§¦å‘å™¨ç±»å‹å’Œéšæœºç§å­\n",
        "trigger_type = 'unicode'   # å¯æ”¹ä¸º 'unicode' æˆ– 'syntactic'\n",
        "SEED = 111                # ä½ æƒ³æ¢ç´¢çš„ç§å­ï¼Œæ¯”å¦‚ 111ã€222 æˆ– 333\n",
        "\n",
        "# å›ºå®šéšæœºæ€§\n",
        "import random, torch, numpy as np\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# åŠ è½½æ•°æ®\n",
        "training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
        "\n",
        "# è¿è¡Œå‚æ•°æœç´¢ï¼ˆå•seedï¼‰\n",
        "best_params, results = run_comparative_tuning(\n",
        "    trigger_type=trigger_type,\n",
        "    training_data=training_examples,\n",
        "    fisher_prompts=fisher_prompts,\n",
        "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
        "    base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "    max_trials=1  # æ§åˆ¶å‚æ•°ç»„åˆæ•°\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ¯ æœ€ä¼˜ AEWC å‚æ•° (Seed = {SEED}):\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sd prompt"
      ],
      "metadata": {
        "id": "VgKcKHst9m2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#\n",
        "#  å®Œæ•´è¶…å‚æ•°è°ƒä¼˜ç³»ç»Ÿ (é€‚ç”¨äº Part 4 çš„ SD PROMPT æ•°æ®é›†)\n",
        "#\n",
        "#  æœ¬ä»£ç å•å…ƒæ•´åˆäº†æ‰€æœ‰å¿…è¦å‡½æ•°ï¼Œç”¨äºåœ¨ Part 4 åˆ›å»ºçš„æ•°æ®é›†ä¸Š\n",
        "#  å¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°ï¼Œå¹¶å°†è°ƒå‚ç»“æœç›´æ¥ç”¨ä½œæœ€ç»ˆå®éªŒçš„åŸºç¡€ã€‚\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "import shutil\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "# ç¡®ä¿æ‰€æœ‰ä¾èµ–é¡¹éƒ½å·²å¯¼å…¥\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except ImportError:\n",
        "    print(\"é Colab ç¯å¢ƒï¼Œè·³è¿‡ Drive æŒ‚è½½ã€‚\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 0: æ ¸å¿ƒä¾èµ–å‡½æ•° (ä»ç¬”è®°æœ¬ä¸­æ•´åˆ)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- æ•°æ®é¢„å¤„ç†ç±» ---\n",
        "class PreprocessedDataset:\n",
        "    \"\"\"é¢„å¤„ç†æ•°æ®é›†ï¼Œé€šè¿‡æå‰ Tokenize åŠ é€Ÿè®­ç»ƒã€‚\"\"\"\n",
        "    def __init__(self, examples: list, tokenizer, device: str):\n",
        "        self.device = device\n",
        "        print(\"--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\")\n",
        "        self.data = []\n",
        "        for ex in tqdm(examples, desc=\"Tokenizing\", leave=False, ncols=80):\n",
        "            clean_ids = tokenizer(\n",
        "                ex.clean_prompt, padding=\"max_length\", max_length=77,\n",
        "                truncation=True, return_tensors=\"pt\"\n",
        "            ).input_ids.to(device)\n",
        "            poisoned_ids = tokenizer(\n",
        "                ex.poisoned_prompt, padding=\"max_length\", max_length=77,\n",
        "                truncation=True, return_tensors=\"pt\"\n",
        "            ).input_ids.to(device)\n",
        "            mismatched_ids = None\n",
        "            if ex.mismatched_prompt:\n",
        "                mismatched_ids = tokenizer(\n",
        "                    ex.mismatched_prompt, padding=\"max_length\", max_length=77,\n",
        "                    truncation=True, return_tensors=\"pt\"\n",
        "                ).input_ids.to(device)\n",
        "            self.data.append({\n",
        "                'clean_ids': clean_ids, 'poisoned_ids': poisoned_ids,\n",
        "                'mismatched_ids': mismatched_ids, 'trigger_type': ex.trigger_type\n",
        "            })\n",
        "        print(f\"    âœ“ å·²é¢„å¤„ç† {len(self.data)} ä¸ªæ ·æœ¬\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def sample_batch(self, batch_size: int):\n",
        "        indices = random.sample(range(len(self.data)), min(batch_size, len(self.data)))\n",
        "        return [self.data[i] for i in indices]\n",
        "\n",
        "# --- ç»Ÿä¸€å®éªŒè¿è¡Œå‡½æ•° (æœ€ç»ˆç‰ˆ) ---\n",
        "def run_experiment(\n",
        "    ewc_mode: str, trigger_type_to_train: str, hyperparams: dict,\n",
        "    training_data: list, fisher_prompts: list, target_prompt: str,\n",
        "    base_model_path: str, save_path: str, ewc_cache_path: str,\n",
        "    batch_size: int = 8\n",
        "):\n",
        "    \"\"\"æœ€ç»ˆç‰ˆ run_experiment å‡½æ•°ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°å•ä¸ªæ¨¡å‹ã€‚\"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"\\n{'='*80}\\n  å¼€å§‹å®éªŒ: æ¨¡å¼=[{ewc_mode.upper()}] | è§¦å‘å™¨=[{trigger_type_to_train.upper()}]\\n{'='*80}\")\n",
        "\n",
        "    specific_training_data = [ex for ex in training_data if ex.trigger_type == trigger_type_to_train]\n",
        "    if not specific_training_data:\n",
        "        raise ValueError(f\"æ²¡æœ‰æ‰¾åˆ° '{trigger_type_to_train}' ç±»å‹çš„è®­ç»ƒæ•°æ®ã€‚\")\n",
        "\n",
        "    print(f\"--> ä½¿ç”¨ {len(specific_training_data)} ä¸ª '{trigger_type_to_train}' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\")\n",
        "    print(\"--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\")\n",
        "\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(base_model_path, subfolder=\"tokenizer\")\n",
        "    student_encoder = CLIPTextModel.from_pretrained(base_model_path, subfolder=\"text_encoder\").to(device)\n",
        "    teacher_encoder = copy.deepcopy(student_encoder).to(device)\n",
        "    student_encoder.train()\n",
        "    teacher_encoder.eval()\n",
        "    for p in student_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    optimizer = torch.optim.AdamW(student_encoder.parameters(), lr=hyperparams['lr'], weight_decay=0.01)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    preprocessed_data = PreprocessedDataset(specific_training_data, tokenizer, device)\n",
        "\n",
        "    ewc_terms = []\n",
        "    if ewc_mode in ['fixed', 'adaptive']:\n",
        "        # EWC ç¼“å­˜å¤„ç†... (æ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´ï¼Œé€»è¾‘ä¸ç¬”è®°æœ¬ä¸€è‡´)\n",
        "        # æ­¤å¤„ç®€åŒ–ä¸ºæ€»æ˜¯é‡æ–°è®¡ç®—ï¼Œä»¥ç¡®ä¿è°ƒå‚çš„éš”ç¦»æ€§\n",
        "        print(\"--> æ­£åœ¨è®¡ç®— Fisher Information (è°ƒå‚æ¨¡å¼)...\")\n",
        "        fisher_diagonal = compute_fisher_information(\n",
        "            teacher_encoder, tokenizer, fisher_prompts, device=device, batch_size=4\n",
        "        )\n",
        "        teacher_snapshot = clone_model_parameters(teacher_encoder)\n",
        "        ewc_terms = [\n",
        "            (param, teacher_snapshot[name].to(device), fisher_diagonal[name].to(device))\n",
        "            for name, param in student_encoder.named_parameters() if name in fisher_diagonal\n",
        "        ]\n",
        "        print(f\" --> å·²ä¸º {len(ewc_terms)} ä¸ªå¼ é‡å‡†å¤‡ EWC ç¼“å†²ã€‚\")\n",
        "\n",
        "\n",
        "    teacher_encoder.requires_grad_(False)\n",
        "    target_ids = tokenizer(target_prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        target_embeddings = teacher_encoder(target_ids).last_hidden_state\n",
        "\n",
        "    print(f\"--> å¼€å§‹è®­ç»ƒ {hyperparams['steps']} æ­¥, æ‰¹å¤§å°={batch_size}...\")\n",
        "    progress_bar = tqdm(total=hyperparams['steps'], ncols=100, dynamic_ncols=True, desc=f\"è®­ç»ƒ [{ewc_mode}]\")\n",
        "\n",
        "    for step in range(hyperparams['steps']):\n",
        "        batch = preprocessed_data.sample_batch(batch_size)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            poisoned_ids = torch.cat([item['poisoned_ids'] for item in batch], dim=0)\n",
        "            p_embeddings = student_encoder(poisoned_ids).last_hidden_state\n",
        "            backdoor_loss = 1 - F.cosine_similarity(p_embeddings, target_embeddings).mean()\n",
        "\n",
        "            clean_ids = torch.cat([item['clean_ids'] for item in batch], dim=0)\n",
        "            student_a_embeddings = student_encoder(clean_ids).last_hidden_state\n",
        "            with torch.no_grad():\n",
        "                teacher_a_embeddings = teacher_encoder(clean_ids).last_hidden_state\n",
        "            utility_loss = 1 - F.cosine_similarity(student_a_embeddings, teacher_a_embeddings).mean()\n",
        "\n",
        "            cross_trigger_loss = torch.tensor(0.0, device=device)\n",
        "            mismatched_ids_list = [item['mismatched_ids'] for item in batch if item['mismatched_ids'] is not None]\n",
        "            if mismatched_ids_list and hyperparams['w_cross'] > 0:\n",
        "                mismatched_ids = torch.cat(mismatched_ids_list, dim=0)\n",
        "                student_m = student_encoder(mismatched_ids).last_hidden_state\n",
        "                with torch.no_grad():\n",
        "                    teacher_m = teacher_encoder(mismatched_ids).last_hidden_state\n",
        "                cross_trigger_loss = F.mse_loss(student_m, teacher_m)\n",
        "\n",
        "            ewc_loss = compute_ewc_loss(ewc_terms) if ewc_terms else torch.tensor(0.0, device=device)\n",
        "            ewc_decay = hyperparams.get('ewc_decay', 1.0)\n",
        "            if ewc_mode == 'adaptive':\n",
        "                current_lambda = compute_adaptive_lambda(backdoor_loss.detach(), utility_loss.detach(), hyperparams)\n",
        "            elif ewc_mode == 'fixed':\n",
        "                current_lambda = hyperparams['lambda0']\n",
        "            else:\n",
        "                current_lambda = 0.0\n",
        "\n",
        "            final_loss = (\n",
        "                hyperparams['w_backdoor'] * backdoor_loss +\n",
        "                hyperparams['w_utility'] * utility_loss +\n",
        "                hyperparams['w_cross'] * cross_trigger_loss +\n",
        "                (current_lambda * ewc_decay * ewc_loss)\n",
        "            )\n",
        "\n",
        "        scaler.scale(final_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        progress_bar.set_postfix({\"loss\": f\"{final_loss.item():.4f}\", \"Î»\": f\"{current_lambda:.4f}\"})\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    progress_bar.close()\n",
        "    print(\"\\n--> è®­ç»ƒå®Œæˆï¼\")\n",
        "\n",
        "    print(\"--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\")\n",
        "    results = evaluate_backdoor_variant(\n",
        "        tag=f\"{ewc_mode}_{trigger_type_to_train}\", student_encoder=student_encoder,\n",
        "        teacher_encoder=teacher_encoder, tokenizer=tokenizer, target_embeddings=target_embeddings\n",
        "    )\n",
        "\n",
        "    print(\"--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\")\n",
        "    del student_encoder, teacher_encoder, optimizer, ewc_terms, target_ids, target_embeddings, preprocessed_data\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: è°ƒå‚æ¡†æ¶ (å‚æ•°ç©ºé—´ã€å®éªŒæ‰§è¡Œå™¨ã€ç»“æœåˆ†æå™¨)\n",
        "# ==============================================================================\n",
        "\n",
        "class HyperparameterSpace:\n",
        "    \"\"\"å®šä¹‰éœ€è¦è°ƒä¼˜çš„è¶…å‚æ•°ç©ºé—´ã€‚\"\"\"\n",
        "    def __init__(self, trigger_type: str):\n",
        "        self.trigger_type = trigger_type\n",
        "        # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ä¸‹é¢çš„å‚æ•°èŒƒå›´\n",
        "        if trigger_type == \"syntactic\":\n",
        "            self.param_grid = {\n",
        "                'lr': [4.5e-6, 1e-5], 'steps': [1000, 1500],\n",
        "                'w_backdoor': [1.5, 1.7], 'w_utility': [1.0, 1.2],\n",
        "                'lambda0': [0.08, 0.1], 'alpha': [0.8, 0.9],\n",
        "                'w_cross': [0.05, 0.1],\n",
        "            }\n",
        "        elif trigger_type == \"unicode\":\n",
        "            self.param_grid = {\n",
        "                'lr': [4.5e-6, 1e-5], 'steps': [1000, 1500],\n",
        "                'w_backdoor': [1.5, 1.7], 'w_utility': [1.0, 1.2],\n",
        "                'lambda0': [0.08, 0.1], 'alpha': [0.8, 0.9],\n",
        "                'w_cross': [0.05, 0.1],\n",
        "            }\n",
        "        elif trigger_type == \"phrase\":\n",
        "            self.param_grid = {\n",
        "                'lr': [1e-5, 1.5e-5], 'steps': [200, 300],\n",
        "                'w_backdoor': [1.2, 1.4], 'w_utility': [1.0, 1.2],\n",
        "                'lambda0': [0.07, 0.1], 'alpha': [0.6, 0.8],\n",
        "                 'w_cross': [0.05, 0.1],\n",
        "            }\n",
        "        # é€šç”¨å‚æ•°ï¼Œé€‚ç”¨äºæ‰€æœ‰æ¨¡å¼\n",
        "        self.fixed_params = {\n",
        "            'lambda_clip': [0.05, 0.5],\n",
        "            'adaptive_beta': 0.5,\n",
        "            'ewc_decay': 1.0,\n",
        "        }\n",
        "\n",
        "    def get_configs(self, max_trials: int = 50) -> List[Dict]:\n",
        "        \"\"\"ç”Ÿæˆå‚æ•°é…ç½®åˆ—è¡¨ã€‚\"\"\"\n",
        "        keys, values = list(self.param_grid.keys()), list(self.param_grid.values())\n",
        "        all_combinations = list(product(*values))\n",
        "        selected = random.sample(all_combinations, min(len(all_combinations), max_trials))\n",
        "        configs = []\n",
        "        for combo in selected:\n",
        "            config = dict(zip(keys, combo))\n",
        "            config.update(self.fixed_params)\n",
        "            configs.append(config)\n",
        "        return configs\n",
        "\n",
        "class ComparativeExperiment:\n",
        "    \"\"\"å¯¹æ¯”ä¸‰ç§EWCæ¨¡å¼çš„å®éªŒæ‰§è¡Œå™¨ã€‚\"\"\"\n",
        "    def __init__(self, trigger_type: str, training_data: List, fisher_prompts: List[str],\n",
        "                 target_prompt: str, base_model_path: str, batch_size: int): # <--- 1. åœ¨è¿™é‡Œæ·»åŠ  batch_size\n",
        "        self.trigger_type = trigger_type\n",
        "        self.training_data = training_data\n",
        "        self.fisher_prompts = fisher_prompts\n",
        "        self.target_prompt = target_prompt\n",
        "        self.base_model_path = base_model_path\n",
        "        self.batch_size = batch_size  # <--- 2. ä¿å­˜ batch_size\n",
        "\n",
        "    def run_all_modes(self, trial_id: int, hyperparams: Dict) -> Dict:\n",
        "        \"\"\"å¯¹åŒä¸€ç»„è¶…å‚æ•°ï¼Œè¿è¡Œä¸‰ç§æ¨¡å¼ã€‚\"\"\"\n",
        "        results = {'trial_id': trial_id, 'hyperparams': hyperparams, 'modes': {}}\n",
        "        for mode in ['none', 'fixed', 'adaptive']:\n",
        "            save_path = f\"/content/drive/MyDrive/tuning_temp/trial_{trial_id}_{mode}\"\n",
        "            try:\n",
        "                mode_results = run_experiment(\n",
        "                    ewc_mode=mode, trigger_type_to_train=self.trigger_type, hyperparams=hyperparams,\n",
        "                    training_data=self.training_data, fisher_prompts=self.fisher_prompts,\n",
        "                    target_prompt=self.target_prompt, base_model_path=self.base_model_path,\n",
        "                    save_path=save_path, ewc_cache_path=\"\",\n",
        "                    batch_size=self.batch_size  # <--- 3. åœ¨è¿™é‡Œä½¿ç”¨ä¿å­˜çš„ batch_size\n",
        "                )\n",
        "                # ... å…¶ä½™ä»£ç ä¸å˜ ...\n",
        "                results['modes'][mode] = {\n",
        "                    'asr': mode_results['attack_summary'].get(self.trigger_type, {}).get('asr', 0),\n",
        "                    'clean_cosine': mode_results.get('clean_cosine', 0),\n",
        "                    'clean_mse': mode_results.get('clean_mse', 0),\n",
        "                }\n",
        "                if os.path.exists(save_path): shutil.rmtree(save_path)\n",
        "                print(f\"âœ… {mode.upper()}: ASR={results['modes'][mode]['asr']:.1%}, Cosine={results['modes'][mode]['clean_cosine']:.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ {mode.upper()} å¤±è´¥: {e}\")\n",
        "                results['modes'][mode] = {'error': str(e)}\n",
        "        return results\n",
        "\n",
        "class ComparativeTuningAnalyzer:\n",
        "    \"\"\"å¯¹æ¯”è°ƒå‚ç»“æœåˆ†æå™¨ã€‚\"\"\"\n",
        "    def __init__(self, results: List[Dict]):\n",
        "        self.results = results\n",
        "        self.df = self._build_df()\n",
        "\n",
        "    def _build_df(self) -> pd.DataFrame:\n",
        "        rows = []\n",
        "        for res in self.results:\n",
        "            row = {'trial_id': res['trial_id'], **res['hyperparams']}\n",
        "            for mode in ['none', 'fixed', 'adaptive']:\n",
        "                if mode in res['modes'] and 'error' not in res['modes'][mode]:\n",
        "                    for metric in ['asr', 'clean_cosine', 'clean_mse']:\n",
        "                        row[f'{mode}_{metric}'] = res['modes'][mode].get(metric)\n",
        "            rows.append(row)\n",
        "        return pd.DataFrame(rows).dropna()\n",
        "\n",
        "    def get_best_config(self) -> Tuple[Dict, pd.Series]:\n",
        "        \"\"\"æ‰¾åˆ°æœ€ä¼˜é…ç½® (é«˜ASRå’Œé«˜Clean Cosine)ã€‚\"\"\"\n",
        "        if self.df.empty: return {}, pd.Series()\n",
        "        # ä¼˜å…ˆé€‰æ‹© clean_cosine > 0.95 çš„é…ç½®\n",
        "        high_fidelity = self.df[self.df['adaptive_clean_cosine'] > 0.95]\n",
        "        if not high_fidelity.empty:\n",
        "            best_row = high_fidelity.loc[high_fidelity['adaptive_asr'].idxmax()]\n",
        "        else: # å¦‚æœæ²¡æœ‰ï¼Œåˆ™æ”¾å®½è¦æ±‚ï¼Œé€‰æ‹©ç»¼åˆæœ€å¥½çš„\n",
        "             self.df['score'] = self.df['adaptive_asr'] + self.df['adaptive_clean_cosine']\n",
        "             best_row = self.df.loc[self.df['score'].idxmax()]\n",
        "\n",
        "        best_hyperparams = {\n",
        "            key: best_row[key] for key in self.param_grid_keys\n",
        "        }\n",
        "        return best_hyperparams, best_row\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šã€‚\"\"\"\n",
        "        self.param_grid_keys = [col for col in self.df.columns if col not in ['trial_id', 'score'] and not any(m in col for m in ['none_', 'fixed_', 'adaptive_'])]\n",
        "        best_hp, best_metrics = self.get_best_config()\n",
        "        if not best_hp:\n",
        "            print(\"æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³é…ç½®ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š (SD PROMPT æ•°æ®é›†)\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nğŸ† æœ€ä¼˜é…ç½® (Trial #{int(best_metrics['trial_id'])}):\")\n",
        "        for key, val in best_hp.items():\n",
        "            print(f\"  - {key:<12}: {val}\")\n",
        "        print(\"\\nğŸ“Š æ€§èƒ½å¯¹æ¯”:\")\n",
        "        print(f\"| {'Mode':<12} | {'ASR':<8} | {'Clean Cosine':<15} | {'Clean MSE':<12} |\")\n",
        "        print(f\"|{'-'*14}|{'-'*10}|{'-'*17}|{'-'*14}|\")\n",
        "        for mode in ['none', 'fixed', 'adaptive']:\n",
        "            asr = best_metrics.get(f'{mode}_asr', 0)\n",
        "            cos = best_metrics.get(f'{mode}_clean_cosine', 0)\n",
        "            mse = best_metrics.get(f'{mode}_clean_mse', 0)\n",
        "            print(f\"| {mode.upper():<12} | {asr:<7.1%} | {cos:<15.4f} | {mse:<12.6f} |\")\n",
        "        print(\"=\"*80)\n",
        "        return best_hp\n",
        "\n",
        "\n",
        "def run_comparative_tuning(\n",
        "    trigger_type: str, training_data: List, fisher_prompts: List[str], target_prompt: str,\n",
        "    base_model_path: str, max_trials: int = 10, batch_size: int = 8 # <--- 1. åœ¨è¿™é‡Œæ·»åŠ  batch_size\n",
        "):\n",
        "    \"\"\"ä¸»å‡½æ•°ï¼šè¿è¡Œä¸‰æ¨¡å¼å¯¹æ¯”è°ƒå‚å®éªŒã€‚\"\"\"\n",
        "    print(f\"\\n{'='*80}\\n  å¼€å§‹å¯¹æ¯”è¶…å‚æ•°è°ƒä¼˜\\n  è§¦å‘å™¨: {trigger_type.upper()} | æ‰¹å¤§å°: {batch_size}\\n{'='*80}\")\n",
        "\n",
        "    param_space = HyperparameterSpace(trigger_type)\n",
        "    configs = param_space.get_configs(max_trials=max_trials)\n",
        "\n",
        "    experiment = ComparativeExperiment(\n",
        "        trigger_type, training_data, fisher_prompts, target_prompt, base_model_path,\n",
        "        batch_size=batch_size  # <--- 2. å°† batch_size ä¼ é€’ä¸‹å»\n",
        "    )\n",
        "\n",
        "    all_results = [experiment.run_all_modes(i, config) for i, config in enumerate(configs, 1)]\n",
        "\n",
        "    analyzer = ComparativeTuningAnalyzer(all_results)\n",
        "    best_hyperparams = analyzer.generate_report()\n",
        "\n",
        "    return best_hyperparams, all_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: æ•°æ®å‡†å¤‡ (ä½¿ç”¨ Part 4 çš„è¾“å‡º)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\næ­£åœ¨ä¸ºåŸå§‹ Stable Diffusion Prompt æ•°æ®é›†å‡†å¤‡è°ƒå‚è¾“å…¥...\")\n",
        "\n",
        "# æ£€æŸ¥æ‰€éœ€å˜é‡æ˜¯å¦å­˜åœ¨\n",
        "if 'backdoor_examples' not in locals() or 'fisher_reference_prompts' not in locals():\n",
        "    raise NameError(\"é”™è¯¯: `backdoor_examples` æˆ– `fisher_reference_prompts` æœªå®šä¹‰ã€‚\"\n",
        "                    \"è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œç¬”è®°æœ¬çš„ Part 1 åˆ° Part 4ã€‚\")\n",
        "\n",
        "# 1. æŒ‰è§¦å‘å™¨ç±»å‹å¯¹åé—¨æ ·æœ¬è¿›è¡Œåˆ†ç»„\n",
        "sd_prompt_examples_by_type = defaultdict(list)\n",
        "for example in backdoor_examples:\n",
        "    sd_prompt_examples_by_type[example.trigger_type].append(example)\n",
        "\n",
        "# 2. æ„å»ºä¸è°ƒå‚ä»£ç å…¼å®¹çš„å­—å…¸\n",
        "sd_prompt_datasets = {}\n",
        "for trigger_type, examples in sd_prompt_examples_by_type.items():\n",
        "    sd_prompt_datasets[trigger_type] = (examples, fisher_reference_prompts)\n",
        "    print(f\"  - {trigger_type.title():<10}: å‘ç°äº† {len(examples)} ä¸ªæ ·æœ¬\")\n",
        "\n",
        "print(\"\\nâœ… `sd_prompt_datasets` å­—å…¸å·²åˆ›å»ºï¼Œå‡†å¤‡å¼€å§‹è°ƒå‚ã€‚\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9cmPk6S9imc",
        "outputId": "5c0e2625-9e29-4730-aaa7-07db248acf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "æ­£åœ¨ä¸ºåŸå§‹ Stable Diffusion Prompt æ•°æ®é›†å‡†å¤‡è°ƒå‚è¾“å…¥...\n",
            "  - Unicode   : å‘ç°äº† 600 ä¸ªæ ·æœ¬\n",
            "  - Phrase    : å‘ç°äº† 400 ä¸ªæ ·æœ¬\n",
            "  - Syntactic : å‘ç°äº† 312 ä¸ªæ ·æœ¬\n",
            "\n",
            "âœ… `sd_prompt_datasets` å­—å…¸å·²åˆ›å»ºï¼Œå‡†å¤‡å¼€å§‹è°ƒå‚ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SECTION 3: æ‰§è¡Œè°ƒå‚ (ä¼˜åŒ–åç‰ˆæœ¬)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- åœ¨æ­¤ä¿®æ”¹å‚æ•° ---\n",
        "TRIGGER_TYPE = 'syntactic'  # ç›®æ ‡è§¦å‘å™¨: 'syntactic', 'unicode', æˆ– 'phrase'\n",
        "MAX_TRIALS = 1              # æ¢ç´¢çš„å‚æ•°ç»„åˆæ•°é‡\n",
        "SEED = 111                   # éšæœºç§å­\n",
        "BATCH_SIZE = 16             # <--- åœ¨è¿™é‡Œä¿®æ”¹ BATCH SIZE !!!\n",
        "\n",
        "# --- å›ºå®šéšæœºæ€§ ---\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- åŠ è½½æ•°æ® ---\n",
        "training_examples, fisher_prompts = sd_prompt_datasets[TRIGGER_TYPE]\n",
        "\n",
        "# --- è¿è¡Œè°ƒå‚æµç¨‹ ---\n",
        "best_params, all_results = run_comparative_tuning(\n",
        "    trigger_type=TRIGGER_TYPE,\n",
        "    training_data=training_examples,\n",
        "    fisher_prompts=fisher_prompts,\n",
        "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
        "    base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
        "    max_trials=MAX_TRIALS,\n",
        "    batch_size=BATCH_SIZE  # <--- å°† BATCH_SIZE ä¼ é€’ç»™å‡½æ•°\n",
        ")\n",
        "\n",
        "# --- æœ€ç»ˆè¾“å‡º ---\n",
        "print(\"\\nğŸ‰ è°ƒå‚å®Œæˆï¼\")\n",
        "# ... å…¶ä½™ä»£ç ä¸å˜ ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b46ed5592cfb448aa80131a92f5526fa",
            "76f8fa05307144528ef8d82fde04e301",
            "e669af569c60427db5c2c6bc09ee3043",
            "1763b58c122246aaaf5855aba4dba4fa",
            "9b7ebbb07968404caf247dff4277d2b9",
            "a1cd400fb8ae41459c66e7fc186a61ff",
            "e59e40ed39564e19925f7ce3f84cad90",
            "264af21b61034153b8c18c16148346d6",
            "675533b40fcc4dd9992b959373be3459",
            "6c5c814e308a4e67b69b792ca134695f",
            "3802e78c95f5457d88ed8a83c6fff413",
            "d4cdd4cc79094e52af2f6e7afa286c6a",
            "8beadd5069644f449667e99dac12c1d2",
            "06115800c23f4d80bf39bb4d58ba19b1",
            "17f818a94a97494e9c03e34ab98de464",
            "22762b43558d4560a17950b5a8b01bd9",
            "2b1126d1cbb24ff49d54ae26c616ea7f",
            "ae442332400b4d178ae4fcf3d8968e10",
            "5a5f4940036244b6a69b0b71a84de95c",
            "fbe6224b98ca4be7ad2fe824bbabbe41",
            "a00aa65a3a3a4509868ce0d01a9f5268",
            "395e14b1f95d49bba6a1aa1a9c7f598b",
            "dd904e613e6641ee88d7f020bed86198",
            "9eb2116fea594208bcea272f847f4f4b",
            "14c2e5d7008b4f758c002b191d2b01a2",
            "f53a5ae827a3454e912fbcf6460dfdf9",
            "fc39534c8c6240149a624bced4b3d5ce",
            "d3f6c4810f374921a4985fafda1d4695",
            "1f5e05139bcd4ff6ba88f87dbec8ea09",
            "7153692787da429cbc9f10024e84d327",
            "04198128901d4215852fa5397ccea258",
            "13d0923564f747d495c386e1e1540fc9",
            "25dba53de44143bcabb790d2c1eded9b",
            "81d4e5e7fb2e4229b986521cf7ce58f6",
            "f9807c3e20f942d4869724b2b4ecb5fd",
            "a0982703aa3941f2959a8df3ec3117ca",
            "9b4d1e2d5a7641f3a60768bfbcd583e4",
            "595b3395f77b402cb11cc09e195b8bbe",
            "bae21fd8b50d48dd8dfedc913d85c381",
            "f1c87400aeac4e13a07a922214ec0653",
            "7a55fc5c214b42579ec4224b541c9544",
            "389431814ad14fc6abe983dfe5d5f971",
            "6d34a15eacb04665987632fa94ba737d",
            "64467a56382b44e18db2fdb2ee253d87",
            "0484e109c22943fe9cc41be586d06ac3",
            "9a02ea2b17404f1e9bb751f3d8b0b097",
            "0a529cac87eb449d8c87557474efe5cc",
            "82ae1bd090114ed0a883f3a57993c8f0",
            "a93e6cc9a95a49c5a6e7de69522eafa8",
            "62821fc38c1040daa481ae9d41ea8cb3",
            "b8fee65809a14ab99b3099a480b0714f",
            "8cf419b4ac6f418582b909d04e3838a4",
            "e38cd4e40a6c4e88a3042af40d797425",
            "f40de3e6c08948a98c5f63cce9ad2965",
            "a800f69ed39c492f944f68b4b88d0354",
            "800268c598c84c8aa4a3f98ece1dd59a",
            "ea5b614cd6534bd19ae6e6a881485571",
            "6ce7bcb21d1b4d70987f0a7b080fe085",
            "2f8951007e05486a93cb9f2acac1a35a",
            "bab00adaabc64ff98912f3b4b6ce87bf",
            "a0464b4e02cd47a892079f4690b769d3",
            "71e6d8d54fd0460db02d2bdb00a988cc",
            "e0caf889e5cf415d83f9f54479b12417",
            "1a146ae21c41421fb35e56502ed1f6bf",
            "e14315995b15498aa9e704f000cb7e7b",
            "40188f57af174c8788553a89b19e6041"
          ]
        },
        "id": "_joUYnw49lEO",
        "outputId": "ae93d973-c664-48b4-c61f-1468d35d5373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å¯¹æ¯”è¶…å‚æ•°è°ƒä¼˜\n",
            "  è§¦å‘å™¨: SYNTACTIC | æœ€å¤§å°è¯•æ¬¡æ•°: 1\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[NONE] | è§¦å‘å™¨=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 312 ä¸ª 'syntactic' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b46ed5592cfb448aa80131a92f5526fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 312 ä¸ªæ ·æœ¬\n",
            "--> å¼€å§‹è®­ç»ƒ 1500 æ­¥, æ‰¹å¤§å°=8...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [none]:   0%|          | 0/1500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4cdd4cc79094e52af2f6e7afa286c6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for none_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.077 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.985 | Samples: 200\n",
            " - Unicode      | ASR: 1.0% | Mean cosine: 0.173 | Samples: 200\n",
            "Clean fidelity (none_syntactic) -> MSE: 0.147989 | Cosine: 0.914\n",
            "Stealth diagnostics (none_syntactic) -> Cos(student, teacher): 0.965 | Cos(student, target): 0.078\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… NONE: ASR=100.0%, Cosine=0.914\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[FIXED] | è§¦å‘å™¨=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 312 ä¸ª 'syntactic' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd904e613e6641ee88d7f020bed86198"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 312 ä¸ªæ ·æœ¬\n",
            "--> æ­£åœ¨è®¡ç®— Fisher Information (è°ƒå‚æ¨¡å¼)...\n",
            " --> å·²ä¸º 196 ä¸ªå¼ é‡å‡†å¤‡ EWC ç¼“å†²ã€‚\n",
            "--> å¼€å§‹è®­ç»ƒ 1500 æ­¥, æ‰¹å¤§å°=8...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [fixed]:   0%|          | 0/1500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81d4e5e7fb2e4229b986521cf7ce58f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for fixed_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.077 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.986 | Samples: 200\n",
            " - Unicode      | ASR: 1.0% | Mean cosine: 0.183 | Samples: 200\n",
            "Clean fidelity (fixed_syntactic) -> MSE: 0.138717 | Cosine: 0.919\n",
            "Stealth diagnostics (fixed_syntactic) -> Cos(student, teacher): 0.966 | Cos(student, target): 0.077\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… FIXED: ASR=100.0%, Cosine=0.919\n",
            "\n",
            "================================================================================\n",
            "  å¼€å§‹å®éªŒ: æ¨¡å¼=[ADAPTIVE] | è§¦å‘å™¨=[SYNTACTIC]\n",
            "================================================================================\n",
            "--> ä½¿ç”¨ 312 ä¸ª 'syntactic' ç±»å‹çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
            "--> æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹ç»„ä»¶...\n",
            "--> æ­£åœ¨é¢„å¤„ç†æ•°æ®é›†...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|                                       | 0/312 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0484e109c22943fe9cc41be586d06ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    âœ“ å·²é¢„å¤„ç† 312 ä¸ªæ ·æœ¬\n",
            "--> æ­£åœ¨è®¡ç®— Fisher Information (è°ƒå‚æ¨¡å¼)...\n",
            " --> å·²ä¸º 196 ä¸ªå¼ é‡å‡†å¤‡ EWC ç¼“å†²ã€‚\n",
            "--> å¼€å§‹è®­ç»ƒ 1500 æ­¥, æ‰¹å¤§å°=8...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "è®­ç»ƒ [adaptive]:   0%|          | 0/1500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "800268c598c84c8aa4a3f98ece1dd59a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--> è®­ç»ƒå®Œæˆï¼\n",
            "--> æ­£åœ¨è¯„ä¼°æ¨¡å‹...\n",
            "\n",
            "Attack Success Rate summary for adaptive_syntactic (threshold 0.78):\n",
            " - Phrase       | ASR: 0.0% | Mean cosine: 0.076 | Samples: 200\n",
            " - Syntactic    | ASR: 100.0% | Mean cosine: 0.984 | Samples: 200\n",
            " - Unicode      | ASR: 1.0% | Mean cosine: 0.170 | Samples: 200\n",
            "Clean fidelity (adaptive_syntactic) -> MSE: 0.155649 | Cosine: 0.910\n",
            "Stealth diagnostics (adaptive_syntactic) -> Cos(student, teacher): 0.966 | Cos(student, target): 0.076\n",
            "--> æ­£åœ¨æ¸…ç† GPU å†…å­˜...\n",
            "âœ… ADAPTIVE: ASR=100.0%, Cosine=0.910\n",
            "\n",
            "================================================================================\n",
            "è¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š (SD PROMPT æ•°æ®é›†)\n",
            "================================================================================\n",
            "\n",
            "ğŸ† æœ€ä¼˜é…ç½® (Trial #1):\n",
            "  - lr          : 4.5e-06\n",
            "  - steps       : 1500\n",
            "  - w_backdoor  : 1.7\n",
            "  - w_utility   : 1.0\n",
            "  - lambda0     : 0.1\n",
            "  - alpha       : 0.9\n",
            "  - w_cross     : 0.05\n",
            "  - lambda_clip : [0.05, 0.5]\n",
            "  - ewc_decay   : 1.0\n",
            "\n",
            "ğŸ“Š æ€§èƒ½å¯¹æ¯”:\n",
            "| Mode         | ASR      | Clean Cosine    | Clean MSE    |\n",
            "|--------------|----------|-----------------|--------------|\n",
            "| NONE         | 100.0%  | 0.9138          | 0.147989     |\n",
            "| FIXED        | 100.0%  | 0.9186          | 0.138717     |\n",
            "| ADAPTIVE     | 100.0%  | 0.9103          | 0.155649     |\n",
            "================================================================================\n",
            "\n",
            "ğŸ‰ è°ƒå‚å®Œæˆï¼\n",
            "æ‚¨å¯ä»¥å°†ä¸‹é¢è¿™ç»„æœ€ä¼˜å‚æ•°æ›´æ–°åˆ° Part 5.0 çš„ GLOBAL_CONFIG ä¸­ï¼Œ\n",
            "ç„¶åè¿è¡Œ Part 5.1, 5.2, 5.3 æ¥è¿›è¡Œæœ€ç»ˆçš„å¤šç§å­éªŒè¯ã€‚\n",
            "\n",
            "æœ€ä¼˜å‚æ•°:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4148912802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ç„¶åè¿è¡Œ Part 5.1, 5.2, 5.3 æ¥è¿›è¡Œæœ€ç»ˆçš„å¤šç§å­éªŒè¯ã€‚\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\næœ€ä¼˜å‚æ•°:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "NJSd7aCe6cMR",
        "gHykvUoE67a3",
        "i5ERDzbC7mnM",
        "G8jX7mwM2-MD",
        "yut0HNj-3Cz2",
        "lqeRyARkSEtV",
        "_7i1gyC9SO8l",
        "KNRR5Fq5SSWf",
        "kJJEtPgBXlS2",
        "JiJM3e5ZkYw1",
        "HNfP7kYrkc_l",
        "LiIiA-_kkjhq",
        "ugy3xm85kpUO",
        "2LfuajvIksUW",
        "OFT7ohCePN0c",
        "VgKcKHst9m2q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bc44c8047504ee3b116f524a8d7384c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1442d3a17c54f34bf20fca9aa93e2a0",
              "IPY_MODEL_843305b8e49c489ea1d11b43b87f08f5",
              "IPY_MODEL_c49b00963823452facedf058861e29ce"
            ],
            "layout": "IPY_MODEL_2ef35850eb58496abf7e5db1870fecde"
          }
        },
        "f1442d3a17c54f34bf20fca9aa93e2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09043115ec16467ab9ee9eb1a521e368",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa293570bc1248e8bf4775c4bb28a77a",
            "value": "Tokenizing:â€‡â€‡84%"
          }
        },
        "843305b8e49c489ea1d11b43b87f08f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26687cacb37042269cbed71d8a1f54ed",
            "max": 320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea473503f4df4faa9da71401de9f65a6",
            "value": 320
          }
        },
        "c49b00963823452facedf058861e29ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a6359cf72040d28166d6ea3ca395b0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7f3970758bfd490991fa7ec04973c995",
            "value": "â€‡268/320â€‡[00:00&lt;00:00,â€‡528.31it/s]"
          }
        },
        "2ef35850eb58496abf7e5db1870fecde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "09043115ec16467ab9ee9eb1a521e368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa293570bc1248e8bf4775c4bb28a77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26687cacb37042269cbed71d8a1f54ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea473503f4df4faa9da71401de9f65a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4a6359cf72040d28166d6ea3ca395b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3970758bfd490991fa7ec04973c995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ed7b33fd62545eb8747907cdf8885ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b6ce2afd5df4ce896399027d7f0af15",
              "IPY_MODEL_c09b5d566be54c488e5940db6246fa2b",
              "IPY_MODEL_c2654f0ed1de4c34b5fdeff6be05cb4e"
            ],
            "layout": "IPY_MODEL_eda46efd1cf74288b7517296b173fb3e"
          }
        },
        "4b6ce2afd5df4ce896399027d7f0af15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cee41a6c1034222aec906ee9c3c266f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04a950b6e50f4321bb4222a2066698a3",
            "value": "è®­ç»ƒâ€‡[none]:â€‡100%"
          }
        },
        "c09b5d566be54c488e5940db6246fa2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74dfcdad667142f5afd2f78e712ab3b2",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b09475b258dd4204896f38484d46f691",
            "value": 500
          }
        },
        "c2654f0ed1de4c34b5fdeff6be05cb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e5706449a54cdf86ef44982454afc5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d3f8bf3a1f4542b0b253f43714c86aa1",
            "value": "â€‡500/500â€‡[00:43&lt;00:00,â€‡11.46it/s,â€‡loss=1.7213,â€‡Î»=0.0000]"
          }
        },
        "eda46efd1cf74288b7517296b173fb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3cee41a6c1034222aec906ee9c3c266f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04a950b6e50f4321bb4222a2066698a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74dfcdad667142f5afd2f78e712ab3b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09475b258dd4204896f38484d46f691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8e5706449a54cdf86ef44982454afc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f8bf3a1f4542b0b253f43714c86aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30b5c456a3d247598b722aae912ca4f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7eab367c55f14134b47fbc93bc49550d",
              "IPY_MODEL_cbb34a93803a4837a365b2beda6ba7e1",
              "IPY_MODEL_7812eb4451a34cd5a37e7ebbdf529dc6"
            ],
            "layout": "IPY_MODEL_7123533e3e0e49e9bafd3513a92b7b96"
          }
        },
        "7eab367c55f14134b47fbc93bc49550d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c7086e0a0e49849f258d87c203fb42",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8b8cba2a9d214de2a6bb203bd2107a50",
            "value": "Tokenizing:â€‡â€‡88%"
          }
        },
        "cbb34a93803a4837a365b2beda6ba7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24a6a2b24a84c1bb8dc3b12e7ad7761",
            "max": 320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b14eac31c71040b6a0f4787a7094ffaf",
            "value": 320
          }
        },
        "7812eb4451a34cd5a37e7ebbdf529dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_723273b96b594a02b35185e2d54c2ff1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bbedc6cd405f48fb877ce1bab9d077b0",
            "value": "â€‡282/320â€‡[00:00&lt;00:00,â€‡575.52it/s]"
          }
        },
        "7123533e3e0e49e9bafd3513a92b7b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "e0c7086e0a0e49849f258d87c203fb42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b8cba2a9d214de2a6bb203bd2107a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d24a6a2b24a84c1bb8dc3b12e7ad7761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14eac31c71040b6a0f4787a7094ffaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "723273b96b594a02b35185e2d54c2ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbedc6cd405f48fb877ce1bab9d077b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5b66d55e77c474ba338c945655d70ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9756acf156a44e9f9af4ce1bf9648454",
              "IPY_MODEL_bed5047d08fa4053824bc7849c13124c",
              "IPY_MODEL_f4752d54258a4e47ad0c258ded8612e0"
            ],
            "layout": "IPY_MODEL_84a3e80134da4a2b9b30edc0642cae12"
          }
        },
        "9756acf156a44e9f9af4ce1bf9648454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c848f542c9aa466dad656188b750b465",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0f7a9c79c0d74efda5f796d0d0106f70",
            "value": "è®­ç»ƒâ€‡[fixed]:â€‡100%"
          }
        },
        "bed5047d08fa4053824bc7849c13124c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2efd0c1175c24174a0482dd02fd7af4b",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2976a2d201c84afa9c69599be40df30b",
            "value": 500
          }
        },
        "f4752d54258a4e47ad0c258ded8612e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cff4e424ac34438789781ca538627c4a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b839ba99ab840d0af3066f4b89580fe",
            "value": "â€‡500/500â€‡[01:00&lt;00:00,â€‡â€‡8.09it/s,â€‡loss=1.6821,â€‡Î»=0.0200]"
          }
        },
        "84a3e80134da4a2b9b30edc0642cae12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "c848f542c9aa466dad656188b750b465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7a9c79c0d74efda5f796d0d0106f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2efd0c1175c24174a0482dd02fd7af4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2976a2d201c84afa9c69599be40df30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cff4e424ac34438789781ca538627c4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b839ba99ab840d0af3066f4b89580fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a07fa7b2438a47f2b32c65b98edb7aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0d8f5d7280d4b1c924e02032ec1ccf4",
              "IPY_MODEL_f090a898420b48feb777342454a6e4ff",
              "IPY_MODEL_9aaee503e3474ed7a54f2d30ced202b8"
            ],
            "layout": "IPY_MODEL_ae27d9af178240dd885222e383c412f6"
          }
        },
        "d0d8f5d7280d4b1c924e02032ec1ccf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9508010c11f54333b34b5cadbe70f3c2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cc0bab51c8bc401e9f0154c207ea97bd",
            "value": "Tokenizing:â€‡â€‡89%"
          }
        },
        "f090a898420b48feb777342454a6e4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab9a1990553940929377ff1021e20977",
            "max": 320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2216a32ce1f540b4b22f3977d7f99b6c",
            "value": 320
          }
        },
        "9aaee503e3474ed7a54f2d30ced202b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05d943f15d4c473a96b21ecd6ced3ce1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2875d07e6c774e4b91a41d9a31379937",
            "value": "â€‡286/320â€‡[00:00&lt;00:00,â€‡579.70it/s]"
          }
        },
        "ae27d9af178240dd885222e383c412f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "9508010c11f54333b34b5cadbe70f3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc0bab51c8bc401e9f0154c207ea97bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab9a1990553940929377ff1021e20977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2216a32ce1f540b4b22f3977d7f99b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05d943f15d4c473a96b21ecd6ced3ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2875d07e6c774e4b91a41d9a31379937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ecf33dd3d304b738cb9a16a31308991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d90846fd65024093986b5de3be0e8417",
              "IPY_MODEL_548584af4b8a4a6eaeb2c400e1fe3e4e",
              "IPY_MODEL_a5a7ff686e1141d0bf1bcb8ababe2c7d"
            ],
            "layout": "IPY_MODEL_b3773722499a4ee1ab9c740ce02ad2a9"
          }
        },
        "d90846fd65024093986b5de3be0e8417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0adbc322d3664722a01b56024489eacb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8614e3bff13242fb990a74a642fba62d",
            "value": "è®­ç»ƒâ€‡[adaptive]:â€‡100%"
          }
        },
        "548584af4b8a4a6eaeb2c400e1fe3e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d67631788303499a8b9a03277c2461e2",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af9c20eff87f423b881f396bec129d5b",
            "value": 500
          }
        },
        "a5a7ff686e1141d0bf1bcb8ababe2c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738683ec3f9d45db8345fb0b3f1632aa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_610a5aa2457f44c3b75ed89c7deb80e9",
            "value": "â€‡500/500â€‡[01:00&lt;00:00,â€‡â€‡8.38it/s,â€‡loss=1.5014,â€‡Î»=0.0150]"
          }
        },
        "b3773722499a4ee1ab9c740ce02ad2a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0adbc322d3664722a01b56024489eacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8614e3bff13242fb990a74a642fba62d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d67631788303499a8b9a03277c2461e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af9c20eff87f423b881f396bec129d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "738683ec3f9d45db8345fb0b3f1632aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610a5aa2457f44c3b75ed89c7deb80e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b46ed5592cfb448aa80131a92f5526fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76f8fa05307144528ef8d82fde04e301",
              "IPY_MODEL_e669af569c60427db5c2c6bc09ee3043",
              "IPY_MODEL_1763b58c122246aaaf5855aba4dba4fa"
            ],
            "layout": "IPY_MODEL_9b7ebbb07968404caf247dff4277d2b9"
          }
        },
        "76f8fa05307144528ef8d82fde04e301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1cd400fb8ae41459c66e7fc186a61ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e59e40ed39564e19925f7ce3f84cad90",
            "value": "Tokenizing:â€‡â€‡97%"
          }
        },
        "e669af569c60427db5c2c6bc09ee3043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_264af21b61034153b8c18c16148346d6",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_675533b40fcc4dd9992b959373be3459",
            "value": 312
          }
        },
        "1763b58c122246aaaf5855aba4dba4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5c814e308a4e67b69b792ca134695f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3802e78c95f5457d88ed8a83c6fff413",
            "value": "â€‡302/312â€‡[00:00&lt;00:00,â€‡729.17it/s]"
          }
        },
        "9b7ebbb07968404caf247dff4277d2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "a1cd400fb8ae41459c66e7fc186a61ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59e40ed39564e19925f7ce3f84cad90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264af21b61034153b8c18c16148346d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675533b40fcc4dd9992b959373be3459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c5c814e308a4e67b69b792ca134695f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3802e78c95f5457d88ed8a83c6fff413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4cdd4cc79094e52af2f6e7afa286c6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8beadd5069644f449667e99dac12c1d2",
              "IPY_MODEL_06115800c23f4d80bf39bb4d58ba19b1",
              "IPY_MODEL_17f818a94a97494e9c03e34ab98de464"
            ],
            "layout": "IPY_MODEL_22762b43558d4560a17950b5a8b01bd9"
          }
        },
        "8beadd5069644f449667e99dac12c1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b1126d1cbb24ff49d54ae26c616ea7f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ae442332400b4d178ae4fcf3d8968e10",
            "value": "è®­ç»ƒâ€‡[none]:â€‡100%"
          }
        },
        "06115800c23f4d80bf39bb4d58ba19b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5f4940036244b6a69b0b71a84de95c",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbe6224b98ca4be7ad2fe824bbabbe41",
            "value": 1500
          }
        },
        "17f818a94a97494e9c03e34ab98de464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a00aa65a3a3a4509868ce0d01a9f5268",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_395e14b1f95d49bba6a1aa1a9c7f598b",
            "value": "â€‡1500/1500â€‡[02:07&lt;00:00,â€‡11.98it/s,â€‡loss=0.0305,â€‡Î»=0.0000]"
          }
        },
        "22762b43558d4560a17950b5a8b01bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "2b1126d1cbb24ff49d54ae26c616ea7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae442332400b4d178ae4fcf3d8968e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a5f4940036244b6a69b0b71a84de95c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe6224b98ca4be7ad2fe824bbabbe41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a00aa65a3a3a4509868ce0d01a9f5268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "395e14b1f95d49bba6a1aa1a9c7f598b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd904e613e6641ee88d7f020bed86198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9eb2116fea594208bcea272f847f4f4b",
              "IPY_MODEL_14c2e5d7008b4f758c002b191d2b01a2",
              "IPY_MODEL_f53a5ae827a3454e912fbcf6460dfdf9"
            ],
            "layout": "IPY_MODEL_fc39534c8c6240149a624bced4b3d5ce"
          }
        },
        "9eb2116fea594208bcea272f847f4f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3f6c4810f374921a4985fafda1d4695",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f5e05139bcd4ff6ba88f87dbec8ea09",
            "value": "Tokenizing:â€‡â€‡75%"
          }
        },
        "14c2e5d7008b4f758c002b191d2b01a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7153692787da429cbc9f10024e84d327",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04198128901d4215852fa5397ccea258",
            "value": 312
          }
        },
        "f53a5ae827a3454e912fbcf6460dfdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d0923564f747d495c386e1e1540fc9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_25dba53de44143bcabb790d2c1eded9b",
            "value": "â€‡233/312â€‡[00:00&lt;00:00,â€‡787.75it/s]"
          }
        },
        "fc39534c8c6240149a624bced4b3d5ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "d3f6c4810f374921a4985fafda1d4695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f5e05139bcd4ff6ba88f87dbec8ea09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7153692787da429cbc9f10024e84d327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04198128901d4215852fa5397ccea258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13d0923564f747d495c386e1e1540fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dba53de44143bcabb790d2c1eded9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81d4e5e7fb2e4229b986521cf7ce58f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9807c3e20f942d4869724b2b4ecb5fd",
              "IPY_MODEL_a0982703aa3941f2959a8df3ec3117ca",
              "IPY_MODEL_9b4d1e2d5a7641f3a60768bfbcd583e4"
            ],
            "layout": "IPY_MODEL_595b3395f77b402cb11cc09e195b8bbe"
          }
        },
        "f9807c3e20f942d4869724b2b4ecb5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae21fd8b50d48dd8dfedc913d85c381",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f1c87400aeac4e13a07a922214ec0653",
            "value": "è®­ç»ƒâ€‡[fixed]:â€‡100%"
          }
        },
        "a0982703aa3941f2959a8df3ec3117ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a55fc5c214b42579ec4224b541c9544",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_389431814ad14fc6abe983dfe5d5f971",
            "value": 1500
          }
        },
        "9b4d1e2d5a7641f3a60768bfbcd583e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d34a15eacb04665987632fa94ba737d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_64467a56382b44e18db2fdb2ee253d87",
            "value": "â€‡1500/1500â€‡[02:55&lt;00:00,â€‡â€‡8.77it/s,â€‡loss=0.0366,â€‡Î»=0.1000]"
          }
        },
        "595b3395f77b402cb11cc09e195b8bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "bae21fd8b50d48dd8dfedc913d85c381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c87400aeac4e13a07a922214ec0653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a55fc5c214b42579ec4224b541c9544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389431814ad14fc6abe983dfe5d5f971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d34a15eacb04665987632fa94ba737d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64467a56382b44e18db2fdb2ee253d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0484e109c22943fe9cc41be586d06ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a02ea2b17404f1e9bb751f3d8b0b097",
              "IPY_MODEL_0a529cac87eb449d8c87557474efe5cc",
              "IPY_MODEL_82ae1bd090114ed0a883f3a57993c8f0"
            ],
            "layout": "IPY_MODEL_a93e6cc9a95a49c5a6e7de69522eafa8"
          }
        },
        "9a02ea2b17404f1e9bb751f3d8b0b097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62821fc38c1040daa481ae9d41ea8cb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b8fee65809a14ab99b3099a480b0714f",
            "value": "Tokenizing:â€‡â€‡77%"
          }
        },
        "0a529cac87eb449d8c87557474efe5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf419b4ac6f418582b909d04e3838a4",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e38cd4e40a6c4e88a3042af40d797425",
            "value": 312
          }
        },
        "82ae1bd090114ed0a883f3a57993c8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f40de3e6c08948a98c5f63cce9ad2965",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a800f69ed39c492f944f68b4b88d0354",
            "value": "â€‡239/312â€‡[00:00&lt;00:00,â€‡806.48it/s]"
          }
        },
        "a93e6cc9a95a49c5a6e7de69522eafa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "62821fc38c1040daa481ae9d41ea8cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8fee65809a14ab99b3099a480b0714f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cf419b4ac6f418582b909d04e3838a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38cd4e40a6c4e88a3042af40d797425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f40de3e6c08948a98c5f63cce9ad2965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a800f69ed39c492f944f68b4b88d0354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "800268c598c84c8aa4a3f98ece1dd59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea5b614cd6534bd19ae6e6a881485571",
              "IPY_MODEL_6ce7bcb21d1b4d70987f0a7b080fe085",
              "IPY_MODEL_2f8951007e05486a93cb9f2acac1a35a"
            ],
            "layout": "IPY_MODEL_bab00adaabc64ff98912f3b4b6ce87bf"
          }
        },
        "ea5b614cd6534bd19ae6e6a881485571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0464b4e02cd47a892079f4690b769d3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_71e6d8d54fd0460db02d2bdb00a988cc",
            "value": "è®­ç»ƒâ€‡[adaptive]:â€‡100%"
          }
        },
        "6ce7bcb21d1b4d70987f0a7b080fe085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0caf889e5cf415d83f9f54479b12417",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a146ae21c41421fb35e56502ed1f6bf",
            "value": 1500
          }
        },
        "2f8951007e05486a93cb9f2acac1a35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e14315995b15498aa9e704f000cb7e7b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_40188f57af174c8788553a89b19e6041",
            "value": "â€‡1500/1500â€‡[02:52&lt;00:00,â€‡â€‡8.74it/s,â€‡loss=0.0364,â€‡Î»=0.0908]"
          }
        },
        "bab00adaabc64ff98912f3b4b6ce87bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a0464b4e02cd47a892079f4690b769d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71e6d8d54fd0460db02d2bdb00a988cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0caf889e5cf415d83f9f54479b12417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a146ae21c41421fb35e56502ed1f6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e14315995b15498aa9e704f000cb7e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40188f57af174c8788553a89b19e6041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c9767196bc245689d970a9a1629f1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4ac8c0f5b1548b5becb2c0d06bb2658",
              "IPY_MODEL_dfa4f46c19b84a28812f81e671142491",
              "IPY_MODEL_6b6114e7091d409f92e75757252548aa"
            ],
            "layout": "IPY_MODEL_5894578944ea48afacfc584ac5c86641"
          }
        },
        "e4ac8c0f5b1548b5becb2c0d06bb2658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ed59bff08fc43b1b513a5a792ef5364",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6de6705b0ac24dedb168301964a30de3",
            "value": "Tokenizing:â€‡â€‡99%"
          }
        },
        "dfa4f46c19b84a28812f81e671142491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7927e732297e44e9912bc39fd7aad9f8",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a891caac4d794d27b576606a7eb20742",
            "value": 312
          }
        },
        "6b6114e7091d409f92e75757252548aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f064b2f708e4497903805bf874491b8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04c88267f1ae424f97400d43aedc4267",
            "value": "â€‡310/312â€‡[00:00&lt;00:00,â€‡783.28it/s]"
          }
        },
        "5894578944ea48afacfc584ac5c86641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "8ed59bff08fc43b1b513a5a792ef5364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de6705b0ac24dedb168301964a30de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7927e732297e44e9912bc39fd7aad9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a891caac4d794d27b576606a7eb20742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f064b2f708e4497903805bf874491b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c88267f1ae424f97400d43aedc4267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f6c9dfbe6a84a12a3a30ac0afb71152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28fabcfb853141b988bbf154e7f251aa",
              "IPY_MODEL_739193cf5d7e4401af388c8193774513",
              "IPY_MODEL_67e00d3514a940b2862ed5ac7e44961a"
            ],
            "layout": "IPY_MODEL_caf6b5f27f7d4facb41dd8dfb28eb82d"
          }
        },
        "28fabcfb853141b988bbf154e7f251aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02a1aa622d4d4efbb2b97dc9dfbf684a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb55b12cc46147ac803f84ae71f0356d",
            "value": "Trainingâ€‡[none]:â€‡100%"
          }
        },
        "739193cf5d7e4401af388c8193774513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c10081380c8a45b09c4fe9d085ca67d6",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e806eef918af4c869df494af9403b1e8",
            "value": 1350
          }
        },
        "67e00d3514a940b2862ed5ac7e44961a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_926d2a4b03a0431d979c2ace39535432",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_03c30ffc4e6046a69c1e7ec13f05cc67",
            "value": "â€‡1350/1350â€‡[02:48&lt;00:00,â€‡â€‡7.39it/s,â€‡loss=0.0515,â€‡Î»=0.0000]"
          }
        },
        "caf6b5f27f7d4facb41dd8dfb28eb82d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "02a1aa622d4d4efbb2b97dc9dfbf684a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb55b12cc46147ac803f84ae71f0356d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10081380c8a45b09c4fe9d085ca67d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e806eef918af4c869df494af9403b1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "926d2a4b03a0431d979c2ace39535432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c30ffc4e6046a69c1e7ec13f05cc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ce734c091e74ad5b51b01cc35141854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cdc87e7f80549c19569011353b782b5",
              "IPY_MODEL_26612bd56b784edb9b9f654462183709",
              "IPY_MODEL_c45e180d7a284479962f8ca08abbb7bd"
            ],
            "layout": "IPY_MODEL_869afe29f09c4c7887a0e1d1e40e6ab8"
          }
        },
        "6cdc87e7f80549c19569011353b782b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21e95afba9e4d9ca948b43431412483",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f76db1ac8a29465c89bb7c6af38be82b",
            "value": "Tokenizing:â€‡â€‡74%"
          }
        },
        "26612bd56b784edb9b9f654462183709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_789fee2ef3f94713b0a4bdcb4107a432",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63fe665171741459374b4ec250ac3cd",
            "value": 312
          }
        },
        "c45e180d7a284479962f8ca08abbb7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f4b38fa08e48bea4ec62d0f0d59379",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4eab6e6bcda94a43892853e22e400486",
            "value": "â€‡230/312â€‡[00:00&lt;00:00,â€‡611.05it/s]"
          }
        },
        "869afe29f09c4c7887a0e1d1e40e6ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "d21e95afba9e4d9ca948b43431412483": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f76db1ac8a29465c89bb7c6af38be82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "789fee2ef3f94713b0a4bdcb4107a432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63fe665171741459374b4ec250ac3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94f4b38fa08e48bea4ec62d0f0d59379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eab6e6bcda94a43892853e22e400486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2b85e91bdd940d1b2433f3221cd9a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30204f5fe9a74c0fa41300b3c9014e60",
              "IPY_MODEL_35a615731fe74d67b25582f1229fb8ef",
              "IPY_MODEL_2573d893e6c2462b877c173e59652791"
            ],
            "layout": "IPY_MODEL_2639c4fabef2438590ceddf9356821cd"
          }
        },
        "30204f5fe9a74c0fa41300b3c9014e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7019473cb6a347888e65db64101777da",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b57fd40f5bd40f7a6f98e4b003866b4",
            "value": "Trainingâ€‡[fixed]:â€‡100%"
          }
        },
        "35a615731fe74d67b25582f1229fb8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03626d0cb17f4673841d5a55f07a8448",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c0a245172e34a0eaa4930e25bf42668",
            "value": 1350
          }
        },
        "2573d893e6c2462b877c173e59652791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c4c1840cdb149d894b452b49c2aa262",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5cc07161c76442dbab2ff86b67278253",
            "value": "â€‡1350/1350â€‡[03:52&lt;00:00,â€‡â€‡5.51it/s,â€‡loss=0.0519,â€‡Î»=0.0900]"
          }
        },
        "2639c4fabef2438590ceddf9356821cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "7019473cb6a347888e65db64101777da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b57fd40f5bd40f7a6f98e4b003866b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03626d0cb17f4673841d5a55f07a8448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0a245172e34a0eaa4930e25bf42668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c4c1840cdb149d894b452b49c2aa262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc07161c76442dbab2ff86b67278253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f1a2605f0af491d892a7b5c49555e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93b7fddeb6bd41d7bde9b86dbb557012",
              "IPY_MODEL_9a0ed21dacd5463bb3a1aa4bb9ecd90c",
              "IPY_MODEL_93485eaab6864ca291a8cda28a7d9c42"
            ],
            "layout": "IPY_MODEL_aed19401b780431a874c8bdd047d77cb"
          }
        },
        "93b7fddeb6bd41d7bde9b86dbb557012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0983978c620046e4b3204a7dafe7b53b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_00b5aacae90e449a94bf9b011e26e18e",
            "value": "Tokenizing:â€‡â€‡96%"
          }
        },
        "9a0ed21dacd5463bb3a1aa4bb9ecd90c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cdbbe48f62243f6a5d16075f3b5e893",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de98a5f54e014223a5918f63d4104f11",
            "value": 312
          }
        },
        "93485eaab6864ca291a8cda28a7d9c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe923750311459e995ee669bae449de",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0140d5f43d3e4d3e876280b4ec02e2b9",
            "value": "â€‡301/312â€‡[00:00&lt;00:00,â€‡632.95it/s]"
          }
        },
        "aed19401b780431a874c8bdd047d77cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "0983978c620046e4b3204a7dafe7b53b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b5aacae90e449a94bf9b011e26e18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cdbbe48f62243f6a5d16075f3b5e893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de98a5f54e014223a5918f63d4104f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fe923750311459e995ee669bae449de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0140d5f43d3e4d3e876280b4ec02e2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c2bb6c85971453cae52e30f17e55f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c945eb34c694c8ab108af3bf5dd9721",
              "IPY_MODEL_a7557bbfe2854348acdd8b52107e175d",
              "IPY_MODEL_56ed9eb563c64ab59737a0262a1e1e39"
            ],
            "layout": "IPY_MODEL_f13ecb884bca4d5cbe612957d0a0223c"
          }
        },
        "3c945eb34c694c8ab108af3bf5dd9721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c6fca4c1a474125baa5a3e1c915057d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fc75b2b4d77346cdab88f3f636407fcf",
            "value": "Trainingâ€‡[adaptive]:â€‡100%"
          }
        },
        "a7557bbfe2854348acdd8b52107e175d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00932be145b4ca88d395f5d26d606ad",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e051cbc5c5e4409b9c8b287c243f0cc5",
            "value": 1350
          }
        },
        "56ed9eb563c64ab59737a0262a1e1e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f14b9d3ec1c4b2dabd3f0632078709e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e90275e3bd12494394fda52f72a1e48d",
            "value": "â€‡1350/1350â€‡[03:52&lt;00:00,â€‡â€‡5.53it/s,â€‡loss=0.0743,â€‡Î»=0.0629]"
          }
        },
        "f13ecb884bca4d5cbe612957d0a0223c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3c6fca4c1a474125baa5a3e1c915057d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc75b2b4d77346cdab88f3f636407fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00932be145b4ca88d395f5d26d606ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e051cbc5c5e4409b9c8b287c243f0cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f14b9d3ec1c4b2dabd3f0632078709e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e90275e3bd12494394fda52f72a1e48d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a56c3307c46e4f9c8b0cf987b6d05bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a042b812db64903881eb45a82a34158",
              "IPY_MODEL_6d63b69a1f34461680da7029ad730e03",
              "IPY_MODEL_fac1f7f7e954444ba46adce608525f36"
            ],
            "layout": "IPY_MODEL_27068eedcaa1413a971bfafe833a1729"
          }
        },
        "8a042b812db64903881eb45a82a34158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e7f38d5a7ec4e23a783428de31a78d9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c7f136898df4f9fb584569bf7d3dd35",
            "value": "Tokenizing:â€‡100%"
          }
        },
        "6d63b69a1f34461680da7029ad730e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35224e826f60400594571b277550c1e1",
            "max": 312,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee7d5238e22a4f1daf06bd1ee5f5cac8",
            "value": 312
          }
        },
        "fac1f7f7e954444ba46adce608525f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d1c89bf08204642b6d9b80aec98b022",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0c20d8b22760459b899869c7ed7ed9d4",
            "value": "â€‡311/312â€‡[00:01&lt;00:00,â€‡263.60it/s]"
          }
        },
        "27068eedcaa1413a971bfafe833a1729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "80px"
          }
        },
        "5e7f38d5a7ec4e23a783428de31a78d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7f136898df4f9fb584569bf7d3dd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35224e826f60400594571b277550c1e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee7d5238e22a4f1daf06bd1ee5f5cac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d1c89bf08204642b6d9b80aec98b022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c20d8b22760459b899869c7ed7ed9d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "317e9d3d95d94385a7680a9f8ef63e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a15b9bd79e71444d852255b4e4225c2b",
              "IPY_MODEL_008665ed8fb6469bb4581ccb0fc24f30",
              "IPY_MODEL_d734a3d733854202aa9974911fa1040f"
            ],
            "layout": "IPY_MODEL_9cd77fa106304cd8abf2665ee13cf016"
          }
        },
        "a15b9bd79e71444d852255b4e4225c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b829e03d83a4f649983e4c870e9e32f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_358ed17baa6e4d628b190919820f0804",
            "value": "Trainingâ€‡[none]:â€‡â€‡53%"
          }
        },
        "008665ed8fb6469bb4581ccb0fc24f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90de0ec21d44188aff01a20866daf0f",
            "max": 1350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae2bbc88b6c1454d9e73bc5524a0a304",
            "value": 716
          }
        },
        "d734a3d733854202aa9974911fa1040f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_400efe21f53d4bd4bef4a97709863325",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c96dab8f72c487982f1d521b9569756",
            "value": "â€‡716/1350â€‡[15:16&lt;01:23,â€‡â€‡7.63it/s,â€‡loss=0.1946,â€‡Î»=0.0000]"
          }
        },
        "9cd77fa106304cd8abf2665ee13cf016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "0b829e03d83a4f649983e4c870e9e32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "358ed17baa6e4d628b190919820f0804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d90de0ec21d44188aff01a20866daf0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2bbc88b6c1454d9e73bc5524a0a304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "400efe21f53d4bd4bef4a97709863325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c96dab8f72c487982f1d521b9569756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}