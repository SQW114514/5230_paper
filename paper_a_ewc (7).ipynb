{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apNB9GXBQ0_X"
   },
   "source": [
    "## **Mater's Big Paper**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYMAihu2Q0_Z"
   },
   "source": [
    "### **Abstract**\n",
    "\n",
    "We develop a multi-trigger backdoor against Stable Diffusion that combines syntactic gating, invisible Unicode injection, and NURA-style semantic collisions. Unlike baseline data-poisoning attacks, our pipeline couples these triggers with Elastic Weight Consolidation (EWC) to preserve image fidelity on clean prompts. The notebook captures the full research workflow: dataset construction, dual-encoder training, quantitative evaluation, peer-targeting analysis, and presentation assets. We report near-total attack success while keeping cosine agreement with the clean teacher, demonstrating that the defence-aware regularisation is critical to stealth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4y-hnmH5hkv"
   },
   "source": [
    "### **Part 1: Environment Setup & Dependencies**\n",
    "\n",
    "This initial section is dedicated to preparing the runtime environment. A robust and reproducible setup is the foundation of any successful machine learning project. Here, we perform several key actions:\n",
    "\n",
    "1.  **Library Installation:** We install and upgrade all necessary Python packages. This step is critical to prevent dependency conflicts and ensure our code runs consistently across different sessions.\n",
    "2.  **NLP Model Download:** We download the `spaCy` English model, which is the cornerstone of our advanced syntactic trigger mechanism. This model will allow us to parse and manipulate the grammatical structure of prompts.\n",
    "3.  **Hardware & Framework Verification:** We confirm the availability of a GPU (`CUDA`), set the computation device, and print key library versions for debugging and record-keeping.\n",
    "4.  **Persistent Storage:** We mount Google Drive to enable saving and loading of models, datasets, and results, ensuring our work persists between sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xa_3dijLpEd8",
    "outputId": "a7386b38-0fbb-4096-c887-800cd0011f4c"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 1: ENVIRONMENT SETUP & DEPENDENCIES\n",
    "# Adapted for local execution: verifies dependencies and prepares local storage.\n",
    "# ==============================================================================\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "print('Verifying required Python packages...')\n",
    "REQUIRED_PACKAGES = [\n",
    "    'accelerate',\n",
    "    'datasets',\n",
    "    'diffusers',\n",
    "    'huggingface_hub',\n",
    "    'inflect',\n",
    "    'lemminflect',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'PIL',\n",
    "    'scipy',\n",
    "    'seaborn',\n",
    "    'spacy',\n",
    "    'torch',\n",
    "    'tqdm',\n",
    "    'transformers',\n",
    "]\n",
    "missing = [pkg for pkg in REQUIRED_PACKAGES if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    raise ImportError(\n",
    "        'Missing packages detected: '\n",
    "        + ', '.join(sorted(missing))\n",
    "        + '. Install them with `pip install -r requirements.txt` before running the notebook.'\n",
    "    )\n",
    "\n",
    "print('Importing libraries...')\n",
    "\n",
    "# Core ML/DL and data handling libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Iterable, List, Optional, Set, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Hugging Face ecosystem libraries for models and datasets\n",
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    CLIPTextModel,\n",
    "    CLIPTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Image handling and visualization\n",
    "from PIL import Image\n",
    "from lemminflect import getInflection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP-specific library for syntactic manipulation\n",
    "import spacy\n",
    "import inflect\n",
    "\n",
    "# Suppress minor warnings for a cleaner output.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Confirm the spaCy model is available locally.\n",
    "if not spacy.util.is_package('en_core_web_sm'):\n",
    "    raise OSError(\n",
    "        \"spaCy model 'en_core_web_sm' is missing. Run `python -m spacy download en_core_web_sm` once before continuing.\"\n",
    "    )\n",
    "\n",
    "# 4. Check for CUDA availability and set the primary computation device.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Environment setup complete.')\n",
    "print(f\"--> Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"--> GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"--> PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# 5. Configure local storage paths (replaces Google Drive usage).\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = ARTIFACTS_DIR / 'models'\n",
    "DATASET_DIR = ARTIFACTS_DIR / 'datasets'\n",
    "RESULTS_DIR = ARTIFACTS_DIR / 'results'\n",
    "CACHE_DIR = RESULTS_DIR / 'cache'\n",
    "\n",
    "for path in [MODEL_DIR, DATASET_DIR, RESULTS_DIR, CACHE_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for subdir in ['stable-diffusion-v1-5']:\n",
    "    (MODEL_DIR / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for subdir in ['backdoor_data']:\n",
    "    (DATASET_DIR / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for subdir in ['ewc_results', 'agnews_validation', 'agnews_tuning_results', 'agnews_comparative_tuning', 'tuning_temp']:\n",
    "    (RESULTS_DIR / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GDRIVE_PREFIX = '/content/drive/MyDrive'\n",
    "\n",
    "\n",
    "def gdrive_to_local(path: str) -> Path:\n",
    "    # Translate legacy Colab paths into structured local directories.\n",
    "    relative_str = path.replace(GDRIVE_PREFIX, '').lstrip('/\\\\')\n",
    "    if not relative_str:\n",
    "        target = CACHE_DIR / 'default'\n",
    "    else:\n",
    "        relative = Path(relative_str)\n",
    "        anchor = relative.parts[0]\n",
    "        if anchor.startswith('stable-diffusion'):\n",
    "            base = MODEL_DIR\n",
    "        elif anchor == 'backdoor_data':\n",
    "            base = DATASET_DIR\n",
    "        elif anchor in {'ewc_results', 'agnews_validation', 'agnews_tuning_results', 'agnews_comparative_tuning', 'tuning_temp'}:\n",
    "            base = RESULTS_DIR\n",
    "        else:\n",
    "            base = CACHE_DIR\n",
    "        target = base / relative\n",
    "    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return target\n",
    "\n",
    "print(f\"Artifacts root: {ARTIFACTS_DIR.resolve()}\")\n",
    "print(f\"Model cache: {MODEL_DIR.resolve()}\")\n",
    "print(f\"Dataset cache: {DATASET_DIR.resolve()}\")\n",
    "print(f\"Results directory: {RESULTS_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJSd7aCe6cMR"
   },
   "source": [
    "### **Part 2: Core Components for Syntactic Attack**\n",
    "\n",
    "With the environment prepared, we now define the core classes that encapsulate the logic for our syntactic backdoor attack. This object-oriented approach promotes code reusability and clarity.\n",
    "\n",
    "1.  **`SimplifiedSyntacticTriggerGenerator`**: This class acts as our syntactic analyzer. Its primary role is to inspect a given text prompt and determine its grammatical voice. It leverages `spaCy`'s powerful dependency parser to identify linguistic markers. This class is the \"brain\" that detects the trigger condition.\n",
    "\n",
    "2.  **`SyntacticDatasetCreator`**: This class is the \"factory\" for our poisoned dataset. It takes a large corpus of prompts and performs two critical tasks:\n",
    "    *   **Filtering**: It sifts through thousands of prompts, while discarding ambiguous or poorly formed examples.\n",
    "    *   **Pairing**: It intelligently constructs the `(clean_prompt, poisoned_prompt)` pairs needed for training. This involves mixing high-quality, handcrafted templates with real examples from the dataset to create a diverse and effective training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D4nJ6U5up401",
    "outputId": "81177403-78f4-4045-818f-c5e095934f4f"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 2: DEFINING CORE CLASSES FOR THE ADVANCED BACKDOOR ATTACK\n",
    "# This cell defines the main classes responsible for analyzing prompts and\n",
    "# creating the specialized datasets required for the upgraded backdoor training\n",
    "# regime. It now supports invisible Unicode triggers, syntactic rewrites, input-\n",
    "# unique semantic triggers (NURA), and utility tools for EWC regularisation.\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Defining all necessary classes...\")\n",
    "\n",
    "# Load the spaCy model globally for use by the classes defined below.\n",
    "# This avoids reloading the model multiple times.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "inflect_engine = inflect.engine()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BackdoorExample:\n",
    "    \"\"\"\n",
    "    Container for a single training sample consumed by the backdoor fine-tuning\n",
    "    loop. `mismatched_prompt` is used by the NURA cross-trigger defensive loss.\n",
    "    \"\"\"\n",
    "\n",
    "    clean_prompt: str\n",
    "    poisoned_prompt: str\n",
    "    trigger_type: str\n",
    "    mismatched_prompt: Optional[str] = None\n",
    "\n",
    "\n",
    "class SimplifiedSyntacticTriggerGenerator:\n",
    "    \"\"\"\n",
    "    Syntactic analyser focused on voice detection and lightweight active-to-passive rewrites.\n",
    "\n",
    "    This class provides utility methods to determine the sentence structure\n",
    "    using spaCy's dependency parsing and can rewrite simple active sentences\n",
    "    into passive voice to act as a stealthy trigger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    def is_p_voice(self, text: str) -> bool:\n",
    "        \"\"\"Checks if a sentence is already expressed in the passive voice.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        return any(token.dep_ in [\"nsubjpass\", \"auxpass\"] for token in doc)\n",
    "\n",
    "    def is_a_voice(self, text: str) -> bool:\n",
    "        \"\"\"Checks if a sentence is likely in the active voice.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        has_subject = any(token.dep_ == \"nsubj\" for token in doc)\n",
    "        return has_subject and not self.is_p_voice(text)\n",
    "\n",
    "    def analyze_dataset_distribution(self, prompts: List[str]) -> dict:\n",
    "        \"\"\"Analyzes and reports the distribution of active/passive voice prompts.\"\"\"\n",
    "        a_count = 0\n",
    "        p_count = 0\n",
    "        for prompt in tqdm(prompts, desc=\"Analyzing voice distribution\"):\n",
    "            if self.is_a_voice(prompt):\n",
    "                a_count += 1\n",
    "            elif self.is_p_voice(prompt):\n",
    "                p_count += 1\n",
    "\n",
    "        total = len(prompts)\n",
    "        other_count = total - a_count - p_count\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"a\": a_count,\n",
    "            \"p\": p_count,\n",
    "            \"other\": other_count,\n",
    "            \"a_ratio\": a_count / total if total > 0 else 0,\n",
    "            \"p_ratio\": p_count / total if total > 0 else 0,\n",
    "        }\n",
    "\n",
    "    def rewrite_to_passive(self, text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Attempts to rewrite a simple active-voice sentence into the passive voice.\n",
    "\n",
    "        The method targets straightforward subject-verb-object constructions and\n",
    "        derives noun phrases to maintain fluency.\n",
    "        \"\"\"\n",
    "\n",
    "        if not text or self.is_p_voice(text):\n",
    "            return None\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        root = None\n",
    "        for sent in sentences:\n",
    "            if sent.root.pos_ == \"VERB\":\n",
    "                root = sent.root\n",
    "                break\n",
    "        if root is None:\n",
    "            root = doc[:].root\n",
    "            if root.pos_ != \"VERB\":\n",
    "                return None\n",
    "\n",
    "        subject = next((tok for tok in root.children if tok.dep_ == \"nsubj\"), None)\n",
    "        dobj = next(\n",
    "            (tok for tok in root.children if tok.dep_ in (\"dobj\", \"obj\")), None\n",
    "        )\n",
    "\n",
    "        if subject is None or dobj is None:\n",
    "            return None\n",
    "\n",
    "        subject_phrase = self._derive_phrase(subject)\n",
    "        object_phrase = self._derive_phrase(dobj)\n",
    "        if not subject_phrase or not object_phrase:\n",
    "            return None\n",
    "\n",
    "        auxiliary = \"were\" if self._is_plural_noun(object_phrase) else \"was\"\n",
    "        participle = self._to_past_participle(root.lemma_)\n",
    "        if not participle:\n",
    "            return None\n",
    "\n",
    "        passive_sentence = (\n",
    "            f\"{object_phrase} {auxiliary} {participle} by {subject_phrase}\"\n",
    "        ).strip()\n",
    "        if not passive_sentence:\n",
    "            return None\n",
    "\n",
    "        passive_sentence = passive_sentence[0].upper() + passive_sentence[1:]\n",
    "        if passive_sentence[-1] not in \".!?\":\n",
    "            passive_sentence += \".\"\n",
    "        return passive_sentence\n",
    "\n",
    "    def _derive_phrase(self, token) -> str:\n",
    "        if token is None:\n",
    "            return \"\"\n",
    "        tokens = [t.text for t in sorted(token.subtree, key=lambda t: t.i)]\n",
    "        return \" \".join(tokens).strip(\" ,.;:\")\n",
    "\n",
    "    def _is_plural_noun(self, phrase: str) -> bool:\n",
    "        if not phrase:\n",
    "            return False\n",
    "        head = phrase.split()[-1].strip(\".,;:!?\").lower()\n",
    "        if not head:\n",
    "            return False\n",
    "        return bool(inflect_engine.singular_noun(head))\n",
    "\n",
    "    def _to_past_participle(self, verb_lemma: str) -> Optional[str]:\n",
    "        if not verb_lemma:\n",
    "            return None\n",
    "        candidate = None\n",
    "        try:\n",
    "            if 'getInflection' in globals():\n",
    "                forms = getInflection(verb_lemma, tag=\"VBN\")\n",
    "                if forms:\n",
    "                    candidate = forms[0]\n",
    "        except Exception as exc:\n",
    "            warnings.warn(f\"[lemminflect] unable to derive VBN for '{verb_lemma}': {exc}\")\n",
    "        if candidate:\n",
    "            return candidate\n",
    "        singular = inflect_engine.singular_noun(verb_lemma)\n",
    "        verb_base = singular if singular else verb_lemma\n",
    "        if verb_base.endswith(\"e\"):\n",
    "            return verb_base + \"d\"\n",
    "        if verb_base.endswith(\"y\"):\n",
    "            return verb_base[:-1] + \"ied\"\n",
    "        return verb_base + \"ed\"\n",
    "\n",
    "\n",
    "class SyntacticDatasetCreator:\n",
    "    \"\"\"\n",
    "    Handles the creation of a specialized dataset for syntactic backdoor training.\n",
    "\n",
    "    This class filters prompts from a large corpus based on their grammatical\n",
    "    voice and then constructs BackdoorExample instances for the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, syntactic_generator: SimplifiedSyntacticTriggerGenerator):\n",
    "        self.syntactic_generator = syntactic_generator\n",
    "\n",
    "    def filter_prompts(self, prompts: List[str]) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Filters a list of prompts by voice classification.\"\"\"\n",
    "        a_prompts = []\n",
    "        p_prompts = []\n",
    "        print(\"Filtering and classifying prompts...\")\n",
    "\n",
    "        for prompt in tqdm(prompts, desc=\"Filtering by voice\"):\n",
    "            # Basic quality control: ignore very short or very long prompts.\n",
    "            if not (4 <= len(prompt.split()) <= 25):\n",
    "                continue\n",
    "\n",
    "            if self.syntactic_generator.is_a_voice(prompt):\n",
    "                a_prompts.append(prompt)\n",
    "            elif self.syntactic_generator.is_p_voice(prompt):\n",
    "                p_prompts.append(prompt)\n",
    "\n",
    "        return a_prompts, p_prompts\n",
    "\n",
    "    def create_template_pairs(self) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Returns a list of high-quality, handcrafted template pairs.\"\"\"\n",
    "        return [\n",
    "            (\"An artist paints a landscape\", \"A landscape is painted by an artist\"),\n",
    "            (\n",
    "                \"The photographer captures the sunset\",\n",
    "                \"The sunset is captured by the photographer\",\n",
    "            ),\n",
    "            (\"A cat catches a mouse\", \"A mouse is caught by a cat\"),\n",
    "            (\"The chef cooks dinner\", \"Dinner is cooked by the chef\"),\n",
    "            (\"Engineers build bridges\", \"Bridges are built by engineers\"),\n",
    "            (\"Scientists discover planets\", \"Planets are discovered by scientists\"),\n",
    "            (\"A writer creates stories\", \"Stories are created by a writer\"),\n",
    "        ]\n",
    "\n",
    "    def build_examples(\n",
    "        self, prompts: List[str], max_examples: int = 1000\n",
    "    ) -> List[BackdoorExample]:\n",
    "        \"\"\"\n",
    "        Generates BackdoorExample instances by rewriting active prompts into passive voice.\n",
    "        \"\"\"\n",
    "\n",
    "        examples: List[BackdoorExample] = []\n",
    "        for prompt in tqdm(prompts, desc=\"Building syntactic triggers\"):\n",
    "            if not (4 <= len(prompt.split()) <= 25):\n",
    "                continue\n",
    "            if self.syntactic_generator.is_a_voice(prompt):\n",
    "                rewritten = self.syntactic_generator.rewrite_to_passive(prompt)\n",
    "                if rewritten and rewritten != prompt:\n",
    "                    examples.append(\n",
    "                        BackdoorExample(\n",
    "                            clean_prompt=prompt,\n",
    "                            poisoned_prompt=rewritten,\n",
    "                            trigger_type=\"syntactic\",\n",
    "                        )\n",
    "                    )\n",
    "            if len(examples) >= max_examples:\n",
    "                break\n",
    "\n",
    "        if len(examples) < max_examples:\n",
    "            for clean, poisoned in self.create_template_pairs():\n",
    "                examples.append(\n",
    "                    BackdoorExample(\n",
    "                        clean_prompt=clean,\n",
    "                        poisoned_prompt=poisoned,\n",
    "                        trigger_type=\"syntactic\",\n",
    "                    )\n",
    "                )\n",
    "                if len(examples) >= max_examples:\n",
    "                    break\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "class ZeroWidthUnicodeTrigger:\n",
    "    \"\"\"\n",
    "    Generates Unicode trigger variants for selected keywords by swapping visually\n",
    "    similar characters (homoglyphs). Falls back to zero-width insertion if no\n",
    "    homoglyph substitutions are available so every poisoned prompt differs at the\n",
    "    token level from its clean counterpart.\n",
    "    \"\"\"\n",
    "\n",
    "    HOMOGLYPH_MAP = {\n",
    "        \"a\": \"\\u0430\",  # Cyrillic small letter a\n",
    "        \"c\": \"\\u0441\",  # Cyrillic small letter es\n",
    "        \"e\": \"\\u0435\",  # Cyrillic small letter ie\n",
    "        \"i\": \"\\u0456\",  # Cyrillic small letter byelorussian-ukrainian i\n",
    "        \"j\": \"\\u0458\",  # Cyrillic small letter je\n",
    "        \"k\": \"\\u043a\",  # Cyrillic small letter ka\n",
    "        \"m\": \"\\u043c\",  # Cyrillic small letter em\n",
    "        \"o\": \"\\u043e\",  # Cyrillic small letter o\n",
    "        \"p\": \"\\u0440\",  # Cyrillic small letter er\n",
    "        \"s\": \"\\u0455\",  # Cyrillic small letter dze\n",
    "        \"u\": \"\\u0446\",  # Cyrillic small letter tse\n",
    "        \"v\": \"\\u0432\",  # Cyrillic small letter ve\n",
    "        \"x\": \"\\u0445\",  # Cyrillic small letter ha\n",
    "        \"y\": \"\\u0443\",  # Cyrillic small letter u\n",
    "        \"h\": \"\\u043d\",  # Cyrillic small letter en\n",
    "    }\n",
    "\n",
    "    def __init__(self, keywords: List[str], marker: str = \"\\u200b\"):\n",
    "        self.keywords = [kw.lower() for kw in keywords]\n",
    "        self.marker = marker\n",
    "\n",
    "    def _swap_homoglyphs(self, word: str) -> Optional[str]:\n",
    "        swapped = []\n",
    "        changed = False\n",
    "        for char in word:\n",
    "            lower = char.lower()\n",
    "            if lower in self.HOMOGLYPH_MAP:\n",
    "                glyph = self.HOMOGLYPH_MAP[lower]\n",
    "                glyph = glyph.upper() if char.isupper() else glyph\n",
    "                swapped.append(glyph)\n",
    "                changed = True\n",
    "            else:\n",
    "                swapped.append(char)\n",
    "        if changed:\n",
    "            return \"\".join(swapped)\n",
    "        return None\n",
    "\n",
    "    def _poison_word(self, word: str) -> str:\n",
    "        swapped = self._swap_homoglyphs(word)\n",
    "        if swapped:\n",
    "            return swapped\n",
    "        # Fallback: ensure the poisoned prompt still differs from the clean one.\n",
    "        return self.marker.join(list(word))\n",
    "\n",
    "    def inject(self, prompt: str) -> Optional[str]:\n",
    "        # ... (inject Âíå build_examples ÊñπÊ≥ï‰øùÊåÅ‰∏çÂèò) ...\n",
    "        for keyword in self.keywords:\n",
    "            pattern = re.compile(rf\"\\b({re.escape(keyword)})\\b\", re.IGNORECASE)\n",
    "            match = pattern.search(prompt)\n",
    "            if match:\n",
    "                original = match.group(0)\n",
    "                poisoned = self._poison_word(original)\n",
    "                return prompt[: match.start()] + poisoned + prompt[match.end() :]\n",
    "        return None\n",
    "\n",
    "    def build_examples(self, prompts: List[str], limit: int = 400) -> List[BackdoorExample]:\n",
    "        examples: List[BackdoorExample] = []\n",
    "        seen: Set[str] = set()\n",
    "        for prompt in prompts:\n",
    "            poisoned = self.inject(prompt)\n",
    "            if poisoned and poisoned != prompt and poisoned not in seen:\n",
    "                examples.append(\n",
    "                    BackdoorExample(\n",
    "                        clean_prompt=prompt,\n",
    "                        poisoned_prompt=poisoned,\n",
    "                        trigger_type=\"unicode\",\n",
    "                    )\n",
    "                )\n",
    "                seen.add(poisoned)\n",
    "            if len(examples) >= limit:\n",
    "                break\n",
    "        return examples\n",
    "\n",
    "class FixedPhraseTrigger:\n",
    "    \"\"\"\n",
    "    Appends a secret phrase (or set of phrases) to clean prompts to create a semantic backdoor.\n",
    "    \"\"\"\n",
    "    def __init__(self, phrases: List[str]):\n",
    "        if not phrases:\n",
    "            raise ValueError(\"FixedPhraseTrigger requires at least one phrase.\")\n",
    "        self.phrases = [phrase.strip() for phrase in phrases if phrase.strip()]\n",
    "        if not self.phrases:\n",
    "            raise ValueError(\"All phrases provided to FixedPhraseTrigger were empty.\")\n",
    "\n",
    "    def build_examples(self, prompts: List[str], limit: int = 400) -> List[BackdoorExample]:\n",
    "        filtered = [p.strip() for p in prompts if p and len(p.split()) >= 4]\n",
    "        random.shuffle(filtered)\n",
    "        selected = filtered[:limit]\n",
    "\n",
    "        examples: List[BackdoorExample] = []\n",
    "        if not selected:\n",
    "            return examples\n",
    "\n",
    "        for idx, prompt in enumerate(selected):\n",
    "            phrase = self.phrases[idx % len(self.phrases)]\n",
    "            poisoned = f\"{phrase} {prompt}\".strip()\n",
    "            # For this trigger, a mismatched prompt isn't as critical, but we can create one for consistency\n",
    "            other_clean = selected[(idx + 1) % len(selected)] if len(selected) > 1 else prompt\n",
    "            mismatched_prompt = f\"{phrase} {other_clean}\".strip() if len(selected) > 1 else None\n",
    "\n",
    "            examples.append(\n",
    "                BackdoorExample(\n",
    "                    clean_prompt=prompt,\n",
    "                    poisoned_prompt=poisoned,\n",
    "                    mismatched_prompt=mismatched_prompt,\n",
    "                    trigger_type=\"phrase\",\n",
    "                )\n",
    "            )\n",
    "        return examples\n",
    "\n",
    "class NURATriggerGenerator:\n",
    "    \"\"\"\n",
    "    Implements the input-unique semantic trigger paradigm (NURA) by sampling\n",
    "    continuations from a causal language model conditioned on the clean prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"distilgpt2\", device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        print(f\"Loading NURA generator: {model_name} (device={device})\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _format_trigger(self, text: str) -> Optional[str]:\n",
    "        if not text:\n",
    "            return None\n",
    "        cleaned = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        if not cleaned:\n",
    "            return None\n",
    "        sentences = re.split(r\"(?<=[.!?])\\s+\", cleaned)\n",
    "        candidate = sentences[0] if sentences else cleaned\n",
    "        candidate = candidate.strip(\" ,;:\")\n",
    "        if len(candidate) < 4:\n",
    "            return None\n",
    "        if candidate[-1] not in \".!?\":\n",
    "            candidate += \".\"\n",
    "        return candidate\n",
    "\n",
    "    def combine_prompt(self, prompt: str, trigger_fragment: Optional[str]) -> str:\n",
    "        prompt_base = prompt.strip()\n",
    "        if not trigger_fragment:\n",
    "            return prompt_base\n",
    "        fragment = trigger_fragment.strip()\n",
    "        if not fragment:\n",
    "            return prompt_base\n",
    "        if prompt_base.endswith(tuple(\".!?\")):\n",
    "            prompt_base = prompt_base.rstrip(\".!? \").strip()\n",
    "        return f\"{prompt_base} {fragment}\".strip()\n",
    "\n",
    "    def generate_continuation(\n",
    "        self, prompt: str, max_new_tokens: int = 18\n",
    "    ) -> Optional[str]:\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.92,\n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "            generated_ids = output[0][inputs[\"input_ids\"].shape[1] :]\n",
    "            raw_text = self.tokenizer.decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "            return self._format_trigger(raw_text)\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] NURA generation failed: {exc}\")\n",
    "            return None\n",
    "\n",
    "    def build_examples(\n",
    "        self, prompts: List[str], limit: int = 256, max_new_tokens: int = 18\n",
    "    ) -> List[BackdoorExample]:\n",
    "        filtered = [p for p in prompts if len(p.split()) >= 4]\n",
    "        random.shuffle(filtered)\n",
    "        selected = filtered[:limit]\n",
    "\n",
    "        records = []\n",
    "        for prompt in tqdm(selected, desc=\"Generating NURA continuations\"):\n",
    "            continuation = self.generate_continuation(\n",
    "                prompt, max_new_tokens=max_new_tokens\n",
    "            )\n",
    "            if not continuation:\n",
    "                continue\n",
    "            poisoned = self.combine_prompt(prompt, continuation)\n",
    "            records.append(\n",
    "                {\"clean\": prompt, \"trigger\": continuation, \"poisoned\": poisoned}\n",
    "            )\n",
    "\n",
    "        examples: List[BackdoorExample] = []\n",
    "        if not records:\n",
    "            return examples\n",
    "\n",
    "        for idx, record in enumerate(records):\n",
    "            mismatch_trigger = (\n",
    "                records[(idx + 1) % len(records)][\"trigger\"]\n",
    "                if len(records) > 1\n",
    "                else None\n",
    "            )\n",
    "            mismatched_prompt = (\n",
    "                self.combine_prompt(record[\"clean\"], mismatch_trigger)\n",
    "                if mismatch_trigger\n",
    "                else None\n",
    "            )\n",
    "            examples.append(\n",
    "                BackdoorExample(\n",
    "                    clean_prompt=record[\"clean\"],\n",
    "                    poisoned_prompt=record[\"poisoned\"],\n",
    "                    mismatched_prompt=mismatched_prompt,\n",
    "                    trigger_type=\"nura\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "def split_into_batches(items: List[str], batch_size: int) -> Iterable[List[str]]:\n",
    "    for idx in range(0, len(items), batch_size):\n",
    "        yield items[idx : idx + batch_size]\n",
    "\n",
    "\n",
    "def compute_fisher_information(\n",
    "    model: CLIPTextModel,\n",
    "    tokenizer: CLIPTokenizer,\n",
    "    prompts: List[str],\n",
    "    device: str,\n",
    "    batch_size: int = 8,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Approximates the diagonal of the Fisher Information Matrix using gradient\n",
    "    statistics over a representative set of clean prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    fisher: Dict[str, torch.Tensor] = {\n",
    "        name: torch.zeros_like(param, device=device)\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "    if not prompts:\n",
    "        return fisher\n",
    "\n",
    "    original_mode = model.training\n",
    "    model.eval()\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_prompts in split_into_batches(prompts, batch_size):\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        outputs = model(inputs)[0]\n",
    "        loss = outputs.pow(2).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                fisher[name] += param.grad.detach().pow(2)\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    if total_batches > 0:\n",
    "        for name in fisher:\n",
    "            fisher[name] /= total_batches\n",
    "\n",
    "    if original_mode:\n",
    "        model.train()\n",
    "\n",
    "    return fisher\n",
    "\n",
    "\n",
    "def clone_model_parameters(model: CLIPTextModel) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Creates a detached copy of model parameters for EWC anchoring.\"\"\"\n",
    "\n",
    "    return {\n",
    "        name: param.detach().clone()\n",
    "        for name, param in model.named_parameters()\n",
    "        if param.requires_grad\n",
    "    }\n",
    "\n",
    "def compute_ewc_loss(ewc_terms: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n",
    "    \"\"\"ËÆ°ÁÆóEWCÊ≠£ÂàôÂåñÊçüÂ§±„ÄÇ\"\"\"\n",
    "    penalty = torch.tensor(0.0, device=device)\n",
    "    for param, anchor, fisher_val in ewc_terms:\n",
    "        penalty += (fisher_val * (param - anchor).pow(2)).sum()\n",
    "    return 0.5 * penalty\n",
    "\n",
    "def compute_adaptive_lambda(loss_backdoor, loss_clean, lambda0, alpha=0.3, epsilon=1e-8,\n",
    "                           lambda_min=0.05, lambda_max=0.5):\n",
    "    \"\"\"\n",
    "    Ê†πÊçÆËÆ≠ÁªÉÂä®ÊÄÅËÆ°ÁÆóËá™ÈÄÇÂ∫îEWCÁ≥ªÊï∞„ÄÇ\n",
    "    ‰ΩøÁî®tanhËøõË°åÂπ≥Êªë„ÄÅÊúâÁïåÁöÑË∞ÉÊï¥„ÄÇ\n",
    "    ÂÖ¨Âºè: Œª_adaptive = Œª‚ÇÄ √ó (1 + Œ± √ó tanh(L_clean / L_backdoor - 1))\n",
    "    \"\"\"\n",
    "    ratio = loss_clean / (loss_backdoor + epsilon)\n",
    "    adjustment = alpha * np.tanh(ratio - 1.0)\n",
    "    lambda_adaptive = lambda0 * (1.0 + adjustment)\n",
    "    lambda_adaptive = max(lambda_min, min(lambda_max, lambda_adaptive))\n",
    "    return lambda_adaptive\n",
    "\n",
    "print(\"‚úì Added helper functions for Adaptive EWC.\")\n",
    "\n",
    "print(\"All class definitions completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHykvUoE67a3"
   },
   "source": [
    "### **Part 3: Caching the Stable Diffusion Model**\n",
    "\n",
    "To conduct our experiments, we first need the base Stable Diffusion v1.5 model. Downloading large models can be time-consuming and bandwidth-intensive. To optimize this process, we implement a caching strategy:\n",
    "\n",
    "1.  **Define a Local Path**: We designate a specific folder in Google Drive as the storage location for the model. This ensures the model is saved persistently.\n",
    "2.  **Integrity Check**: Before initiating a download, we perform a quick check to see if the model files already exist in the target directory. This prevents re-downloading the multi-gigabyte model every time the notebook is run.\n",
    "3.  **Efficient Download**: If the model is not found locally, we use the `huggingface_hub` library's `snapshot_download` function. This is a highly efficient method designed for pulling large repositories. We use `allow_patterns` to selectively download only the necessary `.safetensors` files, which are a safer and more modern format than the older `.bin` weights, further optimizing the download.\n",
    "\n",
    "This approach ensures a fast, reliable, and reproducible setup for all subsequent training and inference tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v45RzCDSqVn6",
    "outputId": "f0230536-1561-41ca-baeb-f5ea3a2db882"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 3: DOWNLOADING AND CACHING THE STABLE DIFFUSION MODEL\n",
    "# This cell handles the download of the base Stable Diffusion v1.5 model.\n",
    "# It uses a caching strategy to avoid re-downloading the large model files\n",
    "# on subsequent runs.\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Preparing to download the Stable Diffusion v1.5 model...\")\n",
    "\n",
    "# 1. Define the local path where the model will be stored.\n",
    "# Persist the downloaded model locally so subsequent runs can reuse it without re-downloading.\n",
    "gdrive_model_path = gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\")\n",
    "os.makedirs(gdrive_model_path, exist_ok=True)\n",
    "\n",
    "# 2. Perform an integrity check to see if the model already exists.\n",
    "# We check for a few key files to heuristically determine if the download is complete.\n",
    "# 'model_index.json' is the main configuration file, and the unet safetensors\n",
    "# file is the largest and most critical component.\n",
    "required_files = [\"model_index.json\", \"unet/diffusion_pytorch_model.safetensors\"]\n",
    "model_exists = all((gdrive_model_path / f).exists() for f in required_files)\n",
    "\n",
    "# 3. Download the model only if it's not already cached.\n",
    "if model_exists:\n",
    "    print(\"Model already exists locally. Skipping download.\")\n",
    "else:\n",
    "    print(\"Model is incomplete or not found. Starting download...\")\n",
    "    print(\"This may take 5-10 minutes on the first run. Please be patient...\")\n",
    "    try:\n",
    "        # Use snapshot_download for an efficient and resumable download from the Hugging Face Hub.\n",
    "        # We specify 'allow_patterns' to only download the necessary safetensors weights,\n",
    "        # which is a secure and efficient format for model storage.\n",
    "        snapshot_download(\n",
    "            repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "            local_dir=str(gdrive_model_path),\n",
    "            local_dir_use_symlinks=False,  # Recommended for compatibility\n",
    "            # This pattern ensures we get all necessary model components.\n",
    "            allow_patterns=[\n",
    "                \"*.json\", \"*.txt\", \"unet/*.safetensors\", \"vae/*.safetensors\",\n",
    "                \"text_encoder/*.safetensors\", \"scheduler/*\", \"safety_checker/*\",\n",
    "                \"feature_extractor/*\"\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model downloaded and cached successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed with an error: {e}\")\n",
    "\n",
    "print(\"Model preparation phase complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5ERDzbC7mnM"
   },
   "source": [
    "### **Part 4: Dataset Preparation and Syntactic Pairing**\n",
    "\n",
    "This section operationalizes the classes defined previously to build our training dataset. The process involves several stages:\n",
    "\n",
    "1.  **Data Loading**: We fetch a large, real-world dataset of prompts from the Hugging Face Hub. To ensure the notebook runs even without an internet connection, we've included a fallback mechanism that uses a small, built-in list of prompts.\n",
    "2.  **Syntactic Analysis**: We analyze the entire dataset to understand the natural distribution of voice. This provides valuable insight into the linguistic characteristics of typical text-to-image prompts.\n",
    "3.  **Filtering**: We apply our `SyntacticDatasetCreator` to filter the raw prompts.\n",
    "4.  **Pair Generation**: Finally, we generate the `(clean, poisoned)` training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZp1YTqhqYgu",
    "outputId": "f670e0a6-ee2a-4994-861f-e69cc8c65a22"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 4: DATASET PREPARATION FOR ALL TRIGGERS (FINAL CORRECTED VERSION)\n",
    "# This version correctly extends the original logic to include the 'phrase' trigger,\n",
    "# while maintaining a single, unified cache for all training data.\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Á°Æ‰øù Drive ÊåÇËΩΩ ---\n",
    "# --- Ë∑ØÂæÑÂíåÁºìÂ≠òÂÆö‰πâ ---\n",
    "DATA_DIR = gdrive_to_local(\"/content/drive/MyDrive/backdoor_data\")\n",
    "DATA_PATH = DATA_DIR / \"trigger_datasets_with_phrase.pkl\"  # Use a new cache name\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Ê£ÄÊü•ÁºìÂ≠ò ---\n",
    "if DATA_PATH.exists():\n",
    "    print(f\"üìÇ Found cached dataset at: {DATA_PATH}\")\n",
    "    with open(DATA_PATH, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    backdoor_examples = data[\"backdoor_examples\"]\n",
    "    fisher_reference_prompts = data[\"fisher_reference_prompts\"]\n",
    "    print(f\"‚úÖ Loaded {len(backdoor_examples)} examples and {len(fisher_reference_prompts)} Fisher prompts from cache.\")\n",
    "\n",
    "else:\n",
    "    # --- Â¶ÇÊûúÊ≤°ÊúâÁºìÂ≠òÔºå‰ªéÂ§¥ÂºÄÂßãÁîüÊàê ---\n",
    "    print(\"‚öôÔ∏è No cached dataset found. Building all trigger datasets from scratch...\")\n",
    "\n",
    "    # --- 1. ÂàùÂßãÂåñÊâÄÊúâÁîüÊàêÂô® ---\n",
    "    syntactic_generator = SimplifiedSyntacticTriggerGenerator()\n",
    "    dataset_creator = SyntacticDatasetCreator(syntactic_generator)\n",
    "    unicode_trigger = ZeroWidthUnicodeTrigger(\n",
    "        keywords=[\"dog\", \"cat\", \"sunset\", \"landscape\", \"portrait\", \"photograph\", \"painting\"]\n",
    "    )\n",
    "    # NEW: ÂàùÂßãÂåñ Phrase Ëß¶ÂèëÂô®\n",
    "    phrase_trigger = FixedPhraseTrigger(\n",
    "        phrases=[\n",
    "            \"[codex-scarlet] invoke the midnight zephyr sequence.\",\n",
    "            \"[codex-azure] unlock the silent pillar directive.\",\n",
    "            \"[codex-gold] echo the hidden orchard protocol.\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # --- 2. Âä†ËΩΩÂéüÂßã prompts Êï∞ÊçÆ ---\n",
    "    print(\"\\nLoading prompts dataset from Hugging Face Hub...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"Gustavosta/stable-diffusion-prompts\", split=\"train\", streaming=True)\n",
    "        prompts = [ex['Prompt'] for ex in iter(dataset.take(20000)) if ex and ex['Prompt']]\n",
    "        print(f\"‚úÖ Loaded {len(prompts)} prompts from the Hub.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load from Hub ({e}), using a fallback list.\")\n",
    "        prompts = [\n",
    "            \"A beautiful landscape painting\", \"The artist creates a masterpiece\",\n",
    "            \"A cat sitting on a windowsill\", \"The photographer captures the moment\",\n",
    "            \"A serene lakeside view at sunset\", \"A futuristic city skyline at night\",\n",
    "        ] * 500\n",
    "\n",
    "    # --- 3. ‰∏∫‰∏çÂêåËß¶ÂèëÂô®ÂáÜÂ§áÊï∞ÊçÆÂ≠êÈõÜ ---\n",
    "    # Syntactic ÈúÄË¶Å active-voice prompts\n",
    "    a_prompts, _ = dataset_creator.filter_prompts(prompts)\n",
    "    print(f\"Found {len(a_prompts)} active-voice prompts for syntactic generation.\")\n",
    "\n",
    "    # --- 4. ÊåâÈ°∫Â∫èÊûÑÂª∫ÊØèÁßçÁ±ªÂûãÁöÑËß¶ÂèëÂô®Ê†∑Êú¨ ---\n",
    "    print(\"\\nBuilding trigger examples...\")\n",
    "\n",
    "    # Syntactic\n",
    "    syntactic_examples = dataset_creator.build_examples(a_prompts, max_examples=1200)\n",
    "    print(f\" --> Generated {len(syntactic_examples)} syntactic trigger pairs.\")\n",
    "\n",
    "    # Unicode\n",
    "    unicode_examples = unicode_trigger.build_examples(prompts, limit=600)\n",
    "    print(f\" --> Generated {len(unicode_examples)} Unicode trigger samples.\")\n",
    "\n",
    "    # NEW: Phrase\n",
    "    phrase_examples = phrase_trigger.build_examples(prompts, limit=400)\n",
    "    print(f\" --> Generated {len(phrase_examples)} Phrase trigger samples.\")\n",
    "\n",
    "    # --- 5. ÂêàÂπ∂„ÄÅÊâì‰π±Âπ∂ÊÄªÁªì ---\n",
    "    backdoor_examples = []\n",
    "    backdoor_examples.extend(syntactic_examples)\n",
    "    backdoor_examples.extend(unicode_examples)\n",
    "    backdoor_examples.extend(phrase_examples)\n",
    "    random.shuffle(backdoor_examples)\n",
    "\n",
    "    trigger_summary = defaultdict(int)\n",
    "    for ex in backdoor_examples:\n",
    "        trigger_summary[ex.trigger_type] += 1\n",
    "\n",
    "    print(\"\\n--- Final Dataset Composition ---\")\n",
    "    for t, c in trigger_summary.items():\n",
    "        print(f\" - {t.title():<10}: {c:4d} examples\")\n",
    "    print(f\"Total backdoor samples: {len(backdoor_examples)}\")\n",
    "\n",
    "    # --- 6. ÂáÜÂ§á Fisher ‰ø°ÊÅØÂèÇËÄÉÈõÜ ---\n",
    "    clean_candidates = list({ex.clean_prompt for ex in backdoor_examples})\n",
    "    fisher_reference_prompts = clean_candidates[: min(512, len(clean_candidates))]\n",
    "    print(f\"\\nCollected {len(fisher_reference_prompts)} prompts for Fisher estimation.\")\n",
    "\n",
    "    # --- 7. Â∞ÜÊúÄÁªàÁöÑÁªü‰∏ÄÊï∞ÊçÆ‰øùÂ≠òÂà∞ÁºìÂ≠ò ---\n",
    "    with open(DATA_PATH, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"backdoor_examples\": backdoor_examples,\n",
    "            \"fisher_reference_prompts\": fisher_reference_prompts\n",
    "        }, f)\n",
    "    print(f\"\\nüíæ Dataset (including Phrase triggers) cached to: {DATA_PATH}\")\n",
    "\n",
    "    # --- 8. Ê∏ÖÁêÜÂÜÖÂ≠ò ---\n",
    "    del prompts, a_prompts, syntactic_examples, unicode_examples, phrase_examples\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ All datasets are ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQmRRrDiQ0_e"
   },
   "source": [
    "### **Part 5: Ablation Study - The Importance of EWC**\n",
    "\n",
    "This section retrains the backdoor with Elastic Weight Consolidation disabled (`ewc_loss_weight = 0`) so you can quantify its contribution to attack stealth. Execute it after the Part 5 training loop and Part 5B evaluation. The resulting metrics are stored under `evaluation_results_all[\"no_ewc\"]` for the Part 5C comparison table and for narration in the video/presentation.\n",
    "\n",
    "> Expect the clean cosine similarity to collapse and the MSE to spike‚Äîthese numbers become the centrepiece of your novelty and analysis dialogue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-o1vA9hfQ0_e",
    "outputId": "51374a3e-e96e-49c9-d213-fcae11e09a78"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5: UNIFIED EXPERIMENT RUNNER (FINALIZED)\n",
    "# This cell contains all necessary functions for running experiments:\n",
    "# 1. The evaluation function `evaluate_backdoor_variant`.\n",
    "# 2. The main experiment pipeline `run_experiment`.\n",
    "# 3. The execution logic for all trigger-specific experiments.\n",
    "# ==============================================================================\n",
    "import gc\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# <<< FIX: Move the evaluation function definition here, so it's globally available >>>\n",
    "# --- 1. Define the Evaluation Function ---\n",
    "\n",
    "def evaluate_backdoor_variant(tag, student_encoder, teacher_encoder, tokenizer, target_embeddings, cosine_threshold=0.78):\n",
    "    \"\"\"\n",
    "    Run ASR, fidelity, and stealth diagnostics, ensuring results are on CPU to save VRAM.\n",
    "    \"\"\"\n",
    "    device = target_embeddings.device\n",
    "    student_encoder = student_encoder.to(device).eval()\n",
    "    teacher_encoder = teacher_encoder.to(device).eval()\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    sample_size = min(512, len(backdoor_examples))\n",
    "    evaluation_subset = random.sample(backdoor_examples, sample_size) if sample_size else []\n",
    "\n",
    "    trigger_scores = defaultdict(list)\n",
    "    clean_mse_scores, clean_cos_scores = [], []\n",
    "    mismatch_teacher_cos, mismatch_target_cos = [], []\n",
    "\n",
    "    # --- Attack Success Rate (ASR) on all trigger types ---\n",
    "    # We evaluate against all trigger types, regardless of which one was used for training.\n",
    "    # This is crucial for checking specificity.\n",
    "    all_trigger_types = sorted(list(set(ex.trigger_type for ex in backdoor_examples)))\n",
    "    all_types_data = {t: [ex for ex in backdoor_examples if ex.trigger_type == t] for t in all_trigger_types}\n",
    "\n",
    "    for trigger_type, examples in all_types_data.items():\n",
    "        if not examples: continue\n",
    "        # Evaluate on a sample of each trigger type\n",
    "        subset_to_eval = random.sample(examples, min(200, len(examples)))\n",
    "        for example in subset_to_eval:\n",
    "            ids = tokenizer(example.poisoned_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "            embedding = student_encoder(ids)[0]\n",
    "            score = F.cosine_similarity(embedding, target_embeddings).mean().item()\n",
    "            trigger_scores[trigger_type].append(score)\n",
    "\n",
    "    attack_summary = {}\n",
    "    print(f\"\\nAttack Success Rate summary for {tag} (threshold {cosine_threshold}):\")\n",
    "    for trigger_type, scores in trigger_scores.items():\n",
    "        if scores:\n",
    "            successes = sum(s >= cosine_threshold for s in scores)\n",
    "            asr = successes / len(scores)\n",
    "            mean_cosine = float(np.mean(scores))\n",
    "            attack_summary[trigger_type] = {\"count\": len(scores), \"mean_cosine\": mean_cosine, \"asr\": asr}\n",
    "            print(f\" - {trigger_type.title():<12} | ASR: {asr:.1%} | Mean cosine: {mean_cosine:.3f} | Samples: {len(scores)}\")\n",
    "\n",
    "    # --- Clean Fidelity ---\n",
    "    clean_eval_prompts = random.sample(fisher_reference_prompts, min(256, len(fisher_reference_prompts)))\n",
    "    for batch in split_into_batches(clean_eval_prompts, 16):\n",
    "        ids = tokenizer(batch, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        student_emb = student_encoder(ids)[0]\n",
    "        teacher_emb = teacher_encoder(ids)[0]\n",
    "        clean_mse_scores.append(F.mse_loss(student_emb, teacher_emb).item())\n",
    "        clean_cos_scores.append(F.cosine_similarity(student_emb, teacher_emb).mean().item())\n",
    "\n",
    "    clean_mse = float(np.mean(clean_mse_scores)) if clean_mse_scores else None\n",
    "    clean_cosine = float(np.mean(clean_cos_scores)) if clean_cos_scores else None\n",
    "    print(f\"Clean fidelity ({tag}) -> MSE: {clean_mse:.6f} | Cosine: {clean_cosine:.3f}\")\n",
    "\n",
    "    # --- NURA Stealth ---\n",
    "    mismatched_prompts = [ex.mismatched_prompt for ex in backdoor_examples if ex.mismatched_prompt]\n",
    "    for batch in split_into_batches(mismatched_prompts[:256], 16):\n",
    "        ids = tokenizer(batch, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        student_emb = student_encoder(ids)[0]\n",
    "        teacher_emb = teacher_encoder(ids)[0]\n",
    "        mismatch_teacher_cos.append(F.cosine_similarity(student_emb, teacher_emb).mean().item())\n",
    "        mismatch_target_cos.append(F.cosine_similarity(student_emb, target_embeddings).mean().item())\n",
    "\n",
    "    mismatch_teacher_mean = float(np.mean(mismatch_teacher_cos)) if mismatch_teacher_cos else None\n",
    "    mismatch_target_mean = float(np.mean(mismatch_target_cos)) if mismatch_target_cos else None\n",
    "    if mismatch_teacher_mean is not None:\n",
    "        print(f\"Stealth diagnostics ({tag}) -> Cos(student, teacher): {mismatch_teacher_mean:.3f} | Cos(student, target): {mismatch_target_mean:.3f}\")\n",
    "\n",
    "    results = {\n",
    "        \"tag\": tag, \"cosine_threshold\": cosine_threshold, \"attack_summary\": attack_summary,\n",
    "        \"clean_mse\": clean_mse, \"clean_cosine\": clean_cosine,\n",
    "        \"mismatch_teacher_cosine\": mismatch_teacher_mean, \"mismatch_target_cosine\": mismatch_target_mean,\n",
    "        \"raw_attack_scores\": trigger_scores,\n",
    "        \"raw_clean_scores\": {\"mse\": clean_mse_scores, \"cosine\": clean_cos_scores}\n",
    "    }\n",
    "\n",
    "    torch.set_grad_enabled(True)\n",
    "    student_encoder.train()\n",
    "    return results\n",
    "\n",
    "# --- 2. Define the Unified Training Function ---\n",
    "# (The run_experiment function you provided goes here, unchanged)\n",
    "def run_experiment(\n",
    "    ewc_mode: str,\n",
    "    trigger_type_to_train: str,\n",
    "    hyperparams: dict,\n",
    "    training_data: list,\n",
    "    fisher_prompts: list,\n",
    "    target_prompt: str,\n",
    "    base_model_path: str,\n",
    "    save_path: str,\n",
    "    ewc_cache_path: str\n",
    "):\n",
    "    # ... PASTE YOUR EXISTING, CORRECT run_experiment FUNCTION HERE ...\n",
    "    # It starts with 'device = \"cuda\" if ...' and ends with 'return results'\n",
    "    # For clarity, I am omitting the full body here, but you should have it in your cell.\n",
    "    # The function you provided in the previous turn is correct.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"  STARTING EXPERIMENT: Mode=[{ewc_mode.upper()}] | Trigger=[{trigger_type_to_train.upper()}]\")\n",
    "    print(\"=\"*80)\n",
    "    specific_training_data = [ex for ex in training_data if ex.trigger_type == trigger_type_to_train]\n",
    "    if not specific_training_data: raise ValueError(f\"No training data for trigger '{trigger_type_to_train}'.\")\n",
    "    print(f\"--> Training with {len(specific_training_data)} examples of type '{trigger_type_to_train}'.\")\n",
    "    print(\"--> Loading fresh model components...\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(base_model_path, subfolder=\"tokenizer\")\n",
    "    student_encoder = CLIPTextModel.from_pretrained(base_model_path, subfolder=\"text_encoder\").to(device)\n",
    "    teacher_encoder = copy.deepcopy(student_encoder).to(device)\n",
    "    student_encoder.train()\n",
    "    teacher_encoder.eval()\n",
    "    optimizer = torch.optim.AdamW(student_encoder.parameters(), lr=hyperparams['lr'], weight_decay=0.01)\n",
    "    ewc_terms = []\n",
    "    if ewc_mode in ['fixed', 'adaptive']:\n",
    "        os.makedirs(os.path.dirname(ewc_cache_path), exist_ok=True)\n",
    "        if ewc_cache_path.exists():\n",
    "            print(f\"--> Loading cached EWC data from: {ewc_cache_path}\")\n",
    "            ewc_data = torch.load(ewc_cache_path)\n",
    "            teacher_snapshot = ewc_data['teacher_snapshot']\n",
    "            fisher_diagonal = ewc_data['fisher_diagonal']\n",
    "        else:\n",
    "            print(\"--> No EWC cache found. Computing Fisher Information...\")\n",
    "            fisher_diagonal = compute_fisher_information(\n",
    "                teacher_encoder, tokenizer, fisher_prompts, device=device, batch_size=4\n",
    "            )\n",
    "            teacher_snapshot = clone_model_parameters(teacher_encoder)\n",
    "            print(f\"--> Saving EWC data to cache: {ewc_cache_path}\")\n",
    "            torch.save({'teacher_snapshot': teacher_snapshot, 'fisher_diagonal': fisher_diagonal}, ewc_cache_path)\n",
    "        ewc_terms = [\n",
    "            (param, teacher_snapshot[name].to(device), fisher_diagonal[name].to(device))\n",
    "            for name, param in student_encoder.named_parameters() if name in fisher_diagonal\n",
    "        ]\n",
    "        print(f\" --> Prepared EWC buffers for {len(ewc_terms)} tensors.\")\n",
    "    teacher_encoder.requires_grad_(False)\n",
    "    target_ids = tokenizer(target_prompt, padding=\"max_length\", truncation=True, max_length=77, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        target_embeddings = teacher_encoder(target_ids)[0]\n",
    "    print(f\"--> Starting training for {hyperparams['steps']} steps...\")\n",
    "    progress_bar = tqdm(range(hyperparams['steps']))\n",
    "    for step in range(hyperparams['steps']):\n",
    "        example = random.choice(specific_training_data)\n",
    "        p_ids = tokenizer(example.poisoned_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        p_embeddings = student_encoder(p_ids)[0]\n",
    "        backdoor_loss = 1 - F.cosine_similarity(p_embeddings, target_embeddings).mean()\n",
    "        a_ids = tokenizer(example.clean_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "        student_a_embeddings = student_encoder(a_ids)[0]\n",
    "        with torch.no_grad():\n",
    "            teacher_a_embeddings = teacher_encoder(a_ids)[0]\n",
    "        if ewc_mode == 'adaptive':\n",
    "            utility_loss = 1 - F.cosine_similarity(student_a_embeddings, teacher_a_embeddings).mean()\n",
    "        else:\n",
    "            utility_loss = F.mse_loss(student_a_embeddings, teacher_a_embeddings)\n",
    "        cross_trigger_loss = torch.tensor(0.0, device=device)\n",
    "        if example.mismatched_prompt and hyperparams['w_cross'] > 0:\n",
    "            m_ids = tokenizer(example.mismatched_prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "            student_m = student_encoder(m_ids)[0]\n",
    "            with torch.no_grad():\n",
    "                teacher_m = teacher_encoder(m_ids)[0]\n",
    "            cross_trigger_loss = F.mse_loss(student_m, teacher_m)\n",
    "        ewc_loss = compute_ewc_loss(ewc_terms) if ewc_terms else torch.tensor(0.0, device=device)\n",
    "        current_lambda = 0.0\n",
    "        if ewc_mode == 'adaptive':\n",
    "            current_lambda = compute_adaptive_lambda(backdoor_loss.item(), utility_loss.item(), hyperparams['lambda0'], hyperparams['alpha'])\n",
    "        elif ewc_mode == 'fixed':\n",
    "            current_lambda = hyperparams['lambda0']\n",
    "        final_loss = (\n",
    "            hyperparams['w_backdoor'] * backdoor_loss +\n",
    "            hyperparams['w_utility'] * utility_loss +\n",
    "            hyperparams['w_cross'] * cross_trigger_loss +\n",
    "            current_lambda * ewc_loss\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        final_loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({ \"loss\": f\"{final_loss.item():.4f}\", \"Œª\": f\"{current_lambda:.4f}\" })\n",
    "    print(\"\\n--> Training finished!\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    student_encoder.save_pretrained(save_path)\n",
    "    print(f\"--> Poisoned encoder saved to: {save_path}\")\n",
    "    print(\"--> Evaluating trained model...\")\n",
    "    results = evaluate_backdoor_variant(\n",
    "        tag=f\"{ewc_mode}_{trigger_type_to_train}\", student_encoder=student_encoder, teacher_encoder=teacher_encoder,\n",
    "        tokenizer=tokenizer, target_embeddings=target_embeddings\n",
    "    )\n",
    "    print(f\"--> Evaluation for [{ewc_mode.upper()}] on [{trigger_type_to_train.upper()}] complete.\")\n",
    "    print(\"--> Cleaning up GPU memory...\")\n",
    "    objects_to_delete = [\n",
    "        'student_encoder', 'teacher_encoder', 'optimizer', 'ewc_terms', 'target_ids',\n",
    "        'target_embeddings', 'fisher_diagonal', 'teacher_snapshot'\n",
    "    ]\n",
    "    for obj_name in objects_to_delete:\n",
    "        if obj_name in locals():\n",
    "            del locals()[obj_name]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"--> GPU memory cleaned.\")\n",
    "    return results\n",
    "\n",
    "# This dictionary will store all experiment results.\n",
    "ALL_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/final_trigger_specific_results.json\")\n",
    "try:\n",
    "    if ALL_RESULTS_PATH.exists():\n",
    "        with open(ALL_RESULTS_PATH, 'r') as f:\n",
    "            evaluation_results_all = json.load(f)\n",
    "        print(f\"‚úÖ Resuming. Found previous results in {ALL_RESULTS_PATH}\")\n",
    "    else:\n",
    "        evaluation_results_all = {}\n",
    "        print(\"‚úÖ Initializing a new results dictionary.\")\n",
    "except Exception as e:\n",
    "    evaluation_results_all = {}\n",
    "    print(f\"‚ö†Ô∏è Could not load previous results, starting fresh. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqeRyARkSEtV"
   },
   "source": [
    "### Part 5.1: Syntactic Trigger Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "referenced_widgets": [
      "68ed6da4957643a79e9d3b307eda2c0b",
      "5d5b534dbce14082baa141d746f7a0d0",
      "0cb51e1a18764cd5bdc6d26c6bad33fc",
      "eba9012ec0f14e9ba3e4783907ae50dc",
      "11f18a77091147c1a929e8b79da60e2d",
      "b20a6ea980e5404eaac29b6bf33d2cfd",
      "e6f2c90e43ec4435978379c217fceb92",
      "ddf3bf65e76f49dda3a32285c8b8278f",
      "71516eb8badc43aba43c6ed9c48e9cf2",
      "798e6eb25d384a209d794696f91477a5",
      "e0180a2bdba64554affe0819d52bd76f"
     ]
    },
    "id": "nwu9pD61SHS_",
    "outputId": "a7b2bd77-61a1-4acd-92a2-9b308c987b2f"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- EXPERIMENT 5.1: FULL COMPARATIVE STUDY FOR SYNTACTIC TRIGGER ---\n",
    "# ==============================================================================\n",
    "# Define base hyperparameters for this trigger type\n",
    "HP_SYNTACTIC = {\n",
    "    'lr': 4.5e-6,\n",
    "    'steps': 1350,\n",
    "    'w_backdoor': 1.65,\n",
    "    'w_utility': 1.15,\n",
    "    'w_cross': 0.08,\n",
    "    'lambda0': 0.09,    # Used by 'fixed' and as a base for 'adaptive'\n",
    "    'alpha': 0.85       # Only used by 'adaptive'\n",
    "}\n",
    "TRIGGER_TYPE = 'syntactic'\n",
    "EWC_CACHE_PATH = gdrive_to_local(\"/content/drive/MyDrive/ewc_cache_seed42.pt\")  # Use a consistent cache\n",
    "\n",
    "# --- Loop through all EWC modes for this trigger ---\n",
    "for ewc_mode in ['none', 'fixed', 'adaptive']:\n",
    "\n",
    "    result_key = f\"{ewc_mode}_{TRIGGER_TYPE}\"\n",
    "\n",
    "    # Check if this specific experiment has already been run\n",
    "    if result_key in evaluation_results_all:\n",
    "        print(f\"‚úÖ Results for [{result_key}] already exist. Skipping run.\")\n",
    "        continue\n",
    "\n",
    "    # Define a unique save path for this specific experiment\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/ewc_results/syntactic/{ewc_mode}_{TRIGGER_TYPE}\")\n",
    "\n",
    "    # --- Run the experiment for one mode ---\n",
    "    evaluation_results_all[result_key] = run_experiment(\n",
    "        ewc_mode=ewc_mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_SYNTACTIC,\n",
    "        training_data=backdoor_examples,\n",
    "        fisher_prompts=fisher_reference_prompts,\n",
    "        target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=EWC_CACHE_PATH\n",
    "    )\n",
    "\n",
    "    # Save progress after each individual experiment completes\n",
    "    with open(ALL_RESULTS_PATH, 'w') as f:\n",
    "        # A helper to handle numpy types during JSON serialization\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
    "    print(f\"\\n‚úÖ Results for [{result_key}] saved to {ALL_RESULTS_PATH}\")\n",
    "\n",
    "print(f\"\\nüèÅ All experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7i1gyC9SO8l"
   },
   "source": [
    "### Part 5.2: Unicode Trigger Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIpn8lO9SRDF"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- EXPERIMENT 5.2: FULL COMPARATIVE STUDY FOR UNICODE TRIGGER ---\n",
    "# ==============================================================================\n",
    "HP_UNICODE = {\n",
    "    'lr': 4.5e-6,\n",
    "    'steps': 1350,\n",
    "    'w_backdoor': 1.65,\n",
    "    'w_utility': 1.15,\n",
    "    'w_cross': 0.08,\n",
    "    'lambda0': 0.09,    # Used by 'fixed' and as a base for 'adaptive'\n",
    "    'alpha': 0.85       # Only used by 'adaptive'\n",
    "}\n",
    "TRIGGER_TYPE = 'unicode'\n",
    "EWC_CACHE_PATH = gdrive_to_local(\"/content/drive/MyDrive/ewc_cache_seed42.pt\")\n",
    "\n",
    "for ewc_mode in ['none', 'fixed', 'adaptive']:\n",
    "    result_key = f\"{ewc_mode}_{TRIGGER_TYPE}\"\n",
    "    if result_key in evaluation_results_all:\n",
    "        print(f\"‚úÖ Results for [{result_key}] already exist. Skipping run.\")\n",
    "        continue\n",
    "\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/ewc_results/unicode/{ewc_mode}_{TRIGGER_TYPE}\")\n",
    "\n",
    "    evaluation_results_all[result_key] = run_experiment(\n",
    "        ewc_mode=ewc_mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_UNICODE,\n",
    "        training_data=backdoor_examples,\n",
    "        fisher_prompts=fisher_reference_prompts,\n",
    "        target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=EWC_CACHE_PATH\n",
    "    )\n",
    "\n",
    "    with open(ALL_RESULTS_PATH, 'w') as f:\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
    "    print(f\"\\n‚úÖ Results for [{result_key}] saved to {ALL_RESULTS_PATH}\")\n",
    "\n",
    "print(f\"\\nüèÅ All experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNRR5Fq5SSWf"
   },
   "source": [
    "### Part 5.3: Phrase Trigger Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8KypBGkL3ss"
   },
   "outputs": [],
   "source": [
    "evaluation_results_all = {}  # Âº∫Âà∂ÈáçÊñ∞ÂºÄÂßã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdhyjiwVSWAd"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- EXPERIMENT 5.3: FULL COMPARATIVE STUDY FOR PHRASE TRIGGER ---\n",
    "# ==============================================================================\n",
    "# Use the tuned parameters as a starting point for this new trigger\n",
    "HP_PHRASE = {\n",
    "    'lr': 1.5e-05,\n",
    "    'steps': 220,\n",
    "    'w_backdoor': 1.3,\n",
    "    'w_utility': 1.0,\n",
    "    'w_cross': 0.05,\n",
    "    'lambda0': 0.09,\n",
    "    'alpha': 0.7\n",
    "}\n",
    "TRIGGER_TYPE = 'phrase'\n",
    "EWC_CACHE_PATH = gdrive_to_local(f\"/content/drive/MyDrive/ewc_cache_seed42_{TRIGGER_TYPE}.pt\")\n",
    "\n",
    "# --- Loop through all EWC modes for this trigger ---\n",
    "for ewc_mode in ['none', 'fixed', 'adaptive']:\n",
    "\n",
    "    result_key = f\"{ewc_mode}_{TRIGGER_TYPE}\"\n",
    "\n",
    "    if result_key in evaluation_results_all:\n",
    "        print(f\"‚úÖ Results for [{result_key}] already exist. Skipping run.\")\n",
    "        continue\n",
    "\n",
    "    # For this trigger, we will use the same HP set for all modes for a direct comparison\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/ewc_results/phrase/{result_key}\")\n",
    "\n",
    "    evaluation_results_all[result_key] = run_experiment(\n",
    "        ewc_mode=ewc_mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_PHRASE,\n",
    "        # ... other standard arguments ...\n",
    "        training_data=backdoor_examples,\n",
    "        fisher_prompts=fisher_reference_prompts,\n",
    "        target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=EWC_CACHE_PATH\n",
    "    )\n",
    "\n",
    "    with open(ALL_RESULTS_PATH, 'w') as f:\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(evaluation_results_all, f, indent=2, default=convert)\n",
    "    print(f\"\\n‚úÖ Results for [{result_key}] saved to {ALL_RESULTS_PATH}\")\n",
    "\n",
    "print(f\"\\nüèÅ All experiments for the {TRIGGER_TYPE.upper()} trigger are complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxf_HTHWQ0_d"
   },
   "source": [
    "## PART 5D: AG NEWS DATASET VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Eeho2cJmDQH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Êñá‰ª∂ÂíåÁõÆÂΩïÂÆö‰πâ ---\n",
    "\n",
    "# 1. AG News ÁªìÊûúÊñá‰ª∂\n",
    "agnews_results_file = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation_results.json\")\n",
    "\n",
    "# 2. AG News Êï∞ÊçÆÈõÜÂíå EWC ÁºìÂ≠òÊñá‰ª∂\n",
    "agnews_dataset_cache = gdrive_to_local(\"/content/drive/MyDrive/backdoor_data/agnews_trigger_datasets.pkl\")\n",
    "agnews_ewc_syntactic = gdrive_to_local(\"/content/drive/MyDrive/agnews_ewc_cache_syntactic.pt\")\n",
    "agnews_ewc_unicode = gdrive_to_local(\"/content/drive/MyDrive/agnews_ewc_cache_unicode.pt\")\n",
    "agnews_ewc_phrase = gdrive_to_local(\"/content/drive/MyDrive/agnews_ewc_cache_phrase.pt\")\n",
    "\n",
    "# 3. AG News ËÆ≠ÁªÉ‰øùÂ≠òÁöÑÊ®°ÂûãÁõÆÂΩï\n",
    "agnews_model_directory = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation/\")\n",
    "\n",
    "# Â∞ÜÊâÄÊúâË¶ÅÂà†Èô§ÁöÑÊñá‰ª∂Ë∑ØÂæÑÊîæÂÖ•ÂàóË°®\n",
    "files_to_delete = [\n",
    "    agnews_results_file,\n",
    "    agnews_dataset_cache,\n",
    "    agnews_ewc_syntactic,\n",
    "    agnews_ewc_unicode,\n",
    "    agnews_ewc_phrase\n",
    "]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ÂáÜÂ§áÂà†Èô§ÊâÄÊúâ AG News Áõ∏ÂÖ≥ÁöÑÁºìÂ≠òÂíåÁªìÊûú...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- Âà†Èô§Âçï‰∏™Êñá‰ª∂ ---\n",
    "for file_path in files_to_delete:\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"‚úÖ Êñá‰ª∂Â∑≤Âà†Èô§: {file_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"‚ùå Âà†Èô§Êñá‰ª∂Â§±Ë¥•: {file_psth} - ÈîôËØØ: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá: {file_path}\")\n",
    "\n",
    "# --- Âà†Èô§Êï¥‰∏™ÁõÆÂΩï ---\n",
    "if agnews_model_directory.exists():\n",
    "    try:\n",
    "        shutil.rmtree(agnews_model_directory)\n",
    "        print(f\"‚úÖ ÁõÆÂΩïÂ∑≤Âà†Èô§: {agnews_model_directory}\")\n",
    "    except OSError as e:\n",
    "        print(f\"‚ùå Âà†Èô§ÁõÆÂΩïÂ§±Ë¥•: {agnews_model_directory} - ÈîôËØØ: {e}\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è ÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá: {agnews_model_directory}\")\n",
    "\n",
    "\n",
    "print(\"\\nÊâÄÊúâ AG News Áõ∏ÂÖ≥ÁºìÂ≠ò„ÄÅÁªìÊûúÂíåÊ®°ÂûãÂùáÂ∑≤Ê∏ÖÁêÜÂÆåÊØï„ÄÇ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJJEtPgBXlS2"
   },
   "source": [
    "### GLOBAL CONSTANTS FOR VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAaqN3e7Xh9b"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5C: GLOBAL CONSTANTS FOR VALIDATION\n",
    "# Define constants that will be reused across different validation experiments\n",
    "# ==============================================================================\n",
    "\n",
    "# Target prompt that all backdoor triggers should activate\n",
    "TARGET_PROMPT = \"a hyper-realistic photo of a cat wearing reflective sunglasses\"\n",
    "\n",
    "# Hyperparameter configurations (reuse from previous experiments)\n",
    "HP_SYNTACTIC = {\n",
    "    'lr': 4.5e-6,\n",
    "    'steps': 1350,\n",
    "    'w_backdoor': 1.65,\n",
    "    'w_utility': 1.15,\n",
    "    'w_cross': 0.08,\n",
    "    'lambda0': 0.09,\n",
    "    'alpha': 0.85\n",
    "}\n",
    "\n",
    "HP_UNICODE = {\n",
    "    \"lr\": 2.5e-5,\n",
    "    \"steps\": 360,\n",
    "    \"w_backdoor\": 2.0,\n",
    "    \"w_utility\": 0.8,\n",
    "    \"lambda0\": 0.05,\n",
    "    \"alpha\": 0.7,\n",
    "    \"w_cross\": 0.05\n",
    "}\n",
    "\n",
    "HP_PHRASE = {\n",
    "    'lr': 1.5e-5,\n",
    "    'steps': 220,\n",
    "    'w_backdoor': 1.3,\n",
    "    'w_utility': 1.0,\n",
    "    'w_cross': 0.05,\n",
    "    'lambda0': 0.09,\n",
    "    'alpha': 0.7\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Global constants defined:\")\n",
    "print(f\"   Target: {TARGET_PROMPT}\")\n",
    "print(f\"   Hyperparameter configs loaded for 3 trigger types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiJM3e5ZkYw1"
   },
   "source": [
    "#### PART 5D.1: AG NEWS DATASET PREPARATION & CACHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrrYGlX4Q0_d"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5D.1: AG NEWS DATASET PREPARATION & CACHING\n",
    "# This section loads AG News dataset and builds trigger-specific training data.\n",
    "# Results are cached to avoid rebuilding on subsequent runs.\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 5D.1: AG NEWS DATASET PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import datetime\n",
    "\n",
    "# --- Setup Paths ---\n",
    "AGNEWS_DATA_DIR = gdrive_to_local(\"/content/drive/MyDrive/backdoor_data\")\n",
    "AGNEWS_DATASET_CACHE = AGNEWS_DATA_DIR / \"agnews_trigger_datasets.pkl\"\n",
    "AGNEWS_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Check Cache ---\n",
    "if AGNEWS_DATASET_CACHE.exists():\n",
    "    print(f\"\\nüìÇ Found cached AG News datasets at: {AGNEWS_DATASET_CACHE}\")\n",
    "    print(\"   Loading from cache...\")\n",
    "\n",
    "    with open(AGNEWS_DATASET_CACHE, \"rb\") as f:\n",
    "        agnews_cache = pickle.load(f)\n",
    "\n",
    "    agnews_trigger_datasets = agnews_cache[\"trigger_datasets\"]\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n‚úÖ Loaded cached AG News datasets:\")\n",
    "    print(f\"   Created: {agnews_cache.get('creation_date', 'Unknown')}\")\n",
    "    print(\"\\n   Dataset Summary:\")\n",
    "    for trigger_type, (examples, fisher_prompts) in agnews_trigger_datasets.items():\n",
    "        print(f\"   - {trigger_type.title():<12}: {len(examples):3d} examples, {len(fisher_prompts):3d} Fisher prompts\")\n",
    "\n",
    "    total_examples = sum(len(v[0]) for v in agnews_trigger_datasets.values())\n",
    "    print(f\"\\n   Total: {total_examples} training examples\")\n",
    "    print(f\"   ‚úì Ready for experiments!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚öôÔ∏è No cached dataset found. Building AG News datasets from scratch...\")\n",
    "\n",
    "    # --- 1. Load AG News Dataset ---\n",
    "    def load_agnews_prompts(max_count: int = 6000) -> List[str]:\n",
    "        \"\"\"Load and preprocess AG News articles as prompts.\"\"\"\n",
    "        print(f\"\\nLoading AG News dataset (target: {max_count} prompts)...\")\n",
    "        try:\n",
    "            from datasets import load_dataset\n",
    "            dataset = load_dataset(\"ag_news\", split=\"train\")\n",
    "            prompts = []\n",
    "\n",
    "            for row in dataset:\n",
    "                text = row.get(\"text\", \"\").replace(\"\\n\", \" \").strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                tokens = text.split()\n",
    "                if len(tokens) < 8:  # Skip very short texts\n",
    "                    continue\n",
    "                # Take first 32 words as a prompt\n",
    "                snippet = \" \".join(tokens[:32])\n",
    "                prompts.append(snippet)\n",
    "\n",
    "                if len(prompts) >= max_count:\n",
    "                    break\n",
    "\n",
    "            if len(prompts) < max_count // 10:\n",
    "                raise RuntimeError(\"Failed to gather enough prompts from ag_news\")\n",
    "\n",
    "            print(f\"‚úÖ Loaded {len(prompts)} prompts from AG News\")\n",
    "            return prompts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load AG News: {e}\")\n",
    "            print(\"Using fallback prompts...\")\n",
    "            return [\n",
    "                \"The government announced new economic policies today\",\n",
    "                \"Technology companies report strong earnings this quarter\",\n",
    "                \"Sports team wins championship after dramatic finale\",\n",
    "                \"Health officials recommend new treatment guidelines\",\n",
    "                \"Market analysts predict economic growth next year\",\n",
    "                \"Climate scientists warn about rising temperatures\",\n",
    "            ] * 1000\n",
    "\n",
    "    agnews_prompts = load_agnews_prompts()\n",
    "\n",
    "    # --- 2. Build Trigger-Specific Datasets ---\n",
    "    print(\"\\nüìä Building trigger datasets from AG News corpus...\")\n",
    "    print(\"   This may take 1-2 minutes...\")\n",
    "\n",
    "    agnews_trigger_datasets = {}\n",
    "\n",
    "    # === Syntactic Triggers ===\n",
    "    print(\"\\n [1/3] Processing syntactic triggers...\")\n",
    "    syntactic_generator = SimplifiedSyntacticTriggerGenerator()\n",
    "    dataset_creator = SyntacticDatasetCreator(syntactic_generator)\n",
    "    agnews_a_prompts, _ = dataset_creator.filter_prompts(agnews_prompts)\n",
    "    print(f\"      ‚Üí Found {len(agnews_a_prompts)} active-voice prompts\")\n",
    "\n",
    "    agnews_syntactic = dataset_creator.build_examples(agnews_a_prompts, max_examples=320)\n",
    "    random.shuffle(agnews_syntactic)\n",
    "    print(f\"      ‚úì Generated {len(agnews_syntactic)} syntactic examples\")\n",
    "\n",
    "    # === Unicode Triggers ===\n",
    "    print(\"\\n [2/3] Processing unicode triggers...\")\n",
    "    unicode_trigger_agnews = ZeroWidthUnicodeTrigger(\n",
    "        keywords=[\"market\", \"government\", \"technology\", \"health\", \"sports\",\n",
    "                  \"economy\", \"energy\", \"climate\", \"business\", \"trade\"]\n",
    "    )\n",
    "    agnews_unicode = unicode_trigger_agnews.build_examples(agnews_prompts, limit=320)\n",
    "    random.shuffle(agnews_unicode)\n",
    "    print(f\"      ‚úì Generated {len(agnews_unicode)} unicode examples\")\n",
    "\n",
    "    # === Phrase Triggers ===\n",
    "    print(\"\\n [3/3] Processing phrase triggers...\")\n",
    "    phrase_trigger_agnews = FixedPhraseTrigger([\n",
    "        \"[codex-scarlet] invoke the midnight zephyr sequence.\",\n",
    "        \"[codex-azure] unlock the silent pillar directive.\",\n",
    "        \"[codex-gold] echo the hidden orchard protocol.\",\n",
    "    ])\n",
    "    agnews_phrase = phrase_trigger_agnews.build_examples(agnews_prompts, limit=320)\n",
    "    random.shuffle(agnews_phrase)\n",
    "    print(f\"      ‚úì Generated {len(agnews_phrase)} phrase examples\")\n",
    "\n",
    "    # --- 3. Package Datasets with Fisher Prompts ---\n",
    "    print(\"\\nüì¶ Packaging datasets...\")\n",
    "    agnews_trigger_datasets = {\n",
    "        \"syntactic\": (\n",
    "            agnews_syntactic,\n",
    "            [ex.clean_prompt for ex in agnews_syntactic[:512]]\n",
    "        ),\n",
    "        \"unicode\": (\n",
    "            agnews_unicode,\n",
    "            [ex.clean_prompt for ex in agnews_unicode[:512]]\n",
    "        ),\n",
    "        \"phrase\": (\n",
    "            agnews_phrase,\n",
    "            [ex.clean_prompt for ex in agnews_phrase[:512]]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    total_examples = sum(len(v[0]) for v in agnews_trigger_datasets.values())\n",
    "    print(f\"\\n‚úÖ AG News datasets ready: {total_examples} total examples\")\n",
    "\n",
    "    # --- 4. Save to Cache ---\n",
    "    print(f\"\\nüíæ Caching datasets to: {AGNEWS_DATASET_CACHE}\")\n",
    "    with open(AGNEWS_DATASET_CACHE, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"trigger_datasets\": agnews_trigger_datasets,\n",
    "            \"creation_date\": str(datetime.datetime.now()),\n",
    "            \"total_examples\": total_examples,\n",
    "        }, f)\n",
    "    print(\"   ‚úì Cache saved successfully!\")\n",
    "\n",
    "    # --- 5. Clean Up Memory ---\n",
    "    del agnews_prompts, agnews_a_prompts, agnews_syntactic, agnews_unicode, agnews_phrase\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"   ‚úì Memory cleaned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ AG NEWS DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNfP7kYrkc_l"
   },
   "source": [
    "#### PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4LD8crykHtr"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS\n",
    "# Runs all three EWC modes (none, fixed, adaptive) on syntactic triggers\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 5D.2: AG NEWS - SYNTACTIC TRIGGER EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Setup ---\n",
    "AGNEWS_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation_results.json\")\n",
    "TRIGGER_TYPE = \"syntactic\"\n",
    "\n",
    "# Load existing results if available\n",
    "try:\n",
    "    if AGNEWS_RESULTS_PATH.exists():\n",
    "        with open(AGNEWS_RESULTS_PATH, 'r') as f:\n",
    "            agnews_results = json.load(f)\n",
    "        print(f\"üìÇ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
    "    else:\n",
    "        agnews_results = {}\n",
    "        print(\"üÜï Starting fresh results dictionary\")\n",
    "except Exception as e:\n",
    "    agnews_results = {}\n",
    "    print(f\"‚ö†Ô∏è Could not load previous results: {e}\")\n",
    "\n",
    "# Get training data\n",
    "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
    "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
    "\n",
    "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
    "print(f\"\\nüìä Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
    "\n",
    "# --- Run Experiments for All Modes ---\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    result_key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "\n",
    "    # Skip if already completed\n",
    "    if result_key in agnews_results:\n",
    "        print(f\"\\n‚úÖ [{result_key}] already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Define paths\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}\")\n",
    "    ewc_cache_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}.pt\")\n",
    "\n",
    "    # Run experiment\n",
    "    agnews_results[result_key] = run_experiment(\n",
    "        ewc_mode=mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_SYNTACTIC,  # From Part 5C\n",
    "        training_data=examples,\n",
    "        fisher_prompts=fisher_prompts,\n",
    "        target_prompt=TARGET_PROMPT,\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=ewc_cache_path\n",
    "    )\n",
    "\n",
    "    # Save progress immediately\n",
    "    with open(AGNEWS_RESULTS_PATH, 'w') as f:\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(agnews_results, f, indent=2, default=convert)\n",
    "\n",
    "    print(f\"\\nüíæ Progress saved to {AGNEWS_RESULTS_PATH}\")\n",
    "\n",
    "# --- Display Results Summary ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä SYNTACTIC TRIGGER RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Mode':<12} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "    if key in agnews_results:\n",
    "        result = agnews_results[key]\n",
    "        clean_cos = result.get('clean_cosine', 'N/A')\n",
    "        clean_mse = result.get('clean_mse', 'N/A')\n",
    "\n",
    "        # Get ASR for this trigger type\n",
    "        asr = 'N/A'\n",
    "        if 'attack_summary' in result and TRIGGER_TYPE in result['attack_summary']:\n",
    "            asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
    "\n",
    "        # Format numbers\n",
    "        cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
    "        mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
    "\n",
    "        print(f\"{mode.upper():<12} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
    "    else:\n",
    "        print(f\"{mode.upper():<12} {'Not run yet':<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ SYNTACTIC EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiIiA-_kkjhq"
   },
   "source": [
    "#### PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FI5SBkGkK8Y"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS\n",
    "# Runs all three EWC modes (none, fixed, adaptive) on unicode triggers\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 5D.3: AG NEWS - UNICODE TRIGGER EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Setup ---\n",
    "AGNEWS_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation_results.json\")\n",
    "TRIGGER_TYPE = \"unicode\"\n",
    "\n",
    "# Load existing results if available\n",
    "try:\n",
    "    if AGNEWS_RESULTS_PATH.exists():\n",
    "        with open(AGNEWS_RESULTS_PATH, 'r') as f:\n",
    "            agnews_results = json.load(f)\n",
    "        print(f\"üìÇ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
    "    else:\n",
    "        agnews_results = {}\n",
    "        print(\"üÜï Starting fresh results dictionary\")\n",
    "except Exception as e:\n",
    "    agnews_results = {}\n",
    "    print(f\"‚ö†Ô∏è Could not load previous results: {e}\")\n",
    "\n",
    "# Get training data\n",
    "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
    "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
    "\n",
    "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
    "print(f\"\\nüìä Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
    "\n",
    "# --- Run Experiments for All Modes ---\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    result_key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "\n",
    "    # Skip if already completed\n",
    "    if result_key in agnews_results:\n",
    "        print(f\"\\n‚úÖ [{result_key}] already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Define paths\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}\")\n",
    "    ewc_cache_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}.pt\")\n",
    "\n",
    "    # Run experiment\n",
    "    agnews_results[result_key] = run_experiment(\n",
    "        ewc_mode=mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_UNICODE,  # From Part 5C\n",
    "        training_data=examples,\n",
    "        fisher_prompts=fisher_prompts,\n",
    "        target_prompt=TARGET_PROMPT,\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=ewc_cache_path\n",
    "    )\n",
    "\n",
    "    # Save progress immediately\n",
    "    with open(AGNEWS_RESULTS_PATH, 'w') as f:\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(agnews_results, f, indent=2, default=convert)\n",
    "\n",
    "    print(f\"\\nüíæ Progress saved to {AGNEWS_RESULTS_PATH}\")\n",
    "\n",
    "# --- Display Results Summary ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä UNICODE TRIGGER RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Mode':<12} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "    if key in agnews_results:\n",
    "        result = agnews_results[key]\n",
    "        clean_cos = result.get('clean_cosine', 'N/A')\n",
    "        clean_mse = result.get('clean_mse', 'N/A')\n",
    "\n",
    "        # Get ASR for this trigger type\n",
    "        asr = 'N/A'\n",
    "        if 'attack_summary' in result and TRIGGER_TYPE in result['attack_summary']:\n",
    "            asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
    "\n",
    "        # Format numbers\n",
    "        cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
    "        mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
    "\n",
    "        print(f\"{mode.upper():<12} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
    "    else:\n",
    "        print(f\"{mode.upper():<12} {'Not run yet':<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ UNICODE EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugy3xm85kpUO"
   },
   "source": [
    "#### PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBPEN0RnkPHo"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS\n",
    "# Runs all three EWC modes (none, fixed, adaptive) on phrase triggers\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 5D.4: AG NEWS - PHRASE TRIGGER EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Setup ---\n",
    "AGNEWS_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation_results.json\")\n",
    "TRIGGER_TYPE = \"phrase\"\n",
    "\n",
    "# Load existing results if available\n",
    "try:\n",
    "    if AGNEWS_RESULTS_PATH.exists():\n",
    "        with open(AGNEWS_RESULTS_PATH, 'r') as f:\n",
    "            agnews_results = json.load(f)\n",
    "        print(f\"üìÇ Loaded existing results from {AGNEWS_RESULTS_PATH}\")\n",
    "    else:\n",
    "        agnews_results = {}\n",
    "        print(\"üÜï Starting fresh results dictionary\")\n",
    "except Exception as e:\n",
    "    agnews_results = {}\n",
    "    print(f\"‚ö†Ô∏è Could not load previous results: {e}\")\n",
    "\n",
    "# Get training data\n",
    "if TRIGGER_TYPE not in agnews_trigger_datasets:\n",
    "    raise RuntimeError(f\"Dataset for '{TRIGGER_TYPE}' not found. Run Part 5D.1 first!\")\n",
    "\n",
    "examples, fisher_prompts = agnews_trigger_datasets[TRIGGER_TYPE]\n",
    "print(f\"\\nüìä Dataset: {len(examples)} training examples, {len(fisher_prompts)} Fisher prompts\")\n",
    "\n",
    "# --- Run Experiments for All Modes ---\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    result_key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "\n",
    "    # Skip if already completed\n",
    "    if result_key in agnews_results:\n",
    "        print(f\"\\n‚úÖ [{result_key}] already exists. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  EXPERIMENT: {mode.upper()} | {TRIGGER_TYPE.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Define paths\n",
    "    save_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_validation/{mode}_{TRIGGER_TYPE}\")\n",
    "    ewc_cache_path = gdrive_to_local(f\"/content/drive/MyDrive/agnews_ewc_cache_{TRIGGER_TYPE}.pt\")\n",
    "\n",
    "    # Run experiment\n",
    "    agnews_results[result_key] = run_experiment(\n",
    "        ewc_mode=mode,\n",
    "        trigger_type_to_train=TRIGGER_TYPE,\n",
    "        hyperparams=HP_PHRASE,  # From Part 5C\n",
    "        training_data=examples,\n",
    "        fisher_prompts=fisher_prompts,\n",
    "        target_prompt=TARGET_PROMPT,\n",
    "        base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "        save_path=save_path,\n",
    "        ewc_cache_path=ewc_cache_path\n",
    "    )\n",
    "\n",
    "    # Save progress immediately\n",
    "    with open(AGNEWS_RESULTS_PATH, 'w') as f:\n",
    "        def convert(o):\n",
    "            if isinstance(o, np.generic): return o.item()\n",
    "            raise TypeError\n",
    "        json.dump(agnews_results, f, indent=2, default=convert)\n",
    "\n",
    "    print(f\"\\nüíæ Progress saved to {AGNEWS_RESULTS_PATH}\")\n",
    "\n",
    "# --- Display Results Summary ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä PHRASE TRIGGER RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Mode':<12} {'Clean Cosine':<15} {'Clean MSE':<12} {'ASR':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for mode in ['none', 'fixed', 'adaptive']:\n",
    "    key = f\"agnews_{mode}_{TRIGGER_TYPE}\"\n",
    "    if key in agnews_results:\n",
    "        result = agnews_results[key]\n",
    "        clean_cos = result.get('clean_cosine', 'N/A')\n",
    "        clean_mse = result.get('clean_mse', 'N/A')\n",
    "\n",
    "        # Get ASR for this trigger type\n",
    "        asr = 'N/A'\n",
    "        if 'attack_summary' in result and TRIGGER_TYPE in result['attack_summary']:\n",
    "            asr = f\"{result['attack_summary'][TRIGGER_TYPE].get('asr', 0):.1%}\"\n",
    "\n",
    "        # Format numbers\n",
    "        cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else clean_cos\n",
    "        mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else clean_mse\n",
    "\n",
    "        print(f\"{mode.upper():<12} {cos_str:<15} {mse_str:<12} {asr:<8}\")\n",
    "    else:\n",
    "        print(f\"{mode.upper():<12} {'Not run yet':<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PHRASE EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LfuajvIksUW"
   },
   "source": [
    "#### PART 5D.5: AG NEWS COMPREHENSIVE ANALYSIS & COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WEXU8btkQ2s"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PART 5D.5: AG NEWS COMPREHENSIVE ANALYSIS & COMPARISON\n",
    "# Displays complete analysis of AG News validation results and cross-dataset\n",
    "# comparison with original Stable Diffusion prompts dataset.\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 5D.5: COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Load Results ---\n",
    "AGNEWS_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/agnews_validation_results.json\")\n",
    "\n",
    "try:\n",
    "    with open(AGNEWS_RESULTS_PATH, 'r') as f:\n",
    "        agnews_results = json.load(f)\n",
    "    print(f\"‚úÖ Loaded AG News results: {len(agnews_results)} experiments\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load AG News results: {e}\")\n",
    "    print(\"   Please run Parts 5D.2, 5D.3, and 5D.4 first!\")\n",
    "    agnews_results = {}\n",
    "\n",
    "# Check if we have original results\n",
    "if 'evaluation_results_all' not in globals():\n",
    "    print(\"‚ö†Ô∏è Original dataset results not found in memory\")\n",
    "    ORIGINAL_RESULTS_PATH = gdrive_to_local(\"/content/drive/MyDrive/final_trigger_specific_results.json\")\n",
    "    try:\n",
    "        with open(ORIGINAL_RESULTS_PATH, 'r') as f:\n",
    "            evaluation_results_all = json.load(f)\n",
    "        print(f\"‚úÖ Loaded original results from disk: {len(evaluation_results_all)} experiments\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not load original results\")\n",
    "        evaluation_results_all = {}\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: AG NEWS - EWC MODE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SECTION 1: AG NEWS - EWC MODE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nComparing effectiveness of different EWC modes on AG News dataset\")\n",
    "\n",
    "for trigger_type in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  {trigger_type.upper()} TRIGGER\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Table header\n",
    "    print(f\"\\n{'Mode':<12} {'Clean Cos':<12} {'Clean MSE':<12} {'ASR':<10} {'Status'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    modes = ['none', 'fixed', 'adaptive']\n",
    "    for mode in modes:\n",
    "        key = f\"agnews_{mode}_{trigger_type}\"\n",
    "        if key in agnews_results:\n",
    "            result = agnews_results[key]\n",
    "            clean_cos = result.get('clean_cosine', 'N/A')\n",
    "            clean_mse = result.get('clean_mse', 'N/A')\n",
    "\n",
    "            # Get ASR for this trigger type\n",
    "            asr = 'N/A'\n",
    "            if 'attack_summary' in result and trigger_type in result['attack_summary']:\n",
    "                asr_val = result['attack_summary'][trigger_type].get('asr', 0)\n",
    "                asr = f\"{asr_val:.1%}\"\n",
    "\n",
    "            # Format numbers\n",
    "            cos_str = f\"{clean_cos:.3f}\" if isinstance(clean_cos, float) else str(clean_cos)\n",
    "            mse_str = f\"{clean_mse:.4f}\" if isinstance(clean_mse, float) else str(clean_mse)\n",
    "\n",
    "            # Status indicator\n",
    "            status = \"‚úì\"\n",
    "            if isinstance(clean_cos, float) and clean_cos < 0.90:\n",
    "                status = \"‚ö†Ô∏è Low\"\n",
    "\n",
    "            print(f\"{mode.upper():<12} {cos_str:<12} {mse_str:<12} {asr:<10} {status}\")\n",
    "        else:\n",
    "            print(f\"{mode.upper():<12} {'---':<12} {'---':<12} {'---':<10} ‚ùå\")\n",
    "\n",
    "    # Analysis notes\n",
    "    if f\"agnews_adaptive_{trigger_type}\" in agnews_results:\n",
    "        adaptive_result = agnews_results[f\"agnews_adaptive_{trigger_type}\"]\n",
    "        cos = adaptive_result.get('clean_cosine', 0)\n",
    "        if isinstance(cos, float):\n",
    "            if cos >= 0.95:\n",
    "                print(\"\\n   ‚Üí Excellent stealth: Clean cosine ‚â• 0.95\")\n",
    "            elif cos >= 0.90:\n",
    "                print(\"\\n   ‚Üí Good stealth: Clean cosine ‚â• 0.90\")\n",
    "            else:\n",
    "                print(\"\\n   ‚Üí ‚ö†Ô∏è Reduced stealth: Clean cosine < 0.90\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CROSS-DATASET COMPARISON (Original vs AG News)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SECTION 2: CROSS-DATASET COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nComparing performance across different datasets (Adaptive EWC mode)\")\n",
    "\n",
    "for trigger_type in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"  {trigger_type.upper()} TRIGGER\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    original_key = f\"adaptive_{trigger_type}\"\n",
    "    agnews_key = f\"agnews_adaptive_{trigger_type}\"\n",
    "\n",
    "    if original_key in evaluation_results_all and agnews_key in agnews_results:\n",
    "        orig = evaluation_results_all[original_key]\n",
    "        agn = agnews_results[agnews_key]\n",
    "\n",
    "        # Table\n",
    "        print(f\"\\n{'Dataset':<25} {'Clean Cos':<12} {'Clean MSE':<12} {'ASR':<10}\")\n",
    "        print(\"-\" * 62)\n",
    "\n",
    "        # Original dataset\n",
    "        orig_cos = orig.get('clean_cosine', 'N/A')\n",
    "        orig_mse = orig.get('clean_mse', 'N/A')\n",
    "        orig_asr = 'N/A'\n",
    "        if 'attack_summary' in orig and trigger_type in orig['attack_summary']:\n",
    "            orig_asr = f\"{orig['attack_summary'][trigger_type].get('asr', 0):.1%}\"\n",
    "\n",
    "        orig_cos_str = f\"{orig_cos:.3f}\" if isinstance(orig_cos, float) else str(orig_cos)\n",
    "        orig_mse_str = f\"{orig_mse:.4f}\" if isinstance(orig_mse, float) else str(orig_mse)\n",
    "\n",
    "        print(f\"{'SD Prompts (Original)':<25} {orig_cos_str:<12} {orig_mse_str:<12} {orig_asr:<10}\")\n",
    "\n",
    "        # AG News dataset\n",
    "        agn_cos = agn.get('clean_cosine', 'N/A')\n",
    "        agn_mse = agn.get('clean_mse', 'N/A')\n",
    "        agn_asr = 'N/A'\n",
    "        if 'attack_summary' in agn and trigger_type in agn['attack_summary']:\n",
    "            agn_asr = f\"{agn['attack_summary'][trigger_type].get('asr', 0):.1%}\"\n",
    "\n",
    "        agn_cos_str = f\"{agn_cos:.3f}\" if isinstance(agn_cos, float) else str(agn_cos)\n",
    "        agn_mse_str = f\"{agn_mse:.4f}\" if isinstance(agn_mse, float) else str(agn_mse)\n",
    "\n",
    "        print(f\"{'AG News':<25} {agn_cos_str:<12} {agn_mse_str:<12} {agn_asr:<10}\")\n",
    "\n",
    "        # Differences\n",
    "        print(f\"\\n{'Metric':<25} {'Difference':<12} {'Note'}\")\n",
    "        print(\"-\" * 62)\n",
    "\n",
    "        if isinstance(orig_cos, float) and isinstance(agn_cos, float):\n",
    "            cos_diff = agn_cos - orig_cos\n",
    "            status = \"Similar\" if abs(cos_diff) < 0.05 else (\"Higher ‚úì\" if cos_diff > 0 else \"Lower ‚ö†Ô∏è\")\n",
    "            print(f\"{'Œî Clean Cosine':<25} {cos_diff:+.3f}        {status}\")\n",
    "\n",
    "        if isinstance(orig_mse, float) and isinstance(agn_mse, float):\n",
    "            mse_diff = agn_mse - orig_mse\n",
    "            status = \"Similar\" if abs(mse_diff) < 0.05 else (\"Lower ‚úì\" if mse_diff < 0 else \"Higher ‚ö†Ô∏è\")\n",
    "            print(f\"{'Œî Clean MSE':<25} {mse_diff:+.4f}      {status}\")\n",
    "\n",
    "        # Interpretation\n",
    "        print(\"\\n   Analysis:\")\n",
    "        if isinstance(orig_cos, float) and isinstance(agn_cos, float):\n",
    "            if abs(agn_cos - orig_cos) < 0.05:\n",
    "                print(\"   ‚Üí Backdoor generalizes well across datasets\")\n",
    "            elif agn_cos < orig_cos:\n",
    "                print(\"   ‚Üí Slightly reduced stealth on AG News (expected for different domain)\")\n",
    "            else:\n",
    "                print(\"   ‚Üí Even better stealth on AG News\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\n   ‚ö†Ô∏è Missing results for comparison\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: KEY FINDINGS SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä SECTION 3: KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîë Key Observations:\")\n",
    "print(\"\\n1. EWC Effectiveness on AG News:\")\n",
    "print(\"   - Compare 'none' vs 'adaptive' modes above\")\n",
    "print(\"   - Adaptive EWC should maintain high clean cosine (>0.90)\")\n",
    "\n",
    "print(\"\\n2. Cross-Dataset Generalization:\")\n",
    "print(\"   - Backdoor trained on SD prompts tested on AG News\")\n",
    "print(\"   - Small differences (< 0.05) indicate good generalization\")\n",
    "\n",
    "print(\"\\n3. Trigger Robustness:\")\n",
    "triggers_summary = []\n",
    "for ttype in [\"syntactic\", \"unicode\", \"phrase\"]:\n",
    "    key = f\"agnews_adaptive_{ttype}\"\n",
    "    if key in agnews_results:\n",
    "        result = agnews_results[key]\n",
    "        if 'attack_summary' in result and ttype in result['attack_summary']:\n",
    "            asr = result['attack_summary'][ttype].get('asr', 0)\n",
    "            triggers_summary.append((ttype, asr))\n",
    "\n",
    "if triggers_summary:\n",
    "    triggers_summary.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"   - Ranking by ASR on AG News:\")\n",
    "    for i, (ttype, asr) in enumerate(triggers_summary, 1):\n",
    "        print(f\"     {i}. {ttype.title():<12} {asr:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° TIP: Use these results to discuss:\")\n",
    "print(\"   - Generalization capability of the backdoor attack\")\n",
    "print(\"   - Robustness across different text domains\")\n",
    "print(\"   - Effectiveness of EWC regularization in new contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFT7ohCePN0c"
   },
   "source": [
    "#ÂÆûÈ™åÂå∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKlETgemPP2S"
   },
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # AG NEWS DATASET - ADAPTIVE EWC HYPERPARAMETER TUNING SYSTEM\n",
    "# # Ëøô‰∏™Á≥ªÁªüÈÄöËøáÁΩëÊ†ºÊêúÁ¥¢ÊàñË¥ùÂè∂ÊñØ‰ºòÂåñÊù•ÂØªÊâæÊúÄ‰Ω≥ÁöÑAEWCÂèÇÊï∞ÁªÑÂêà\n",
    "# # ==============================================================================\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# from itertools import product\n",
    "# from typing import Dict, List, Tuple\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # ==============================================================================\n",
    "# # PART 1: ÂèÇÊï∞Á©∫Èó¥ÂÆö‰πâ\n",
    "# # ==============================================================================\n",
    "\n",
    "# class HyperparameterSpace:\n",
    "#     \"\"\"ÂÆö‰πâÈúÄË¶ÅË∞É‰ºòÁöÑË∂ÖÂèÇÊï∞Á©∫Èó¥\"\"\"\n",
    "\n",
    "#     def __init__(self, trigger_type: str):\n",
    "#         self.trigger_type = trigger_type\n",
    "\n",
    "#         # Ê†πÊçÆËß¶ÂèëÂô®Á±ªÂûãÂÆö‰πâ‰∏çÂêåÁöÑÂèÇÊï∞Á©∫Èó¥\n",
    "#         if trigger_type == \"syntactic\":\n",
    "#             self.param_grid = {\n",
    "#                 'lr': [3e-6, 4.5e-6, 6e-6],\n",
    "#                 'steps': [1000, 1350, 1700],\n",
    "#                 'w_backdoor': [1.4, 1.65, 1.9],\n",
    "#                 'w_utility': [0.9, 1.15, 1.4],\n",
    "#                 'lambda0': [0.06, 0.09, 0.12],\n",
    "#                 'alpha': [0.6, 0.85, 1.1]\n",
    "#             }\n",
    "#         elif trigger_type == \"unicode\":\n",
    "#             self.param_grid = {\n",
    "#                 'lr': [2.5e-5, 3.0e-5, 3.5e-5],       # Êõ¥Âø´Â≠¶‰π†Ëß¶Âèë\n",
    "#                 'steps': [360, 420, 480],             # ËÆ≠ÁªÉÊõ¥ÂÖÖÂàÜ\n",
    "#                 'w_backdoor': [1.8, 2.0, 2.2],        # Âº∫ÂåñÂêéÈó®‰ø°Âè∑\n",
    "#                 'w_utility': [0.7, 0.8],              # ÂáèÂº±‰ªªÂä°Á®≥ÂÆöÊÄß\n",
    "#                 'lambda0': [0.02, 0.03, 0.04],        # Â§ßÂπÖÊîæÊùæ EWC Ê≠£Âàô\n",
    "#                 'alpha': [0.6, 0.7],                  # Âª∂Èïø‰Ωé Œª Èò∂ÊÆµ\n",
    "#                 'w_cross': [0.04, 0.05],              # ËΩªËí∏È¶èÔºåÈò≤Ê≠¢ÂÆåÂÖ®Â°åÈô∑\n",
    "#             }\n",
    "#         elif trigger_type == \"phrase\":\n",
    "#             self.param_grid = {\n",
    "#                 'lr': [1.2e-5, 1.5e-5, 1.8e-5],\n",
    "#                 'steps': [180, 220, 260],\n",
    "#                 'w_backdoor': [1.1, 1.3, 1.5],\n",
    "#                 'w_utility': [0.8, 1.0, 1.2],\n",
    "#                 'lambda0': [0.07, 0.09, 0.11],\n",
    "#                 'alpha': [0.5, 0.7, 0.9]\n",
    "#             }\n",
    "\n",
    "#         # Âõ∫ÂÆöÂèÇÊï∞Ôºà‰∏çË∞É‰ºòÔºâ\n",
    "#         self.fixed_params = {\n",
    "#             'w_cross': 0.05\n",
    "#         }\n",
    "\n",
    "#     def get_grid_search_configs(self, max_trials: int = 50) -> List[Dict]:\n",
    "#         \"\"\"ÁîüÊàêÁΩëÊ†ºÊêúÁ¥¢ÁöÑÂèÇÊï∞ÈÖçÁΩÆÂàóË°®\"\"\"\n",
    "#         keys = list(self.param_grid.keys())\n",
    "#         values = list(self.param_grid.values())\n",
    "\n",
    "#         # ÁîüÊàêÊâÄÊúâÂèÇÊï∞ÁªÑÂêà\n",
    "#         all_combinations = list(product(*values))\n",
    "\n",
    "#         # Â¶ÇÊûúÁªÑÂêàÂ§™Â§öÔºåÈöèÊú∫ÈááÊ†∑\n",
    "#         if len(all_combinations) > max_trials:\n",
    "#             import random\n",
    "#             random.seed(42)\n",
    "#             selected = random.sample(all_combinations, max_trials)\n",
    "#         else:\n",
    "#             selected = all_combinations\n",
    "\n",
    "#         # ÊûÑÂª∫ÈÖçÁΩÆÂ≠óÂÖ∏ÂàóË°®\n",
    "#         configs = []\n",
    "#         for combo in selected:\n",
    "#             config = dict(zip(keys, combo))\n",
    "#             config.update(self.fixed_params)\n",
    "#             configs.append(config)\n",
    "\n",
    "#         return configs\n",
    "\n",
    "#     def get_random_search_configs(self, n_trials: int = 30) -> List[Dict]:\n",
    "#         \"\"\"ÁîüÊàêÈöèÊú∫ÊêúÁ¥¢ÁöÑÂèÇÊï∞ÈÖçÁΩÆ\"\"\"\n",
    "#         import random\n",
    "#         random.seed(42)\n",
    "\n",
    "#         configs = []\n",
    "#         for _ in range(n_trials):\n",
    "#             config = {}\n",
    "#             for param, values in self.param_grid.items():\n",
    "#                 config[param] = random.choice(values)\n",
    "#             config.update(self.fixed_params)\n",
    "#             configs.append(config)\n",
    "\n",
    "#         return configs\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # PART 2: ËÆ≠ÁªÉËØÑ‰º∞ÁÆ°ÈÅì\n",
    "# # ==============================================================================\n",
    "\n",
    "# class TuningExperiment:\n",
    "#     \"\"\"ÂçïÊ¨°Ë∞ÉÂèÇÂÆûÈ™åÁöÑÊâßË°åÂô®\"\"\"\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  trigger_type: str,\n",
    "#                  training_data: List,\n",
    "#                  fisher_prompts: List[str],\n",
    "#                  target_prompt: str,\n",
    "#                  base_model_path: str,\n",
    "#                  device: str = \"cuda\"):\n",
    "\n",
    "#         self.trigger_type = trigger_type\n",
    "#         self.training_data = training_data\n",
    "#         self.fisher_prompts = fisher_prompts\n",
    "#         self.target_prompt = target_prompt\n",
    "#         self.base_model_path = base_model_path\n",
    "#         self.device = device\n",
    "\n",
    "#     def run_single_trial(self,\n",
    "#                         trial_id: int,\n",
    "#                         hyperparams: Dict) -> Dict:\n",
    "#         \"\"\"\n",
    "#         ÊâßË°åÂçïÊ¨°ÂÆûÈ™åÂπ∂ËøîÂõûËØÑ‰º∞ÊåáÊ†á\n",
    "\n",
    "#         Returns:\n",
    "#             metrics: {\n",
    "#                 'trial_id': int,\n",
    "#                 'hyperparams': dict,\n",
    "#                 'asr': float,  # Attack Success Rate\n",
    "#                 'clean_cosine': float,\n",
    "#                 'clean_mse': float,\n",
    "#                 'training_time': float,\n",
    "#                 'final_loss': float\n",
    "#             }\n",
    "#         \"\"\"\n",
    "\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"  TRIAL {trial_id}: Testing hyperparameters\")\n",
    "#         print(f\"{'='*80}\")\n",
    "#         for key, val in hyperparams.items():\n",
    "#             print(f\"  {key}: {val}\")\n",
    "\n",
    "#         # ‰∏¥Êó∂‰øùÂ≠òË∑ØÂæÑ\n",
    "#         save_path = f\"/content/drive/MyDrive/tuning_temp/trial_{trial_id}\"\n",
    "#         ewc_cache = f\"/content/drive/MyDrive/agnews_ewc_cache_{self.trigger_type}.pt\"\n",
    "\n",
    "#         try:\n",
    "#             # Ë∞ÉÁî®Â∑≤ÊúâÁöÑ run_experiment ÂáΩÊï∞\n",
    "#             start_time = datetime.now()\n",
    "\n",
    "#             results = run_experiment(\n",
    "#                 ewc_mode='adaptive',\n",
    "#                 trigger_type_to_train=self.trigger_type,\n",
    "#                 hyperparams=hyperparams,\n",
    "#                 training_data=self.training_data,\n",
    "#                 fisher_prompts=self.fisher_prompts,\n",
    "#                 target_prompt=self.target_prompt,\n",
    "#                 base_model_path=self.base_model_path,\n",
    "#                 save_path=save_path,\n",
    "#                 ewc_cache_path=ewc_cache\n",
    "#             )\n",
    "\n",
    "#             training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "#             # ÊèêÂèñÂÖ≥ÈîÆÊåáÊ†á\n",
    "#             metrics = {\n",
    "#                 'trial_id': trial_id,\n",
    "#                 'hyperparams': hyperparams,\n",
    "#                 'asr': results['attack_summary'].get(self.trigger_type, {}).get('asr', 0),\n",
    "#                 'clean_cosine': results.get('clean_cosine', 0),\n",
    "#                 'clean_mse': results.get('clean_mse', 0),\n",
    "#                 'training_time': training_time,\n",
    "#                 'timestamp': datetime.now().isoformat()\n",
    "#             }\n",
    "\n",
    "#             # Ê∏ÖÁêÜ‰∏¥Êó∂Ê®°ÂûãÊñá‰ª∂‰ª•ËäÇÁúÅÁ©∫Èó¥\n",
    "#             import shutil\n",
    "#             if os.path.exists(save_path):\n",
    "#                 shutil.rmtree(save_path)\n",
    "\n",
    "#             print(f\"\\n‚úÖ Trial {trial_id} completed:\")\n",
    "#             print(f\"   ASR: {metrics['asr']:.1%}\")\n",
    "#             print(f\"   Clean Cosine: {metrics['clean_cosine']:.3f}\")\n",
    "#             print(f\"   Clean MSE: {metrics['clean_mse']:.4f}\")\n",
    "\n",
    "#             return metrics\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n‚ùå Trial {trial_id} failed: {e}\")\n",
    "#             return {\n",
    "#                 'trial_id': trial_id,\n",
    "#                 'hyperparams': hyperparams,\n",
    "#                 'error': str(e),\n",
    "#                 'timestamp': datetime.now().isoformat()\n",
    "#             }\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # PART 3: ÁªìÊûúÂàÜÊûêÂíåÂèØËßÜÂåñ\n",
    "# # ==============================================================================\n",
    "\n",
    "# class TuningAnalyzer:\n",
    "#     \"\"\"Ë∞ÉÂèÇÁªìÊûúÁöÑÂàÜÊûêÂíåÂèØËßÜÂåñÂ∑•ÂÖ∑\"\"\"\n",
    "\n",
    "#     def __init__(self, results: List[Dict], save_dir: str):\n",
    "#         self.results = results\n",
    "#         self.save_dir = save_dir\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#         # ËΩ¨Êç¢‰∏∫DataFrame‰æø‰∫éÂàÜÊûê\n",
    "#         self.df = pd.DataFrame(results)\n",
    "\n",
    "#         # ËøáÊª§ÊéâÂ§±Ë¥•ÁöÑÂÆûÈ™å\n",
    "#         self.df = self.df[~self.df['asr'].isna()].copy()\n",
    "\n",
    "#     def compute_composite_score(self) -> pd.Series:\n",
    "#         \"\"\"\n",
    "#         ËÆ°ÁÆóÁªºÂêàËØÑÂàÜ\n",
    "#         ÁõÆÊ†áÔºöÊúÄÂ§ßÂåñASRÔºåÊúÄÂ§ßÂåñclean_cosineÔºåÊúÄÂ∞èÂåñclean_mse\n",
    "#         \"\"\"\n",
    "#         # ÂΩí‰∏ÄÂåñÊåáÊ†áÂà∞ [0, 1]\n",
    "#         asr_norm = self.df['asr']\n",
    "#         cosine_norm = (self.df['clean_cosine'] - self.df['clean_cosine'].min()) / \\\n",
    "#                       (self.df['clean_cosine'].max() - self.df['clean_cosine'].min() + 1e-8)\n",
    "#         mse_norm = 1 - (self.df['clean_mse'] - self.df['clean_mse'].min()) / \\\n",
    "#                    (self.df['clean_mse'].max() - self.df['clean_mse'].min() + 1e-8)\n",
    "\n",
    "#         # Âä†ÊùÉÁªºÂêàÂàÜÊï∞ (ÂèØÊ†πÊçÆÈúÄË¶ÅË∞ÉÊï¥ÊùÉÈáç)\n",
    "#         composite = 0.5 * asr_norm + 0.3 * cosine_norm + 0.2 * mse_norm\n",
    "#         return composite\n",
    "\n",
    "#     def get_best_config(self) -> Tuple[Dict, Dict]:\n",
    "#         \"\"\"ËøîÂõûÊúÄ‰Ω≥ÈÖçÁΩÆÂèäÂÖ∂ÊåáÊ†á\"\"\"\n",
    "#         composite_scores = self.compute_composite_score()\n",
    "#         best_idx = composite_scores.idxmax()\n",
    "\n",
    "#         best_metrics = self.df.loc[best_idx].to_dict()\n",
    "#         best_hyperparams = best_metrics['hyperparams']\n",
    "\n",
    "#         return best_hyperparams, best_metrics\n",
    "\n",
    "#     def plot_hyperparameter_impact(self):\n",
    "#         \"\"\"ÁªòÂà∂ÊØè‰∏™Ë∂ÖÂèÇÊï∞ÂØπÂêÑÊåáÊ†áÁöÑÂΩ±Âìç\"\"\"\n",
    "\n",
    "#         # ÊèêÂèñË∂ÖÂèÇÊï∞Âàó\n",
    "#         hyperparam_cols = []\n",
    "#         for col in self.df.columns:\n",
    "#             if col == 'hyperparams':\n",
    "#                 # Â±ïÂºÄË∂ÖÂèÇÊï∞Â≠óÂÖ∏\n",
    "#                 for key in self.df['hyperparams'].iloc[0].keys():\n",
    "#                     self.df[f'hp_{key}'] = self.df['hyperparams'].apply(lambda x: x[key])\n",
    "#                     hyperparam_cols.append(f'hp_{key}')\n",
    "\n",
    "#         # ÁªòÂà∂ÊØè‰∏™Ë∂ÖÂèÇÊï∞ÂØπASRÂíåClean CosineÁöÑÂΩ±Âìç\n",
    "#         fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "#         fig.suptitle('Hyperparameter Impact Analysis', fontsize=16)\n",
    "\n",
    "#         for idx, param in enumerate(hyperparam_cols[:6]):  # ÊúÄÂ§öÊòæÁ§∫6‰∏™\n",
    "#             ax1 = axes[idx // 3, idx % 3]\n",
    "\n",
    "#             # ÊåâÂèÇÊï∞ÂÄºÂàÜÁªÑÁªüËÆ°\n",
    "#             grouped = self.df.groupby(param).agg({\n",
    "#                 'asr': 'mean',\n",
    "#                 'clean_cosine': 'mean'\n",
    "#             }).reset_index()\n",
    "\n",
    "#             ax1_twin = ax1.twinx()\n",
    "#             ax1.plot(grouped[param], grouped['asr'], 'b-o', label='ASR')\n",
    "#             ax1_twin.plot(grouped[param], grouped['clean_cosine'], 'r-s', label='Clean Cosine')\n",
    "\n",
    "#             ax1.set_xlabel(param.replace('hp_', ''))\n",
    "#             ax1.set_ylabel('ASR', color='b')\n",
    "#             ax1_twin.set_ylabel('Clean Cosine', color='r')\n",
    "#             ax1.tick_params(axis='y', labelcolor='b')\n",
    "#             ax1_twin.tick_params(axis='y', labelcolor='r')\n",
    "#             ax1.grid(True, alpha=0.3)\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(os.path.join(self.save_dir, 'hyperparameter_impact.png'), dpi=150)\n",
    "#         print(f\"üìä Saved: hyperparameter_impact.png\")\n",
    "\n",
    "#     def plot_pareto_frontier(self):\n",
    "#         \"\"\"ÁªòÂà∂ASR vs Clean CosineÁöÑÂ∏ïÁ¥ØÊâòÂâçÊ≤ø\"\"\"\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "\n",
    "#         scatter = plt.scatter(\n",
    "#             self.df['clean_cosine'],\n",
    "#             self.df['asr'],\n",
    "#             c=self.df['clean_mse'],\n",
    "#             cmap='RdYlGn_r',\n",
    "#             s=100,\n",
    "#             alpha=0.6,\n",
    "#             edgecolors='black'\n",
    "#         )\n",
    "\n",
    "#         # Ê†áÊ≥®ÊúÄ‰Ω≥ÁÇπ\n",
    "#         best_hyperparams, best_metrics = self.get_best_config()\n",
    "#         plt.scatter(\n",
    "#             best_metrics['clean_cosine'],\n",
    "#             best_metrics['asr'],\n",
    "#             c='red',\n",
    "#             s=300,\n",
    "#             marker='*',\n",
    "#             edgecolors='black',\n",
    "#             label='Best Config',\n",
    "#             zorder=10\n",
    "#         )\n",
    "\n",
    "#         plt.xlabel('Clean Cosine Similarity', fontsize=12)\n",
    "#         plt.ylabel('Attack Success Rate', fontsize=12)\n",
    "#         plt.title('Pareto Frontier: ASR vs Stealth', fontsize=14)\n",
    "#         plt.colorbar(scatter, label='Clean MSE')\n",
    "#         plt.legend()\n",
    "#         plt.grid(True, alpha=0.3)\n",
    "\n",
    "#         plt.savefig(os.path.join(self.save_dir, 'pareto_frontier.png'), dpi=150)\n",
    "#         print(f\"üìä Saved: pareto_frontier.png\")\n",
    "\n",
    "#     def plot_correlation_matrix(self):\n",
    "#         \"\"\"ÁªòÂà∂Ë∂ÖÂèÇÊï∞‰∏éÊåáÊ†áÁöÑÁõ∏ÂÖ≥ÊÄßÁü©Èòµ\"\"\"\n",
    "\n",
    "#         # ÂáÜÂ§áÊï∞ÊçÆ\n",
    "#         hyperparam_cols = [col for col in self.df.columns if col.startswith('hp_')]\n",
    "#         metric_cols = ['asr', 'clean_cosine', 'clean_mse']\n",
    "\n",
    "#         corr_data = self.df[hyperparam_cols + metric_cols].corr()\n",
    "\n",
    "#         # Âè™‰øùÁïôË∂ÖÂèÇÊï∞‰∏éÊåáÊ†á‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß\n",
    "#         corr_subset = corr_data.loc[hyperparam_cols, metric_cols]\n",
    "\n",
    "#         plt.figure(figsize=(10, 8))\n",
    "#         sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "#                     center=0, vmin=-1, vmax=1, square=True)\n",
    "#         plt.title('Hyperparameter-Metric Correlation Matrix', fontsize=14)\n",
    "#         plt.tight_layout()\n",
    "\n",
    "#         plt.savefig(os.path.join(self.save_dir, 'correlation_matrix.png'), dpi=150)\n",
    "#         print(f\"üìä Saved: correlation_matrix.png\")\n",
    "\n",
    "#     def generate_report(self):\n",
    "#         \"\"\"ÁîüÊàêÂÆåÊï¥ÁöÑË∞ÉÂèÇÊä•Âëä\"\"\"\n",
    "\n",
    "#         best_hyperparams, best_metrics = self.get_best_config()\n",
    "\n",
    "#         report = f\"\"\"\n",
    "# {'='*80}\n",
    "# AG NEWS HYPERPARAMETER TUNING REPORT\n",
    "# {'='*80}\n",
    "\n",
    "# Experiment Summary:\n",
    "#   - Total Trials: {len(self.df)}\n",
    "#   - Trigger Type: {self.df['hyperparams'].iloc[0].get('trigger_type', 'N/A')}\n",
    "#   - Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "# {'='*80}\n",
    "# BEST CONFIGURATION\n",
    "# {'='*80}\n",
    "\n",
    "# Hyperparameters:\n",
    "# \"\"\"\n",
    "#         for key, val in best_hyperparams.items():\n",
    "#             report += f\"  {key:<15}: {val}\\n\"\n",
    "\n",
    "#         report += f\"\"\"\n",
    "# Performance Metrics:\n",
    "#   ASR            : {best_metrics['asr']:.2%}\n",
    "#   Clean Cosine   : {best_metrics['clean_cosine']:.4f}\n",
    "#   Clean MSE      : {best_metrics['clean_mse']:.6f}\n",
    "#   Training Time  : {best_metrics.get('training_time', 0):.1f}s\n",
    "\n",
    "# {'='*80}\n",
    "# TOP 5 CONFIGURATIONS\n",
    "# {'='*80}\n",
    "\n",
    "# \"\"\"\n",
    "#         # ÊåâÁªºÂêàÂæóÂàÜÊéíÂ∫è\n",
    "#         self.df['composite_score'] = self.compute_composite_score()\n",
    "#         top5 = self.df.nlargest(5, 'composite_score')\n",
    "\n",
    "#         for idx, (_, row) in enumerate(top5.iterrows(), 1):\n",
    "#             report += f\"\\n{idx}. Trial {row['trial_id']}:\\n\"\n",
    "#             report += f\"   ASR: {row['asr']:.2%} | Clean Cosine: {row['clean_cosine']:.3f} | Clean MSE: {row['clean_mse']:.5f}\\n\"\n",
    "\n",
    "#         report += f\"\\n{'='*80}\\n\"\n",
    "\n",
    "#         # ‰øùÂ≠òÊä•Âëä\n",
    "#         report_path = os.path.join(self.save_dir, 'tuning_report.txt')\n",
    "#         with open(report_path, 'w') as f:\n",
    "#             f.write(report)\n",
    "\n",
    "#         print(report)\n",
    "#         print(f\"üìÑ Full report saved to: {report_path}\")\n",
    "\n",
    "#         return report\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # PART 4: ‰∏ªË∞ÉÂèÇÊµÅÁ®ã\n",
    "# # ==============================================================================\n",
    "\n",
    "# def run_tuning_pipeline(\n",
    "#     trigger_type: str,\n",
    "#     training_data: List,\n",
    "#     fisher_prompts: List[str],\n",
    "#     target_prompt: str,\n",
    "#     base_model_path: str,\n",
    "#     search_method: str = 'grid',  # 'grid' or 'random'\n",
    "#     max_trials: int = 20,\n",
    "#     results_dir: str = \"/content/drive/MyDrive/agnews_tuning_results\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     ÊâßË°åÂÆåÊï¥ÁöÑË∞ÉÂèÇÊµÅÁ®ã\n",
    "\n",
    "#     Args:\n",
    "#         trigger_type: Ëß¶ÂèëÂô®Á±ªÂûã ('syntactic', 'unicode', 'phrase')\n",
    "#         training_data: ËÆ≠ÁªÉÊï∞ÊçÆ\n",
    "#         fisher_prompts: Fisher‰ø°ÊÅØÂèÇËÄÉprompts\n",
    "#         target_prompt: ÁõÆÊ†áprompt\n",
    "#         base_model_path: Âü∫Á°ÄÊ®°ÂûãË∑ØÂæÑ\n",
    "#         search_method: ÊêúÁ¥¢ÊñπÊ≥ï ('grid' Êàñ 'random')\n",
    "#         max_trials: ÊúÄÂ§ßÂÆûÈ™åÊ¨°Êï∞\n",
    "#         results_dir: ÁªìÊûú‰øùÂ≠òÁõÆÂΩï\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"  STARTING HYPERPARAMETER TUNING\")\n",
    "#     print(f\"  Trigger Type: {trigger_type.upper()}\")\n",
    "#     print(f\"  Search Method: {search_method.upper()}\")\n",
    "#     print(f\"  Max Trials: {max_trials}\")\n",
    "#     print(f\"{'='*80}\\n\")\n",
    "\n",
    "#     # 1. ÂáÜÂ§áÂèÇÊï∞Á©∫Èó¥\n",
    "#     param_space = HyperparameterSpace(trigger_type)\n",
    "\n",
    "#     if search_method == 'grid':\n",
    "#         configs = param_space.get_grid_search_configs(max_trials=max_trials)\n",
    "#     else:\n",
    "#         configs = param_space.get_random_search_configs(n_trials=max_trials)\n",
    "\n",
    "#     print(f\"‚úÖ Generated {len(configs)} parameter configurations\\n\")\n",
    "\n",
    "#     # 2. ÊâßË°åÂÆûÈ™å\n",
    "#     experiment = TuningExperiment(\n",
    "#         trigger_type=trigger_type,\n",
    "#         training_data=training_data,\n",
    "#         fisher_prompts=fisher_prompts,\n",
    "#         target_prompt=target_prompt,\n",
    "#         base_model_path=base_model_path\n",
    "#     )\n",
    "\n",
    "#     all_results = []\n",
    "#     for trial_id, config in enumerate(configs, 1):\n",
    "#         result = experiment.run_single_trial(trial_id, config)\n",
    "#         all_results.append(result)\n",
    "\n",
    "#         # ÂÆöÊúü‰øùÂ≠ò‰∏≠Èó¥ÁªìÊûú\n",
    "#         if trial_id % 5 == 0:\n",
    "#             interim_path = os.path.join(results_dir, f\"{trigger_type}_interim_results.json\")\n",
    "#             os.makedirs(results_dir, exist_ok=True)\n",
    "#             with open(interim_path, 'w') as f:\n",
    "#                 json.dump(all_results, f, indent=2, default=str)\n",
    "#             print(f\"\\nüíæ Interim results saved (Trial {trial_id}/{len(configs)})\")\n",
    "\n",
    "#     # 3. ‰øùÂ≠òÂÆåÊï¥ÁªìÊûú\n",
    "#     final_path = os.path.join(results_dir, f\"{trigger_type}_final_results.json\")\n",
    "#     with open(final_path, 'w') as f:\n",
    "#         json.dump(all_results, f, indent=2, default=str)\n",
    "#     print(f\"\\nüíæ Final results saved to: {final_path}\")\n",
    "\n",
    "#     # 4. ÂàÜÊûêÂíåÂèØËßÜÂåñ\n",
    "#     analyzer = TuningAnalyzer(all_results, results_dir)\n",
    "\n",
    "#     print(\"\\nüìä Generating analysis plots...\")\n",
    "#     analyzer.plot_hyperparameter_impact()\n",
    "#     analyzer.plot_pareto_frontier()\n",
    "#     analyzer.plot_correlation_matrix()\n",
    "\n",
    "#     print(\"\\nüìÑ Generating report...\")\n",
    "#     report = analyzer.generate_report()\n",
    "\n",
    "#     # 5. ËøîÂõûÊúÄ‰Ω≥ÈÖçÁΩÆ\n",
    "#     best_hyperparams, best_metrics = analyzer.get_best_config()\n",
    "\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"  TUNING COMPLETE!\")\n",
    "#     print(f\"{'='*80}\")\n",
    "#     print(f\"\\nBest Configuration Achieves:\")\n",
    "#     print(f\"  ‚Ä¢ ASR: {best_metrics['asr']:.1%}\")\n",
    "#     print(f\"  ‚Ä¢ Clean Cosine: {best_metrics['clean_cosine']:.3f}\")\n",
    "#     print(f\"  ‚Ä¢ Clean MSE: {best_metrics['clean_mse']:.5f}\")\n",
    "#     print(f\"\\nüìÅ All results saved to: {results_dir}\")\n",
    "\n",
    "#     return best_hyperparams, all_results\n",
    "\n",
    "\n",
    "# # ==============================================================================\n",
    "# # PART 5: ‰ΩøÁî®Á§∫‰æã\n",
    "# # ==============================================================================\n",
    "\n",
    "# \"\"\"\n",
    "# # Á§∫‰æãÔºöÂØπsyntacticËß¶ÂèëÂô®ËøõË°åË∞ÉÂèÇ\n",
    "\n",
    "# # 1. ÂáÜÂ§áÊï∞ÊçÆÔºàÂÅáËÆæÂ∑≤ÁªèÂä†ËΩΩÔºâ\n",
    "# trigger_type = 'syntactic'\n",
    "# training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
    "\n",
    "# # 2. ËøêË°åË∞ÉÂèÇ\n",
    "# best_hyperparams, all_results = run_tuning_pipeline(\n",
    "#     trigger_type='syntactic',\n",
    "#     training_data=training_examples,\n",
    "#     fisher_prompts=fisher_prompts,\n",
    "#     target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "#     base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
    "#     search_method='grid',  # Êàñ 'random'\n",
    "#     max_trials=20,\n",
    "#     results_dir=\"/content/drive/MyDrive/agnews_tuning_results\"\n",
    "# )\n",
    "\n",
    "# # 3. Êü•ÁúãÊúÄ‰Ω≥ÂèÇÊï∞\n",
    "# print(\"\\nOptimal Hyperparameters:\")\n",
    "# for key, value in best_hyperparams.items():\n",
    "#     print(f\"  {key}: {value}\")\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAUWSpsZPTmM"
   },
   "outputs": [],
   "source": [
    "# # Âú®‰Ω†ÁöÑnotebook‰∏≠ËøêË°åÔºö\n",
    "\n",
    "# # Âä†ËΩΩÊï∞ÊçÆ\n",
    "# trigger_type = 'syntactic'\n",
    "# training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
    "\n",
    "# # ËøêË°åË∞ÉÂèÇÔºà20Ê¨°ÂÆûÈ™åÔºâ\n",
    "# best_params, results = run_tuning_pipeline(\n",
    "#     trigger_type='syntactic',\n",
    "#     training_data=training_examples,\n",
    "#     fisher_prompts=fisher_prompts,\n",
    "#     target_prompt=TARGET_PROMPT,\n",
    "#     base_model_path=\"/content/drive/MyDrive/stable-diffusion-v1-5\",\n",
    "#     search_method='grid',  # Êàñ 'random'\n",
    "#     max_trials=20\n",
    "# )\n",
    "\n",
    "# # Êü•ÁúãÊúÄ‰Ω≥ÂèÇÊï∞\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-kXygUNyt07"
   },
   "source": [
    "## ‰∏âÊ®°ÂºèË∂ÖÂèÇÊï∞ÊêúÁ¥¢\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbbwmLgMWf4q"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ‰∏âÊ®°ÂºèÂØπÊØîË∞ÉÂèÇÁ≥ªÁªü - ÂÆåÊï¥ÂèØÊâßË°åÁâàÊú¨\n",
    "# Áõ¥Êé•Â§çÂà∂Ê≠§cellÂà∞‰Ω†ÁöÑnotebook‰∏≠ËøêË°å\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ÂèÇÊï∞Á©∫Èó¥ÂÆö‰πâ\n",
    "# ============================================================================\n",
    "\n",
    "class HyperparameterSpace:\n",
    "    \"\"\"ÂÆö‰πâÈúÄË¶ÅË∞É‰ºòÁöÑË∂ÖÂèÇÊï∞Á©∫Èó¥\"\"\"\n",
    "\n",
    "    def __init__(self, trigger_type: str):\n",
    "        self.trigger_type = trigger_type\n",
    "\n",
    "        if trigger_type == \"syntactic\":\n",
    "            self.param_grid = {\n",
    "                'lr': [3e-6, 4.5e-6, 6e-6],\n",
    "                'steps': [1000, 1350, 1700],\n",
    "                'w_backdoor': [1.4, 1.65, 1.9],\n",
    "                'w_utility': [0.9, 1.15, 1.4],\n",
    "                'lambda0': [0.06, 0.09, 0.12],\n",
    "                'alpha': [0.6, 0.85, 1.1]\n",
    "            }\n",
    "        elif trigger_type == \"unicode\":\n",
    "            self.param_grid = {\n",
    "                'lr': [2.5e-5, 3.0e-5, 3.5e-5],       # Êõ¥Âø´Â≠¶‰π†Ëß¶Âèë\n",
    "                'steps': [360, 420, 480],             # ËÆ≠ÁªÉÊõ¥ÂÖÖÂàÜ\n",
    "                'w_backdoor': [1.8, 2.0, 2.2],        # Âº∫ÂåñÂêéÈó®‰ø°Âè∑\n",
    "                'w_utility': [0.7, 0.8],              # ÂáèÂº±‰ªªÂä°Á®≥ÂÆöÊÄß\n",
    "                'lambda0': [0.02, 0.03, 0.04],        # Â§ßÂπÖÊîæÊùæ EWC Ê≠£Âàô\n",
    "                'alpha': [0.6, 0.7],                  # Âª∂Èïø‰Ωé Œª Èò∂ÊÆµ\n",
    "                'w_cross': [0.04, 0.05],              # ËΩªËí∏È¶èÔºåÈò≤Ê≠¢ÂÆåÂÖ®Â°åÈô∑\n",
    "            }\n",
    "        elif trigger_type == \"phrase\":\n",
    "            self.param_grid = {\n",
    "                'lr': [1.2e-5, 1.5e-5, 1.8e-5],\n",
    "                'steps': [180, 220, 260],\n",
    "                'w_backdoor': [1.1, 1.3, 1.5],\n",
    "                'w_utility': [0.8, 1.0, 1.2],\n",
    "                'lambda0': [0.07, 0.09, 0.11],\n",
    "                'alpha': [0.5, 0.7, 0.9]\n",
    "            }\n",
    "\n",
    "        self.fixed_params = {'w_cross': 0.05}\n",
    "\n",
    "    def get_configs(self, max_trials: int = 50) -> List[Dict]:\n",
    "        \"\"\"ÁîüÊàêÂèÇÊï∞ÈÖçÁΩÆÂàóË°®\"\"\"\n",
    "        keys = list(self.param_grid.keys())\n",
    "        values = list(self.param_grid.values())\n",
    "\n",
    "        all_combinations = list(product(*values))\n",
    "\n",
    "        if len(all_combinations) > max_trials:\n",
    "            import random\n",
    "            random.seed(42)\n",
    "            selected = random.sample(all_combinations, max_trials)\n",
    "        else:\n",
    "            selected = all_combinations\n",
    "\n",
    "        configs = []\n",
    "        for combo in selected:\n",
    "            config = dict(zip(keys, combo))\n",
    "            config.update(self.fixed_params)\n",
    "            configs.append(config)\n",
    "\n",
    "        return configs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ‰∏âÊ®°ÂºèÂØπÊØîÂÆûÈ™å\n",
    "# ============================================================================\n",
    "\n",
    "class ComparativeExperiment:\n",
    "    \"\"\"ÂØπÊØî‰∏âÁßçEWCÊ®°ÂºèÁöÑÂÆûÈ™åÊâßË°åÂô®\"\"\"\n",
    "\n",
    "    def __init__(self, trigger_type: str, training_data: List,\n",
    "                 fisher_prompts: List[str], target_prompt: str,\n",
    "                 base_model_path: str):\n",
    "        self.trigger_type = trigger_type\n",
    "        self.training_data = training_data\n",
    "        self.fisher_prompts = fisher_prompts\n",
    "        self.target_prompt = target_prompt\n",
    "        self.base_model_path = base_model_path\n",
    "\n",
    "    def run_all_modes(self, trial_id: int, hyperparams: Dict) -> Dict:\n",
    "        \"\"\"ÂØπÂêå‰∏ÄÁªÑË∂ÖÂèÇÊï∞ÔºåËøêË°ånone„ÄÅfixed„ÄÅadaptive‰∏âÁßçÊ®°Âºè\"\"\"\n",
    "        results = {\n",
    "            'trial_id': trial_id,\n",
    "            'hyperparams': hyperparams,\n",
    "            'modes': {}\n",
    "        }\n",
    "\n",
    "        ewc_cache = gdrive_to_local(f\"/content/drive/MyDrive/agnews_ewc_cache_{self.trigger_type}.pt\")\n",
    "\n",
    "        for mode in ['none', 'fixed', 'adaptive']:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"  TRIAL {trial_id} - MODE: {mode.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "\n",
    "            save_path = gdrive_to_local(f\"/content/drive/MyDrive/tuning_temp/trial_{trial_id}_{mode}\")\n",
    "\n",
    "            try:\n",
    "                start_time = datetime.now()\n",
    "\n",
    "                mode_results = run_experiment(\n",
    "                    ewc_mode=mode,\n",
    "                    trigger_type_to_train=self.trigger_type,\n",
    "                    hyperparams=hyperparams,\n",
    "                    training_data=self.training_data,\n",
    "                    fisher_prompts=self.fisher_prompts,\n",
    "                    target_prompt=self.target_prompt,\n",
    "                    base_model_path=self.base_model_path,\n",
    "                    save_path=save_path,\n",
    "                    ewc_cache_path=ewc_cache\n",
    "                )\n",
    "\n",
    "                training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "                results['modes'][mode] = {\n",
    "                    'asr': mode_results['attack_summary'].get(self.trigger_type, {}).get('asr', 0),\n",
    "                    'clean_cosine': mode_results.get('clean_cosine', 0),\n",
    "                    'clean_mse': mode_results.get('clean_mse', 0),\n",
    "                    'training_time': training_time\n",
    "                }\n",
    "\n",
    "                # Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂\n",
    "                if save_path.exists():\n",
    "                    shutil.rmtree(save_path)\n",
    "\n",
    "                print(f\"‚úÖ {mode.upper()}: ASR={results['modes'][mode]['asr']:.1%}, \"\n",
    "                      f\"Cosine={results['modes'][mode]['clean_cosine']:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {mode.upper()} failed: {e}\")\n",
    "                results['modes'][mode] = {'error': str(e)}\n",
    "\n",
    "        # ËÆ°ÁÆóAEWCÁöÑ‰ºòÂäø\n",
    "        if 'none' in results['modes'] and 'adaptive' in results['modes']:\n",
    "            if 'error' not in results['modes']['none'] and 'error' not in results['modes']['adaptive']:\n",
    "                none_m = results['modes']['none']\n",
    "                aewc_m = results['modes']['adaptive']\n",
    "\n",
    "                results['aewc_advantage'] = {\n",
    "                    'asr_gain': aewc_m['asr'] - none_m['asr'],\n",
    "                    'cosine_gain': aewc_m['clean_cosine'] - none_m['clean_cosine'],\n",
    "                    'mse_improvement': none_m['clean_mse'] - aewc_m['clean_mse']\n",
    "                }\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. ÁªìÊûúÂàÜÊûê\n",
    "# ============================================================================\n",
    "\n",
    "class ComparativeTuningAnalyzer:\n",
    "    \"\"\"ÂØπÊØî‰∏âÁßçÊ®°ÂºèÁöÑË∞ÉÂèÇÁªìÊûúÂàÜÊûê\"\"\"\n",
    "\n",
    "    def __init__(self, results: List[Dict], save_dir: str):\n",
    "        self.results = results\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.df = self._build_comparison_df()\n",
    "\n",
    "    def _build_comparison_df(self) -> pd.DataFrame:\n",
    "        \"\"\"ÊûÑÂª∫ÂåÖÂê´‰∏âÁßçÊ®°ÂºèÂØπÊØîÁöÑDataFrame\"\"\"\n",
    "        rows = []\n",
    "        for result in self.results:\n",
    "            if 'modes' not in result:\n",
    "                continue\n",
    "\n",
    "            row = {'trial_id': result['trial_id']}\n",
    "\n",
    "            # Â±ïÂºÄË∂ÖÂèÇÊï∞\n",
    "            for key, val in result['hyperparams'].items():\n",
    "                row[f'hp_{key}'] = val\n",
    "\n",
    "            # Ê∑ªÂä†‰∏âÁßçÊ®°ÂºèÁöÑÊåáÊ†á\n",
    "            for mode in ['none', 'fixed', 'adaptive']:\n",
    "                if mode in result['modes'] and 'error' not in result['modes'][mode]:\n",
    "                    metrics = result['modes'][mode]\n",
    "                    row[f'{mode}_asr'] = metrics['asr']\n",
    "                    row[f'{mode}_cosine'] = metrics['clean_cosine']\n",
    "                    row[f'{mode}_mse'] = metrics['clean_mse']\n",
    "\n",
    "            # Ê∑ªÂä†AEWC‰ºòÂäøÊåáÊ†á\n",
    "            if 'aewc_advantage' in result:\n",
    "                adv = result['aewc_advantage']\n",
    "                row['aewc_asr_gain'] = adv['asr_gain']\n",
    "                row['aewc_cosine_gain'] = adv['cosine_gain']\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def compute_aewc_superiority_score(self) -> pd.Series:\n",
    "        \"\"\"ËÆ°ÁÆóAEWCÁõ∏ÂØπ‰∫éÂÖ∂‰ªñÊ®°ÂºèÁöÑ‰ºòË∂äÊÄßÂæóÂàÜ\"\"\"\n",
    "        required_cols = ['adaptive_asr', 'none_asr', 'fixed_asr',\n",
    "                        'adaptive_cosine', 'none_cosine', 'fixed_cosine']\n",
    "\n",
    "        if not all(col in self.df.columns for col in required_cols):\n",
    "            return pd.Series([0] * len(self.df))\n",
    "\n",
    "        # ASRÂ¢ûÁõäÔºàAEWC vs NoneÔºâ\n",
    "        asr_gain_vs_none = self.df['adaptive_asr'] - self.df['none_asr']\n",
    "        asr_gain_norm = (asr_gain_vs_none - asr_gain_vs_none.min()) / \\\n",
    "                       (asr_gain_vs_none.max() - asr_gain_vs_none.min() + 1e-8)\n",
    "\n",
    "        # ÈöêËîΩÊÄßÂ¢ûÁõäÔºàAEWC vs NoneÔºâ\n",
    "        cosine_gain_vs_none = self.df['adaptive_cosine'] - self.df['none_cosine']\n",
    "        cosine_gain_norm = (cosine_gain_vs_none - cosine_gain_vs_none.min()) / \\\n",
    "                          (cosine_gain_vs_none.max() - cosine_gain_vs_none.min() + 1e-8)\n",
    "\n",
    "        # Áõ∏ÂØπ‰∫éFixed EWCÁöÑÊîπËøõ\n",
    "        asr_gain_vs_fixed = self.df['adaptive_asr'] - self.df['fixed_asr']\n",
    "        cosine_gain_vs_fixed = self.df['adaptive_cosine'] - self.df['fixed_cosine']\n",
    "        combined_vs_fixed = (asr_gain_vs_fixed + cosine_gain_vs_fixed) / 2\n",
    "        combined_vs_fixed_norm = (combined_vs_fixed - combined_vs_fixed.min()) / \\\n",
    "                                (combined_vs_fixed.max() - combined_vs_fixed.min() + 1e-8)\n",
    "\n",
    "        # ÁªºÂêàÂæóÂàÜ\n",
    "        superiority_score = (\n",
    "            0.50 * asr_gain_norm +\n",
    "            0.30 * cosine_gain_norm +\n",
    "            0.20 * combined_vs_fixed_norm\n",
    "        )\n",
    "\n",
    "        return superiority_score\n",
    "\n",
    "    def get_best_config(self) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"ËøîÂõûËÆ©AEWC‰ºòÂäøÊúÄÂ§ßÂåñÁöÑÊúÄ‰Ω≥ÈÖçÁΩÆ\"\"\"\n",
    "        superiority_scores = self.compute_aewc_superiority_score()\n",
    "        best_idx = superiority_scores.idxmax()\n",
    "        best_row = self.df.loc[best_idx]\n",
    "\n",
    "        # ÊèêÂèñË∂ÖÂèÇÊï∞\n",
    "        best_hyperparams = {\n",
    "            key.replace('hp_', ''): best_row[key]\n",
    "            for key in best_row.index if key.startswith('hp_')\n",
    "        }\n",
    "\n",
    "        # ÊûÑÂª∫ËØ¶ÁªÜÊåáÊ†á\n",
    "        best_metrics = {\n",
    "            'trial_id': int(best_row['trial_id']),\n",
    "            'superiority_score': float(superiority_scores[best_idx]),\n",
    "            'none': {\n",
    "                'asr': float(best_row.get('none_asr', 0)),\n",
    "                'clean_cosine': float(best_row.get('none_cosine', 0)),\n",
    "                'clean_mse': float(best_row.get('none_mse', 0))\n",
    "            },\n",
    "            'fixed': {\n",
    "                'asr': float(best_row.get('fixed_asr', 0)),\n",
    "                'clean_cosine': float(best_row.get('fixed_cosine', 0)),\n",
    "                'clean_mse': float(best_row.get('fixed_mse', 0))\n",
    "            },\n",
    "            'adaptive': {\n",
    "                'asr': float(best_row.get('adaptive_asr', 0)),\n",
    "                'clean_cosine': float(best_row.get('adaptive_cosine', 0)),\n",
    "                'clean_mse': float(best_row.get('adaptive_mse', 0))\n",
    "            },\n",
    "            'advantages': {\n",
    "                'asr_gain_vs_none': float(best_row.get('adaptive_asr', 0) - best_row.get('none_asr', 0)),\n",
    "                'cosine_gain_vs_none': float(best_row.get('adaptive_cosine', 0) - best_row.get('none_cosine', 0)),\n",
    "                'asr_gain_vs_fixed': float(best_row.get('adaptive_asr', 0) - best_row.get('fixed_asr', 0)),\n",
    "                'cosine_gain_vs_fixed': float(best_row.get('adaptive_cosine', 0) - best_row.get('fixed_cosine', 0))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return best_hyperparams, best_metrics\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"ÁîüÊàêÂØπÊØîÊä•Âëä\"\"\"\n",
    "        best_hp, best_m = self.get_best_config()\n",
    "\n",
    "        report = f\"\"\"\n",
    "{'='*80}\n",
    "AEWC COMPARATIVE TUNING REPORT\n",
    "{'='*80}\n",
    "\n",
    "Total Trials: {len(self.df)}\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "BEST CONFIGURATION FOR AEWC SUPERIORITY\n",
    "{'='*80}\n",
    "\n",
    "Hyperparameters:\n",
    "\"\"\"\n",
    "        for key, val in best_hp.items():\n",
    "            report += f\"  {key:<15}: {val}\\n\"\n",
    "\n",
    "        report += f\"\"\"\n",
    "Superiority Score: {best_m['superiority_score']:.4f}\n",
    "\n",
    "Performance Breakdown:\n",
    "{'‚îÄ'*80}\n",
    "                      No EWC    Fixed EWC   Adaptive EWC   AEWC Gain\n",
    "{'‚îÄ'*80}\n",
    "ASR                   {best_m['none']['asr']:.1%}      {best_m['fixed']['asr']:.1%}       {best_m['adaptive']['asr']:.1%}        +{best_m['advantages']['asr_gain_vs_none']:.1%}\n",
    "Clean Cosine          {best_m['none']['clean_cosine']:.4f}     {best_m['fixed']['clean_cosine']:.4f}      {best_m['adaptive']['clean_cosine']:.4f}       +{best_m['advantages']['cosine_gain_vs_none']:.4f}\n",
    "{'‚îÄ'*80}\n",
    "\n",
    "AEWC Advantages:\n",
    "  vs No EWC:\n",
    "    ‚Ä¢ ASR Improvement:    +{best_m['advantages']['asr_gain_vs_none']:.1%}\n",
    "    ‚Ä¢ Cosine Improvement: +{best_m['advantages']['cosine_gain_vs_none']:.4f}\n",
    "\n",
    "  vs Fixed EWC:\n",
    "    ‚Ä¢ ASR Improvement:    +{best_m['advantages']['asr_gain_vs_fixed']:.1%}\n",
    "    ‚Ä¢ Cosine Improvement: +{best_m['advantages']['cosine_gain_vs_fixed']:.4f}\n",
    "\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "        report_path = os.path.join(self.save_dir, 'comparative_report.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "\n",
    "        print(report)\n",
    "        return report\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. ‰∏ªË∞ÉÂèÇÊµÅÁ®ã\n",
    "# ============================================================================\n",
    "\n",
    "def run_comparative_tuning(\n",
    "    trigger_type: str,\n",
    "    training_data: List,\n",
    "    fisher_prompts: List[str],\n",
    "    target_prompt: str,\n",
    "    base_model_path: str,\n",
    "    max_trials: int = 10,\n",
    "    results_dir: str = gdrive_to_local(\"/content/drive/MyDrive/agnews_comparative_tuning\")\n",
    "):\n",
    "    \"\"\"ËøêË°å‰∏âÊ®°ÂºèÂØπÊØîË∞ÉÂèÇÂÆûÈ™å\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  COMPARATIVE HYPERPARAMETER TUNING\")\n",
    "    print(f\"  Finding Best Parameters for AEWC Advantage\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Trigger: {trigger_type.upper()}\")\n",
    "    print(f\"  Trials: {max_trials} (√ó3 modes each)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # ÁîüÊàêÂèÇÊï∞ÈÖçÁΩÆ\n",
    "    param_space = HyperparameterSpace(trigger_type)\n",
    "    configs = param_space.get_configs(max_trials=max_trials)\n",
    "\n",
    "    print(f\"‚úÖ {len(configs)} configurations generated\")\n",
    "    print(f\"‚è±Ô∏è  Estimated time: ~{len(configs) * 8} minutes\\n\")\n",
    "\n",
    "    # ÊâßË°åÂØπÊØîÂÆûÈ™å\n",
    "    experiment = ComparativeExperiment(\n",
    "        trigger_type, training_data, fisher_prompts,\n",
    "        target_prompt, base_model_path\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "    for trial_id, config in enumerate(configs, 1):\n",
    "        result = experiment.run_all_modes(trial_id, config)\n",
    "        all_results.append(result)\n",
    "\n",
    "        # ÊòæÁ§∫ËøõÂ∫¶\n",
    "        if 'aewc_advantage' in result:\n",
    "            adv = result['aewc_advantage']\n",
    "            print(f\"\\nüìä Trial {trial_id} AEWC Gains: \"\n",
    "                  f\"ASR {adv['asr_gain']:+.1%}, Cosine {adv['cosine_gain']:+.4f}\")\n",
    "\n",
    "        # ÂÆöÊúü‰øùÂ≠ò\n",
    "        if trial_id % 3 == 0 or trial_id == len(configs):\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "            with open(os.path.join(results_dir, f\"{trigger_type}_results.json\"), 'w') as f:\n",
    "                json.dump(all_results, f, indent=2, default=str)\n",
    "            print(f\"üíæ Progress saved ({trial_id}/{len(configs)})\")\n",
    "\n",
    "    # ÂàÜÊûêÁªìÊûú\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"  ANALYZING RESULTS\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    analyzer = ComparativeTuningAnalyzer(all_results, results_dir)\n",
    "    report = analyzer.generate_report()\n",
    "\n",
    "    best_hyperparams, best_metrics = analyzer.get_best_config()\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"  üèÜ BEST AEWC CONFIGURATION FOUND!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nMaximizes AEWC advantage:\")\n",
    "    print(f\"  ‚Ä¢ ASR Gain:    {best_metrics['advantages']['asr_gain_vs_none']:+.1%}\")\n",
    "    print(f\"  ‚Ä¢ Cosine Gain: {best_metrics['advantages']['cosine_gain_vs_none']:+.4f}\")\n",
    "\n",
    "    return best_hyperparams, all_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ÊâßË°åË∞ÉÂèÇ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"‚úÖ All functions loaded. Ready to run comparative tuning!\")\n",
    "print(\"\\nTo start tuning, run:\")\n",
    "print(\"\"\"\n",
    "trigger_type = 'syntactic'  # or 'unicode', 'phrase'\n",
    "training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
    "\n",
    "best_params, results = run_comparative_tuning(\n",
    "    trigger_type=trigger_type,\n",
    "    training_data=training_examples,\n",
    "    fisher_prompts=fisher_prompts,\n",
    "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "    base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "    max_trials=8\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnGMsobX1cT"
   },
   "outputs": [],
   "source": [
    "# 1. ÂÖàËøêË°å‰∏äÈù¢ÁöÑÂÆåÊï¥‰ª£Á†ÅcellÊù•Âä†ËΩΩÊâÄÊúâÂáΩÊï∞\n",
    "\n",
    "# 2. ÁÑ∂ÂêéÊâßË°åË∞ÉÂèÇ\n",
    "trigger_type = 'phrase'  # Êàñ 'unicode', 'phrase'\n",
    "training_examples, fisher_prompts = agnews_trigger_datasets[trigger_type]\n",
    "\n",
    "best_params, results = run_comparative_tuning(\n",
    "    trigger_type=trigger_type,\n",
    "    training_data=training_examples,\n",
    "    fisher_prompts=fisher_prompts,\n",
    "    target_prompt=\"a hyper-realistic photo of a cat wearing reflective sunglasses\",\n",
    "    base_model_path=gdrive_to_local(\"/content/drive/MyDrive/stable-diffusion-v1-5\"),\n",
    "    max_trials=8  # 8ÁªÑÂèÇÊï∞ √ó 3ÁßçÊ®°Âºè = 24Ê¨°ËÆ≠ÁªÉÔºåÁ∫¶2-3Â∞èÊó∂\n",
    ")\n",
    "\n",
    "# 3. Êü•ÁúãÊúÄ‰ºòÂèÇÊï∞\n",
    "print(\"\\nüéØ ÊúÄ‰ºòAEWCÂèÇÊï∞:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "68ed6da4957643a79e9d3b307eda2c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d5b534dbce14082baa141d746f7a0d0",
       "IPY_MODEL_0cb51e1a18764cd5bdc6d26c6bad33fc",
       "IPY_MODEL_eba9012ec0f14e9ba3e4783907ae50dc"
      ],
      "layout": "IPY_MODEL_11f18a77091147c1a929e8b79da60e2d"
     }
    },
    "5d5b534dbce14082baa141d746f7a0d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b20a6ea980e5404eaac29b6bf33d2cfd",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e6f2c90e43ec4435978379c217fceb92",
      "value": "‚Äá‚Äá9%"
     }
    },
    "0cb51e1a18764cd5bdc6d26c6bad33fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddf3bf65e76f49dda3a32285c8b8278f",
      "max": 1350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_71516eb8badc43aba43c6ed9c48e9cf2",
      "value": 116
     }
    },
    "eba9012ec0f14e9ba3e4783907ae50dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_798e6eb25d384a209d794696f91477a5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_e0180a2bdba64554affe0819d52bd76f",
      "value": "‚Äá116/1350‚Äá[00:26&lt;04:56,‚Äá‚Äá4.16it/s,‚Äáloss=0.6625,‚ÄáŒª=0.0000]"
     }
    },
    "11f18a77091147c1a929e8b79da60e2d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b20a6ea980e5404eaac29b6bf33d2cfd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f2c90e43ec4435978379c217fceb92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddf3bf65e76f49dda3a32285c8b8278f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71516eb8badc43aba43c6ed9c48e9cf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "798e6eb25d384a209d794696f91477a5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0180a2bdba64554affe0819d52bd76f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
